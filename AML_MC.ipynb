{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c0dd29",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[MC Angewandtes Machine Learning -<br>Frühlingssemester 2025 -<br>Autor: Nabil Mikhael | Alessandro Gregori](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9476d3",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [MC Angewandtes Machine Learning -<br>Frühlingssemester 2025 -<br>Autor: Nabil Mikhael | Alessandro Gregori](#toc1_)    \n",
    "- [Import Libraries](#toc2_)    \n",
    "- [Einführung](#toc3_)    \n",
    "  - [Ziel der Mini-Challenge](#toc3_1_)    \n",
    "  - [ERD](#toc3_2_)    \n",
    "- [Datenaufbereitung](#toc4_)    \n",
    "  - [Datenvorbereitung & EDA pro Entität & Vergleich Konten 14 & 18](#toc4_1_)    \n",
    "    - [Statische Entitäten](#toc4_1_1_)    \n",
    "      - [Credit Card (`credit_card_df`)](#toc4_1_1_1_)    \n",
    "      - [Disposition (`disposition_df`)](#toc4_1_1_2_)    \n",
    "      - [Accounts (`accounts_df`)](#toc4_1_1_3_)    \n",
    "      - [Districts (`district_df`)](#toc4_1_1_4_)    \n",
    "      - [Clients (`client_df`)](#toc4_1_1_5_)    \n",
    "    - [Dynamische Entitäten](#toc4_1_2_)    \n",
    "      - [Permanent Order (`order_df`)](#toc4_1_2_1_)    \n",
    "      - [Loan (`loan_df`)](#toc4_1_2_2_)    \n",
    "      - [Transaction (`transaction_df`)](#toc4_1_2_3_)    \n",
    "    - [Konto 14 und 18](#toc4_1_3_)    \n",
    "  - [Kombination und EDA der statischen Entitäten](#toc4_2_)    \n",
    "    - [Kombination (`merged_df_static`)](#toc4_2_1_)    \n",
    "    - [Unterteilung von Käufern und Nichtkäufern](#toc4_2_2_)    \n",
    "    - [Klärung entitätenspezifischer Fragestellungen (aus Kapitel 2)](#toc4_2_3_)    \n",
    "    - [Implikationen für die Modellentwicklung (Zusammenfassungen)](#toc4_2_4_)    \n",
    "  - [Bereinigung Grundmenge](#toc4_3_)    \n",
    "    - [Entfernen der Junior-Karten](#toc4_3_1_)    \n",
    "    - [Ausschluss Disponenten](#toc4_3_2_)    \n",
    "- [Modellkonstruktion](#toc5_)    \n",
    "  - [Definitionen Kreditkarten-Käufer](#toc5_1_)    \n",
    "    - [Käufer (`buyers_df`)](#toc5_1_1_)    \n",
    "    - [Kaufdatum](#toc5_1_2_)    \n",
    "    - [Rollup-Fenster (`buyers_event_info_df`)](#toc5_1_3_)    \n",
    "  - [Definitionen Kreditkarten-Nichtkäufer](#toc5_2_)    \n",
    "    - [Nichtkäufer (`non_buyers_df`)](#toc5_2_1_)    \n",
    "    - [Kaufdatum](#toc5_2_2_)    \n",
    "    - [Rollup-Fenster (`non_buyers_event_info_df`)](#toc5_2_3_)    \n",
    "  - [EDA Käufer/Nichtkäufer](#toc5_3_)    \n",
    "- [Feature Engineering](#toc6_)    \n",
    "  - [Kombination eventbezogener Informationen (`combined_df`)](#toc6_1_)    \n",
    "  - [Bereinigung und EDA (`final_df`)](#toc6_2_)    \n",
    "- [Modellentwicklung](#toc7_)    \n",
    "  - [Partitionierung Trainings- und Testdaten und NaN Imputation](#toc7_1_)    \n",
    "  - [Baseline-Modell (Logistic Regression)](#toc7_2_)    \n",
    "    - [Training](#toc7_2_1_)    \n",
    "    - [Evaluation](#toc7_2_2_)    \n",
    "  - [Verbesserung Baseline-Modells durch Feature-Selektion (LogReg-Modell VIF-basiert)](#toc7_3_)    \n",
    "    - [Multikollinearität reduzieren: VIF](#toc7_3_1_)    \n",
    "    - [Training](#toc7_3_2_)    \n",
    "    - [Evaluation](#toc7_3_3_)    \n",
    "  - [Vorbereitung Kandidatenmodelle für den Modellvergleich](#toc7_4_)    \n",
    "- [Modellvergleich, -selektion und -optimierung](#toc8_)    \n",
    "  - [Vergleich Modellperformance](#toc8_1_)    \n",
    "    - [Auswahl des besten Modells](#toc8_1_1_)    \n",
    "  - [Vergleich Top-N-Kundenlisten](#toc8_2_)    \n",
    "  - [Hyperparameter-Tuning und Test-Set-Evaluation](#toc8_3_)    \n",
    "    - [Evaluation](#toc8_3_1_)    \n",
    "- [Modellerklärung und -reduktion](#toc9_)    \n",
    "  - [Globale Prädiktorwichtigkeit mittels PFI und PDP](#toc9_1_)    \n",
    "  - [Feature-Importance-Vergleich: Baseline, Kandidatenmodelle und Bestmodell](#toc9_2_)    \n",
    "  - [Modellreduktion: Vereinfachung durch Auswahl der wichtigsten Merkmale](#toc9_3_)    \n",
    "  - [Praktische Bedeutung und Erklärung des finalen Modells](#toc9_4_)    \n",
    "    - [Lift Kurve](#toc9_4_1_)    \n",
    "    - [Quantitative Beschreibung zentraler Predictive Features](#toc9_4_2_)    \n",
    "    - [Schlussfazit (Beschreibung Funktionsweise und Mehrwert des finalen Modells)](#toc9_4_3_)    \n",
    "- [Anhang](#toc10_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11cf41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import plotly.express as px\n",
    "plt.style.use('ggplot')\n",
    "from datetime import timedelta\n",
    "import glob\n",
    "import os\n",
    "# Dezimalstellen auf 2 stellen für pandas DataFrames\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Unterdrückt alle zukünftigen Warnungen (z. B. FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8bbb10",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Einführung](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Ziel der Mini-Challenge](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435deae5",
   "metadata": {},
   "source": [
    "Ziel ist es Kundenlisten für eine personalisierte Kreditkarten-Werbekampagne zu erzeugen, wobei keine\n",
    "Junior-Karten angeboten werden sollen. \n",
    "\n",
    "https://sorry.vse.cz/~berka/challenge/PAST/index.html (Beschreibung der Daten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[ERD](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05e130",
   "metadata": {},
   "source": [
    "![ER-Modell](er_diagramm/er_diagramm_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904096c",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Datenaufbereitung](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Datenvorbereitung & EDA pro Entität & Vergleich Konten 14 & 18](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae828b",
   "metadata": {},
   "source": [
    "In diesem Abschnitt geht es um das Einlesen, Bereinigung und erste Analyse der Daten, um die Qualität und Struktur der Datensätze zu überprüfen. Der Ablauf erfolgt in zwei Hauptschritten:\n",
    "\n",
    "1. **Laden & Bereinigen der Daten**:\n",
    "   - Laden des Datensatzes  \n",
    "   - Duplikate, fehlende und fehlerhafte Werte werden identifiziert und behandelt.  \n",
    "   - Datentypen werden überprüft und gegebenenfalls angepasst, um sicherzustellen, dass die Daten korrekt vorliegen.  \n",
    "   - Primärschlüssel werden geprüft, um die Eindeutigkeit und Integrität der Datensätze sicherzustellen.\n",
    "\n",
    "2. **Explorative Analyse**:  \n",
    "   - Deskriptive Statistiken und Visualisierungen helfen, grundlegende Muster, Verteilungen und Ausreisser in den Daten zu erkennen.\n",
    "   - Erste Einblicke in die Datenstruktur ermöglichen die Identifikation potenziell relevanter Merkmale.\n",
    "\n",
    "Am Ende jedes Unterkapitels fassen wir die gewonnenen Erkenntnisse zusammen und formulieren erste offene Fragen oder Handlungsempfehlungen für den weiteren Verlauf.\n",
    "\n",
    "\n",
    "**Unterscheidung statische und dynamische Entitäten:**\n",
    "\n",
    "   Die Datensätze lassen sich in statische und dynamische Entitäten unterteilen. Diese Differenzierung bildet die Grundlage für die gezielte Aggregation und Ableitung von Merkmalen im Rollup-Fenster und ermöglicht eine strukturierte Analyse sowohl langfristiger als auch zeitabhängiger Informationen.\n",
    "\n",
    "\n",
    "- **Statische Entitäten: Client, Disposition, Account, Credit Card, Districts**\n",
    "   Diese Daten sind weitgehend unveränderlich und werden nur bei spezifischen Ereignissen – etwa einem Umzug oder einer Änderung der Kontodaten – aktualisiert. Sie umfassen langfristig stabile Informationen wie Kundendaten, Kontoinformationen und Kreditkartendetails. Dadurch eignen sich statische Entitäten besonders für die Modellierung konstanter Merkmale eines Kunden, z. B. Alter, Geschlecht, Region oder Kartentyp.\n",
    "\n",
    "- **Dynamische Entitäten: Transaction, Permanent Order, Loan**\n",
    "   Dynamische Entitäten erzeugen fortlaufend neue Datenpunkte und bilden die Grundlage für zeitabhängige Merkmale (z. B. Kontostände, Aktivitätsmuster).\n",
    "\n",
    "Bei den dynamischen Entitäten  führen wir zwar eine explorative Analyse durch, jedoch erfolgt an dieser Stelle noch keine gezielte Formulierung von Fragestellungen. Diese ergeben sich erst im Rahmen der Aggregation im Rollup-Fenster und werden im Kapitel 4 Feature Engineering detaillierter behandelt. Dadurch bleibt die Analyse der dynamischen Entitäten zunächst beschreibend und wird erst später in den Kontext der Modellbildung gesetzt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d4e02",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[Statische Entitäten](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_1_1_'></a>[Credit Card (`credit_card_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931da36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Kreditkartendaten mit spezifizierten Datentypen:\n",
    "# 'card_id' und 'disp_id' als Integer, 'type' als kategorische Variable,\n",
    "# sowie 'issued' als Datum im Format \"%y%m%d %H:%M:%S\"\n",
    "credit_card_df = pd.read_csv(\"xselling_banking_data/card.csv\", delimiter=\";\", dtype={\n",
    "    \"card_id\": \"int64\",\n",
    "    \"disp_id\": \"int64\",\n",
    "    \"type\": \"category\"},\n",
    "    parse_dates=[\"issued\"],\n",
    "    date_parser=lambda x: pd.to_datetime(x, format=\"%y%m%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b09c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506d4d6",
   "metadata": {},
   "source": [
    "Die Spaltentypen sehen korrekt aus, wie sie eingelesen wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb45731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas zeigt in head() keine Uhrzeit an, wenn sie 00:00:00 ist – Anzeigeoptimierung\n",
    "credit_card_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f437f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfen der Nullwerte\n",
    "credit_card_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae6471",
   "metadata": {},
   "source": [
    "Der Datensatz enthält keine Nullwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df['disp_id'].is_unique and credit_card_df['card_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab56bcd",
   "metadata": {},
   "source": [
    "- card_id ist innerhalb von credit_card_df eindeutig und eignet sich daher als Primärschlüssel der Tabelle.\n",
    "- disp_id ist ebenfalls eindeutig in credit_card_df und stellt einen Fremdschlüssel zur Tabelle disposition_df dar.\n",
    "- Zwischen card_id und disp_id besteht eine 1:1-Beziehung:\n",
    "Jeder Dispositionseintrag (disp_id) erhält maximal eine Karte, und jede Karte ist genau einem Nutzer (Dispositionseintrag) zugeordnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867d8c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd66591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Anzahl und Prozentanteile der verschiedenen Kreditkartentypen\n",
    "type_counts = credit_card_df[\"type\"].value_counts().reset_index()\n",
    "type_counts.columns = ['type', 'count']\n",
    "\n",
    "# Prozentuale Anteile der Kreditkartentypen berechnen und auf 1 Dezimalstelle runden\n",
    "type_counts['percent'] = ((type_counts['count'] / type_counts['count'].sum()) * 100).round(1)\n",
    "\n",
    "# Balkendiagramm zur Visualisierung der Verteilung der Kreditkartentypen\n",
    "fig = px.bar(\n",
    "    type_counts, \n",
    "    x='type', \n",
    "    y='percent', \n",
    "    text_auto='.1f', \n",
    "    color='type', \n",
    "    hover_data=['count'],\n",
    "    title='Distribution of Credit Cards by Type',\n",
    "    labels={'type': 'Type', 'percent': 'Percentage (%)'}\n",
    ")\n",
    "\n",
    "# Textbeschriftungen ausserhalb der Balken anzeigen\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975142c",
   "metadata": {},
   "source": [
    "Classic-Karten machen 73.9 % aus, gefolgt von Junior- (16.3 %) und Gold-Karten (9.9 %).\n",
    "Für die geplante Werbekampagne sollen keine Junior-Karten angeboten werden.\n",
    "Da jedoch noch unklar ist, bis zu welchem Alter eine Karte als \"Junior\" gilt, werden sie erst nach dem Mergen der DataFrames entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Anzahl ausgegebener Kreditkarten pro Jahr inklusive prozentualer Anteile\n",
    "yearly_counts = credit_card_df['issued'].dt.year.value_counts().sort_index().reset_index()\n",
    "yearly_counts.columns = ['year', 'count']  \n",
    "\n",
    "yearly_counts['percent'] = (yearly_counts['count'] / yearly_counts['count'].sum() * 100).round(1)\n",
    "\n",
    "# Balkendiagramm zur Darstellung des prozentualen Anteils der ausgegebenen Karten pro Jahr\n",
    "fig = px.bar(\n",
    "    yearly_counts,\n",
    "    x='year',\n",
    "    y='percent',\n",
    "    text_auto=True,\n",
    "    title=\"Percentage of Cards Issued per Year\"\n",
    ")\n",
    "fig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\n",
    "fig.update_layout(yaxis_title='Percent (%)', xaxis_title='Year')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced37371",
   "metadata": {},
   "source": [
    "Analyse der Kartenausgaben pro Jahr (in Prozent)\n",
    "\n",
    "- Ziel: Überprüfen, ob die zeitliche Verteilung realistisch und plausibel ist\n",
    "- Ergebnis: Kontinuierlicher Anstieg – Grossteil der Karten wurde 1997–1998 ausgestellt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeitliche Reihenfolge vs. ID-Reihenfolge\n",
    "#Ist card_id in etwa mit issued korreliert?\n",
    "\n",
    "credit_card_df[['card_id', 'issued']].sort_values('issued').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8b681",
   "metadata": {},
   "source": [
    "Zeitliche Reihenfolge vs. ID-Reihenfolge\n",
    "\n",
    "- Überprüfung, ob card_id chronologisch mit dem Ausgabedatum vergeben wurde\n",
    "- Ergebnis: IDs sind nicht strikt aufsteigend zur Zeit → keine zeitliche Logik in der Vergabe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b111b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df['issued'].dt.month.value_counts(normalize=True).mul(100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17a26b",
   "metadata": {},
   "source": [
    "Die Analyse der Kreditkartenausgabe über alle Monate hinweg zeigt eine deutliche Saisonalität:\n",
    "Die meisten Karten werden in den Monaten Oktober bis Dezember ausgestellt, mit einem Höhepunkt im Dezember (10.9 %).\n",
    "\n",
    "Die schwächsten Monate sind Februar (5.3 %) und März (5.5 %). Diese Verteilung deutet auf ein saisonales Kundeninteresse zum Jahresende hin – möglicherweise bedingt durch bevorstehende Feiertage, vermehrte Ausgaben oder Marketingaktionen in dieser Zeit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c98672",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10a9e9",
   "metadata": {},
   "source": [
    "\n",
    "**Schlüsselstruktur und Datenmodell**\n",
    "- `card_id` ist der **Primärschlüssel** der Tabelle `credit_card`.\n",
    "- `disp_id` ist ein **Fremdschlüssel** zur Tabelle `disposition`.\n",
    "- Es liegt eine **1:1-Beziehung** vor:  \n",
    "  Jede Karte ist genau einem Dispositionseintrag (`disp_id`) zugeordnet, und jeder Dispositionseintrag erhält höchstens eine Karte.\n",
    "\n",
    "\n",
    "**Zusammenfassung der EDA-Erkenntnisse**\n",
    "- Der Kartentyp **classic** dominiert mit einem Anteil von **74 %**.\n",
    "- **Kein erkennbarer linearer Zusammenhang** zwischen `card_id` und Ausstellungsdatum.\n",
    "- Die Mehrheit der Karten wurde in den Jahren **1997–1998** ausgestellt (**77 %**).\n",
    "- Kontinuierlicher Anstieg der Kartenausstellungen von **1993 bis 1998**.\n",
    "- **Saisonalität**: Hohe Ausstellungszahlen zwischen **Oktober und Dezember**, niedrige zwischen **Februar und März**.\n",
    "- Es konnte **kein zeitlicher Zusammenhang zwischen `card_id` und dem Ausstellungsdatum** festgestellt werden, weshalb `card_id` **nicht als zeitliches Merkmal** interpretiert werden sollte.\n",
    "- Der Kartentyp ist ein potenziell nützliches Merkmal, allerdings müssen **Junior-Karten ausgeschlossen** werden, da sie nicht zur Zielgruppe der Werbekampagne zählen.\n",
    "\n",
    "**Offene Punkte und nächste Schritte** \n",
    "- Überprüfen, ob ausschliesslich `owner`-Einträge (nicht `Disponent`) in der `disposition`-Tabelle eine Kreditkarte besitzen. Dies stellt sicher, dass nur tatsächlich entscheidungsbefugte Personen (Kontoinhaber:innen) in die Modellierungsbasis einbezogen werden.\n",
    "- **Filtern von Junior-Karten:** Altersbasierter Ausschluss von Kreditkarten, die nicht zur Zielgruppe der Kampagne gehören.\n",
    "-  **Temporales Feature Engineering:** Bewertung, ob der Ausstellungsmonat saisonale Muster zeigt und als erklärendes Merkmal für das Modell genutzt werden kann.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_1_2_'></a>[Disposition (`disposition_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d377c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Dispositionsdaten mit spezifizierten Datentypen für IDs und Kategorie\n",
    "disposition_df = pd.read_csv(\"xselling_banking_data/disp.csv\", delimiter=\";\", dtype={\n",
    "    \"client_id\": \"int64\",\n",
    "    \"account_id\": \"int64\",\n",
    "    \"disp_id\": \"int64\",\n",
    "    \"type\": \"category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "disposition_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1230b5f",
   "metadata": {},
   "source": [
    "Die Spaltentypen sehen korrekt aus, wie sie eingelesen wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "disposition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfen der Nullwerte\n",
    "disposition_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd74b1d",
   "metadata": {},
   "source": [
    "Der Datensatz enthält keine Nullwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf09caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "disposition_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "disposition_df['disp_id'].is_unique and disposition_df['client_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc22c3",
   "metadata": {},
   "source": [
    "- Die Spalte disp_id ist eindeutig und dient als Primärschlüssel.\n",
    "- In der aktuellen Datenlage kommt jede client_id nur einmal vor \n",
    "- Die Spalte account_id ist nicht eindeutig:\n",
    "    -  Ein Konto kann mehreren Personen zugeordnet sein (\n",
    "        OWNER +USER).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a6dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93986325",
   "metadata": {},
   "outputs": [],
   "source": [
    "disposition_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e15c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Anzahl und prozentualen Anteile der verschiedenen Dispositionstypen (Kontozugriffsrollen)\n",
    "type_counts_disp = disposition_df['type'].value_counts().reset_index(name='count')\n",
    "type_counts_disp.columns = ['type', 'count'] \n",
    "type_counts_disp['percent'] = (type_counts_disp['count'] / type_counts_disp['count'].sum() * 100).round(2)\n",
    "\n",
    "# Balkendiagramm zur Visualisierung der Verteilung der Kontozugriffsrollen\n",
    "fig = px.bar(\n",
    "    type_counts_disp,\n",
    "    x='type',\n",
    "    y='percent',\n",
    "    text_auto=True,\n",
    "    color='type',\n",
    "    hover_data={\"count\": True}\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Account Access Roles\",\n",
    "    xaxis_title=\"Role\",\n",
    "    yaxis_title=\"Percentage (%)\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44598d4f",
   "metadata": {},
   "source": [
    "- OWNER ist deutlich häufiger als DISPONENT vorhanden\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ae597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie viele disposition-Einträge hat jede client_id?\n",
    "client_counts = disposition_df['client_id'].value_counts()\n",
    "client_counts\n",
    "\n",
    "# Zeige alle client_ids, die mehr als 1 Eintrag haben:\n",
    "client_counts[client_counts > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f07e1",
   "metadata": {},
   "source": [
    "- Es liegt eine 1:1-Beziehung zwischen client_id und disp_id vor.\n",
    "- Jede Kundin bzw. jeder Kunde  nur einem Konto zugeordnet.\n",
    "- Mehrfachrollen kommen nicht vor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbd1e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711951a",
   "metadata": {},
   "source": [
    "**Schlüsselstruktur und Datenmodell**\n",
    "\n",
    "- `disp_id` ist der **Primärschlüssel** der Tabelle `disposition`.\n",
    "- `client_id` ist ein **Fremdschlüssel** zur Tabelle `client`.\n",
    "- `account_id` ist ein **Fremdschlüssel** zur Tabelle `account`.\n",
    "\n",
    "Die Analyse zeigt, dass jede `client_id` im Datensatz **nur einmal** vorkommt.  \n",
    "→ Es liegt somit eine **1:1-Beziehung zwischen Kund:in und Dispositionseintrag** vor:\n",
    "- Jede Person ist aktuell **genau einem Konto** zugeordnet.\n",
    "- Und besitzt genau **eine Rolle** – entweder `owner` oder `disponent`.\n",
    "\n",
    "Diese 1:1-Zuordnung vereinfacht die Analyse, ist jedoch nicht zwingend durch das Datenmodell vorgegeben und sollte nach dem Zusammenführen mit anderen Tabellen erneut überprüft werden.\n",
    "\n",
    "Im Gegensatz dazu ist `account_id` **nicht eindeutig** – ein Konto kann mehreren Personen zugeordnet sein ( ein `owner` und ein `disponent`).  → Es liegt vermutlich eine **1:n-Beziehung zwischen `account_id` und `disp_id`** vor.\n",
    "\n",
    "\n",
    "\n",
    "Dies stellt **keinen Widerspruch** zur 1:1-Beziehung zwischen `client_id` und `disp_id` dar, sondern ist eine typische **1:n-Beziehung auf Kontoebene**.  \n",
    "Für spätere Analysen (z. B. Zuordnung von Kreditkarten oder Zahlungsverhalten) ist diese Struktur relevant, da sie Auswirkungen auf die Aggregation und Filterung der Daten haben kann.\n",
    "\n",
    "\n",
    "\n",
    "**Zusammenfassung der EDA-Erkenntnisse**\n",
    "\n",
    "- Der Grossteil der Einträge (ca. **84 %**) trägt den Typ `owner`, der Rest `disponent`.\n",
    "- Es existieren zwei Rollen, über die Kund:innen einem Konto zugeordnet sein können:\n",
    "  - **owner** → Kontoinhaber:in mit Entscheidungsbefugnis\n",
    "  - **disponent** → Mitnutzer:in ohne volle Rechte\n",
    "\n",
    "**Offene Punkte und nächste Schritte**\n",
    "\n",
    "- **Eindeutigkeit von `client_id` nach Datenfusion prüfen:**  \n",
    "  Aktuell ist jede `client_id` eindeutig. Es ist zu prüfen, ob dies auch nach dem Join mit weiteren Tabellen (z. B. `card`, `account`, `loan`) erhalten bleibt.\n",
    "\n",
    "- **Validierung der Rolle im Kontext der Modellierung:**  \n",
    "  Sollte sich bestätigen, dass nur `owner`-Einträge eine Kreditkarte besitzen, können `disponent`-Einträge ausgeschlossen werden. Ansonsten ist zu klären, ob deren Verhalten separat analysiert werden sollte.\n",
    "\n",
    "- **Prüfen, ob es pro `account_id` mehrere `disp_id`-Einträge mit verschiedenen Rollen gibt:**  \n",
    "  → Das ist besonders wichtig, um zu verstehen, ob ein Konto gemeinsam genutzt wird (z. B. `owner` + `disponent`). In diesem Fall müssten Features pro Konto aggregiert oder differenziert behandelt werden.\n",
    "\n",
    "-  **Vorbereitung eines binären Features zur Rolle (`is_owner`)**  \n",
    "  → Dieses kann beim späteren Modelltraining nützlich sein, um zwischen Haupt- und Nebenrollen zu unterscheiden.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_1_3_'></a>[Accounts (`accounts_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcae48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd48df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Kontodaten mit spezifizierten Datentypen für IDs und Frequenz\n",
    "# sowie Parsing der Spalte 'date' als Datum im Format \"%y%m%d\"\n",
    "accounts_df = pd.read_csv(\"xselling_banking_data/account.csv\", delimiter=\";\", dtype={\n",
    "    \"account_id\": \"int64\",\n",
    "    \"district_id\": \"int64\",\n",
    "    \"frequency\": \"category\"}, \n",
    "    parse_dates=[\"date\"], date_format=\"%y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f97c2f",
   "metadata": {},
   "source": [
    "Die Spaltentypen sehen korrekt aus, wie sie eingelesen wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca06bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc15fd",
   "metadata": {},
   "source": [
    "Der Datensatz enthält keine Nullwerte "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47721eb9",
   "metadata": {},
   "source": [
    "Aus der Datensatzbeschreibung geht hervor, dass es drei Typen für die Häufigkeit der Kontoauszüge gibt:\n",
    "\n",
    "- POPLATEK MESICNE = monthly_issuance\n",
    "- POPLATEK TYDNE =  weekly_issuance\"\n",
    "- POPLATEK PO OBRATU =   issuance_after_ transaction\n",
    "\n",
    "Die tschechischen Bezeichnungen in den Spalten werden ins Englische übersetzt, um eine einheitliche und konsistente Benennung innerhalb der Analyse sicherzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df['frequency'] = accounts_df['frequency'].map({\n",
    "    \"POPLATEK MESICNE\": \"monthly_issuance\",\n",
    "    \"POPLATEK TYDNE\": \"weekly_issuance\",\n",
    "    \"POPLATEK PO OBRATU\": \"issuance_after_transaction\"\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df[\"frequency\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc5419",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17330bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c635428",
   "metadata": {},
   "source": [
    "Ergebnis aus describe()):\n",
    "\n",
    " - Es gibt 4'500 Konten (account_id), verteilt auf 77 Distrikte (district_id)\n",
    "- Die Spalte 'frequency' enthält 3 Werte → häufigster: 'POPLATEK MESICNE' (monatlich, 4167×)\n",
    "- Das Kontoeröffnungsdatum ('date') reicht von 01.01.1993 bis 29.12.1997\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df['account_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c58a0",
   "metadata": {},
   "source": [
    "- account_id ist  eindeutig und eignet sich daher als Primärschlüssel der Tabelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df['district_id'].value_counts(normalize=True).mul(100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b1da4",
   "metadata": {},
   "source": [
    "Die Analyse zeigt, dass die Konten auf insgesamt 77 Distrikte verteilt sind.\n",
    "Die Verteilung ist dabei deutlich ungleichmässig:\n",
    "\n",
    "- Der Distrikt mit der ID 1 enthält 12.3 % aller Konten und ist damit mit Abstand am stärksten vertreten.\n",
    "- Die Mehrheit der übrigen Distrikte liegt jeweils unter 4 % Anteil, viele sogar unter 1 %.\n",
    "\n",
    " Dies deutet auf eine konzentrierte Kundenbasis in wenigen Regionen hin ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df['frequency'].value_counts(normalize=True).mul(100).round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead121fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Anzahl und prozentualen Anteile der verschiedenen Frequenzwerte in den Kontodaten\n",
    "type_counts_account = accounts_df['frequency'].value_counts().reset_index(name='count')\n",
    "type_counts_account.columns = ['frequency', 'count'] \n",
    "type_counts_account['percent'] = (type_counts_account['count'] / type_counts_account['count'].sum() * 100).round(2)\n",
    "\n",
    "# Balkendiagramm zur Darstellung der Verteilung der Frequenzen\n",
    "fig = px.bar(\n",
    "    type_counts_account,\n",
    "    x='frequency',\n",
    "    y='percent',\n",
    "    text_auto=True,\n",
    "    color='frequency',\n",
    "    hover_data={\"count\": True}\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Frequencies\",\n",
    "    xaxis_title=\"Frequency\",\n",
    "    yaxis_title=\"Percentage (%)\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadd72a",
   "metadata": {},
   "source": [
    "Die Auswertung zeigt deutlich, dass monatliche Ausszüge dominieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df[['account_id', 'date']].sort_values('date').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e402d94",
   "metadata": {},
   "source": [
    "Zeitliche Reihenfolge vs. ID-Reihenfolge\n",
    "\n",
    "- Überprüfung, ob account_id chronologisch mit dem Ausgabedatum vergeben wurde\n",
    "- Ergebnis: IDs sind nicht strikt aufsteigend zur Zeit → keine zeitliche Logik in der Vergabe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0897988e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba75cb8",
   "metadata": {},
   "source": [
    "**Entitäten**\n",
    "\n",
    "- `account_id` ist der **Primärschlüssel** der Tabelle `account`.\n",
    "- `district_id` ist ein **Fremdschlüssel** zur Tabelle `district`.\n",
    "\n",
    "**Zusammenfassung der EDA-Erkenntnisse**\n",
    "\n",
    "- Die **Kontofrequenz** (Ausstellung der Kontoauszüge) ist bei über **93 % der Konten als `monthly_issuance`** klassifiziert.\n",
    "- Der **Distrikt mit `district_id = 1`** weist mit **12.3 % aller Konten** die höchste Kontendichte auf.\n",
    "- Es besteht **kein linearer Zusammenhang zwischen `account_id` und dem Eröffnungsdatum (`date`)**, d. h. die ID ist nicht zeitlich sortiert.\n",
    "- Die **Distriktzugehörigkeit** (über `district_id`) könnte relevant sein, weil bestimmte Distrikte mehr Konten aufweisen und ggf. andere Kauf- oder Kreditkartenmuster zeigen.\n",
    "\n",
    "\n",
    "**Offene Punkte und nächste Schritte**\n",
    "-  **Regionale Differenzierung**: Prüfen, ob es Distrikte gibt, die signifikant mehr (oder weniger) Kreditkarten besitzen, um sie gezielt zu analysieren\n",
    "- **Nutzungsfrequenz-Analyse**: Untersuchen, ob bestimmte Frequenzgruppen (`weekly`, `yearly`) tatsächlich seltener Karten haben (oder ob kein Zusammenhang besteht).\n",
    "- **Verknüpfung mit anderen Tabellen**  \n",
    "Zusammenführen der Kontodaten (`account_df`) mit `district_df` (z. B. soziodemografische Merkmale) und ggf. `card_df`, um das Zusammenspiel von Distrikt, Frequenz und Kartenbesitz zu analysieren.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_1_4_'></a>[Districts (`district_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d93c34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51e5e8",
   "metadata": {},
   "source": [
    "- Während des Einlesens der Daten wurde festgestellt, dass in gewissen Spalten ein \"?\" als Wert hinterlegt ist. Dies wurde beim Einlesen entsprechend berücksichtigt und behandelt.\n",
    "- Da die Spaltenbezeichnungen nicht aussagend sind, werden diese entsprechend angepasst.\n",
    "- Zudem wurden gewisse Spalten mit dem korrekten Format angegeben/umgewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f007b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Bezirksdaten (ohne spezielle Datentypangaben)\n",
    "district_df = pd.read_csv(\n",
    "    \"xselling_banking_data/district.csv\",\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16656f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c565ad5",
   "metadata": {},
   "source": [
    "Die Spaltennamen ändern wir sodass diese besser interpretierbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df.columns = [\n",
    "    'district_id', \n",
    "    'district_name', \n",
    "    'region', \n",
    "    'n_inhabitants', \n",
    "    'n_municipals_lower_499', \n",
    "    'n_municipals_between_500_1999', \n",
    "    'n_municipals_between_2000_9999', \n",
    "    'n_municipals_higher_10000', \n",
    "    'n_cities', \n",
    "    'ratio_urban_inhabitants', \n",
    "    'avg_salary', \n",
    "    'unemployment_rate_1995', \n",
    "    'unemployment_rate_1996', \n",
    "    'n_enterpreneurs_per_1k_inhabitants', \n",
    "    'n_crimes_1995', \n",
    "    'n_crimes_1996'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5704788",
   "metadata": {},
   "source": [
    "Die meisten Spalten werden bereits korrekt eingelesen. Jedoch scheint es bei den Spalten \"unemployment_rate_1995\" und \"n_crimes_1995\" fehlerhafte Werte zu geben. Die Spalten werden als \"object\" eingelesen obwohl diese numerische Werte aufweisen. Wir untersuchen nun den möglichen Grund hierfür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1390cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df[\"n_crimes_1995\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df[district_df == \"?\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df['district_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144650a8",
   "metadata": {},
   "source": [
    "Die Vermutung war korrekt. In gewissen Spalten finden sich \"?\" als Werte. Wir werden diese nun mit NaN Werten ersetzen. Zudem passen wir noch weitere Spalten an sodass der Typ klarer ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99596f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehlende Werte in 'unemployment_rate_1995' und 'n_crimes_1995' durch NaN ersetzen\n",
    "district_df['unemployment_rate_1995'] = district_df['unemployment_rate_1995'].replace(\"?\", np.nan)\n",
    "district_df['n_crimes_1995'] = district_df['n_crimes_1995'].replace(\"?\", np.nan)\n",
    "\n",
    "# Umwandlung der Spalten in numerische Datentypen\n",
    "district_df['unemployment_rate_1995'] = district_df['unemployment_rate_1995'].apply(pd.to_numeric)\n",
    "district_df['n_crimes_1995'] = district_df['n_crimes_1995'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df789807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandlung der Spalten 'district_name' und 'region' in den String-Datentyp\n",
    "district_df['district_name'] = district_df['district_name'].astype('string')\n",
    "district_df['region'] = district_df['region'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba61c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a582001",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297a431",
   "metadata": {},
   "source": [
    "district_id ist eindeutig und eignet sich daher als Primärschlüssel der Tabelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fe7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e3aa0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b860b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Anzahl und prozentualen Anteile der Bezirke pro Region\n",
    "type_counts = district_df[\"region\"].value_counts().reset_index()\n",
    "type_counts.columns = ['region', 'count']\n",
    "\n",
    "type_counts['percent'] = ((type_counts['count'] / type_counts['count'].sum()) * 100).round(1)\n",
    "\n",
    "# Balkendiagramm zur Verteilung der Bezirke auf die Regionen\n",
    "fig = px.bar(\n",
    "    type_counts, \n",
    "    x='region', \n",
    "    y='percent', \n",
    "    text_auto='.1f', \n",
    "    color='region', \n",
    "    hover_data=['count'],\n",
    "    title='Distribution of Districts to Regions',\n",
    "    labels={'region': 'Region', 'percent': 'Percentage (%)'}\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5ca5d",
   "metadata": {},
   "source": [
    "Die meisten Regions weisen die ungefähre gleiche Anzahl an Districts auf. Nur Prag hat nur einen District."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summe der Einwohner pro Region berechnen und prozentualen Anteil an Gesamtbevölkerung ermitteln\n",
    "region_population = district_df.groupby('region')['n_inhabitants'].sum().reset_index()\n",
    "region_population['inhabitant_percentage'] = (region_population['n_inhabitants'] / region_population['n_inhabitants'].sum()) * 100\n",
    "\n",
    "# Sortierung der Regionen nach Bevölkerungsanteil (absteigend)\n",
    "region_population = region_population.sort_values(by='inhabitant_percentage', ascending=False)\n",
    "\n",
    "# Balkendiagramm zur Darstellung des Bevölkerungsanteils je Region\n",
    "fig = px.bar(\n",
    "    region_population, \n",
    "    x='region', \n",
    "    y='inhabitant_percentage', \n",
    "    title='Share of Total Population by Region (%)',\n",
    "    labels={'region': 'Region', 'inhabitant_percentage': 'Population Share (%)'},\n",
    "    text_auto='.2f',\n",
    "    color='region'\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a6f17",
   "metadata": {},
   "source": [
    "Bei der Einwohnerzahl sieht es schon anders aus: Prag hat trotz des einzigen Districts eine ähnliche Einwohnerzahl wie jene Regions mit vielen Districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397679f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswahl der Spalten zu Gemeinden verschiedener Grössenklassen je Region\n",
    "municipals_df = district_df[['region', 'n_municipals_lower_499', 'n_municipals_between_500_1999', \n",
    "                             'n_municipals_between_2000_9999', 'n_municipals_higher_10000']]\n",
    "\n",
    "# Umwandlung in ein langes Format für gestapeltes Balkendiagramm\n",
    "municipals_df_long = municipals_df.melt(id_vars='region', \n",
    "                                         value_vars=['n_municipals_lower_499', 'n_municipals_between_500_1999', \n",
    "                                                     'n_municipals_between_2000_9999', 'n_municipals_higher_10000'],\n",
    "                                         var_name='municipal_type', value_name='municipal_count')\n",
    "\n",
    "# Berechnung des prozentualen Anteils der Gemeinden je Region\n",
    "municipals_df_long['percent'] = municipals_df_long.groupby('region')['municipal_count'].transform(lambda x: x / x.sum() * 100)\n",
    "\n",
    "# Gestapeltes Balkendiagramm zur Verteilung der Gemeindegrössenklassen je Region\n",
    "fig = px.bar(\n",
    "    municipals_df_long, \n",
    "    x='region', \n",
    "    y='percent', \n",
    "    color='municipal_type', \n",
    "    title='Percentage Distribution of Municipal Counts by Region',\n",
    "    labels={'percent': 'Percentage (%)', 'region': 'Region', 'municipal_type': 'Municipal Type'},\n",
    "    barmode='stack'\n",
    ")\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.update_layout(\n",
    "    xaxis_title='Region',\n",
    "    yaxis_title='Percentage (%)',\n",
    "    xaxis=dict(tickangle=45)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summe der Städte pro Region berechnen und absteigend sortieren\n",
    "region_cities = district_df.groupby('region')['n_cities'].sum().reset_index()\n",
    "region_cities = region_cities.sort_values(by='n_cities', ascending=False)\n",
    "\n",
    "# Balkendiagramm zur Darstellung der Gesamtzahl der Städte je Region\n",
    "fig = px.bar(\n",
    "    region_cities, \n",
    "    x='region', \n",
    "    y='n_cities', \n",
    "    title='Total Number of Cities by Region',\n",
    "    labels={'region': 'Region', 'n_cities': 'Total Number of Cities'},\n",
    "    color='region',\n",
    "    text_auto=True\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchschnittliches Gehalt pro Region berechnen und absteigend sortieren\n",
    "region_avg_salary = district_df.groupby('region')['avg_salary'].mean().reset_index()\n",
    "region_avg_salary = region_avg_salary.sort_values(by='avg_salary', ascending=False)\n",
    "\n",
    "# Balkendiagramm zur Darstellung des durchschnittlichen Gehalts je Region\n",
    "fig = px.bar(\n",
    "    region_avg_salary, \n",
    "    x='region', \n",
    "    y='avg_salary', \n",
    "    title='Average Salary by Region',\n",
    "    labels={'region': 'Region', 'avg_salary': 'Average Salary'},\n",
    "    color='region',\n",
    "    text_auto='.1f'\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45897a",
   "metadata": {},
   "source": [
    "Prag hat ganz klar den höchsten durschnittlichen salary. Die restlichen Regions sind ca. uniform verteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchschnittliche Anzahl Unternehmer pro 1.000 Einwohner je Region berechnen\n",
    "region_n_enterpreneurs_per_1k = district_df.groupby('region')['n_enterpreneurs_per_1k_inhabitants'].mean().reset_index()\n",
    "\n",
    "# Prozentualer Anteil an der Gesamtzahl der Unternehmer pro 1.000 Einwohner berechnen\n",
    "region_n_enterpreneurs_per_1k['n_enterpreneurs_percentage'] = (\n",
    "    region_n_enterpreneurs_per_1k['n_enterpreneurs_per_1k_inhabitants'] /\n",
    "    region_n_enterpreneurs_per_1k['n_enterpreneurs_per_1k_inhabitants'].sum()\n",
    ") * 100\n",
    "\n",
    "# Sortierung nach der durchschnittlichen Unternehmerzahl pro 1.000 Einwohner (absteigend)\n",
    "region_n_enterpreneurs_per_1k = region_n_enterpreneurs_per_1k.sort_values(by='n_enterpreneurs_per_1k_inhabitants', ascending=False)\n",
    "\n",
    "# Balkendiagramm zur Darstellung des Anteils der Unternehmer je Region\n",
    "fig = px.bar(\n",
    "    region_n_enterpreneurs_per_1k, \n",
    "    x='region', \n",
    "    y='n_enterpreneurs_percentage', \n",
    "    title='Share of Total Entrepreneurs per 1,000 Inhabitants by Region (%)',\n",
    "    labels={\n",
    "        'region': 'Region',\n",
    "        'n_enterpreneurs_percentage': 'Entrepreneur Share (%)'\n",
    "    },\n",
    "    color='region',\n",
    "    text_auto='.2f'\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d42e7",
   "metadata": {},
   "source": [
    "Prag hat auch die höchste Anzahl an Unternehmern pro 1k Einwohner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6642ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchschnittliche Arbeitslosenquoten 1995 und 1996 je Region berechnen\n",
    "region_unemployment_rate = district_df.groupby('region')[['unemployment_rate_1995', 'unemployment_rate_1996']].mean().reset_index()\n",
    "\n",
    "# Gruppiertes Balkendiagramm zur Darstellung der Arbeitslosenquoten je Region für 1995 und 1996\n",
    "fig = px.bar(\n",
    "    region_unemployment_rate, \n",
    "    x='region', \n",
    "    y=['unemployment_rate_1995', 'unemployment_rate_1996'], \n",
    "    title='Unemployment Rates 1995 and 1996 by Region',\n",
    "    labels={'region': 'Region', 'value': 'Unemployment Rate'},\n",
    "    barmode='group',\n",
    "    text_auto='.2f'\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summe der Straftaten 1995 und 1996 je Region berechnen\n",
    "region_n_crimes = district_df.groupby('region')[['n_crimes_1995', 'n_crimes_1996']].sum().reset_index()\n",
    "\n",
    "# Gruppiertes Balkendiagramm zur Darstellung der Anzahl der Straftaten je Region für 1995 und 1996\n",
    "fig = px.bar(\n",
    "    region_n_crimes, \n",
    "    x='region', \n",
    "    y=['n_crimes_1995', 'n_crimes_1996'], \n",
    "    title='Number of Crimes 1995 and 1996 by Region',\n",
    "    labels={'region': 'Region', 'value': 'Number of Crimes'},\n",
    "    barmode='group',\n",
    "    text_auto='.0f'\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10911cb",
   "metadata": {},
   "source": [
    "Wir normalisieren die absoluten Zahlen der Anzahl Kriminalität auf pro 1000 Einwohner. So ist der Plot aussagekräftiger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summe der Straftaten und Einwohner je Region berechnen\n",
    "region_n_crimes = district_df.groupby('region')[\n",
    "    ['n_crimes_1995', 'n_crimes_1996', 'n_inhabitants']\n",
    "].sum().reset_index()\n",
    "\n",
    "# Berechnung der Anzahl der Straftaten pro 1000 Einwohner für 1995 und 1996\n",
    "region_n_crimes['n_crimes_1995_per_1000'] = (region_n_crimes['n_crimes_1995'] / region_n_crimes['n_inhabitants']) * 1000\n",
    "region_n_crimes['n_crimes_1996_per_1000'] = (region_n_crimes['n_crimes_1996'] / region_n_crimes['n_inhabitants']) * 1000\n",
    "\n",
    "# Gruppiertes Balkendiagramm zur Darstellung der Straftaten pro 1000 Einwohner je Region\n",
    "fig = px.bar(\n",
    "    region_n_crimes,\n",
    "    x='region',\n",
    "    y=['n_crimes_1995_per_1000', 'n_crimes_1996_per_1000'],\n",
    "    title='Number of Crimes per 1000 Inhabitants (1995 and 1996) by Region',\n",
    "    labels={'region': 'Region', 'value': 'Crimes per 1000 Inhabitants'},\n",
    "    barmode='group',\n",
    "    text_auto='.2f'\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6754fc",
   "metadata": {},
   "source": [
    "Prag hat die höchste Kriminalität per 1000 EW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2eeaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a46ae",
   "metadata": {},
   "source": [
    "**Entitäten**\n",
    "\n",
    "- district_id ist Primärschlüssel\n",
    "- Keine Fremdschlüssel zu anderen Datensätzen\n",
    "\n",
    "**Fazit EDA**\n",
    "\n",
    "- Prag sticht als Metropole klar heraus. Die restlichen Regionen sind im Vergleich eher ländlich geprägt (mit einer gewissen Varianz).\n",
    "- Prag hat einen überdurchschnittlichen:\n",
    "    - average salary\n",
    "    - Unternehmer/Einwohner Ratio\n",
    "    - Kriminalitätsrate\n",
    "- Zudem hat Prag eine (im Vergleich) sehr niedrige Arbeitslosenrate.\n",
    "- Der Vergleich der Jahre 1995 und 1996 zeigt einen Anstieg (oder gleichbleibend) an Kriminalität und Arbeitslosigkeit.\n",
    "\n",
    "**Offene Punkte und nächste Schritte:**\n",
    "\n",
    "- **Regionale Merkmale als Prädiktoren einbeziehen:**  \n",
    "  Regionale Differenzierung: Gibt es Distrikte, die signifikant mehr (oder weniger) Kreditkarten besitzen?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_1_5_'></a>[Clients (`client_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2f364",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Kundendaten mit spezifizierten Datentypen\n",
    "# Hinweis: 'birth_number' wird als String eingelesen, um weitere Verarbeitung zu ermöglichen\n",
    "client_df = pd.read_csv(\n",
    "    \"xselling_banking_data/client.csv\",\n",
    "    sep=\";\",\n",
    "    dtype={\n",
    "        \"client_id\": \"int64\",\n",
    "        \"district_id\": \"int64\",\n",
    "        \"birth_number\": \"string\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d33374",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0146955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number is in the form YYMMDD for men,\n",
    "#the number is in the form YYMM+50DD for women,\n",
    "#where YYMMDD is the date of birth\n",
    "\n",
    "# Extrahieren von Jahr, Monat und Tag aus 'birth_number'\n",
    "# Für Frauen wird zum Monat 50 addiert, daher Korrektur durch Abzug von 50, falls Monat > 12\n",
    "year = client_df[\"birth_number\"].str.slice(0, 2)\n",
    "month_true = client_df[\"birth_number\"].str.slice(2, 4).astype(int).apply(lambda x: x - 50 if x > 12 else x)\n",
    "day = client_df[\"birth_number\"].str.slice(4, 6)\n",
    "\n",
    "# Geschlecht basierend auf Monatsteil bestimmen: Monat > 12 -> Frau (F), sonst Mann (M)\n",
    "client_df[\"gender\"] = client_df[\"birth_number\"].str.slice(2, 4).astype(int).apply(lambda x: \"F\" if x > 12 else \"M\")\n",
    "\n",
    "# Monat mit führender Null auffüllen und zusammenfügen\n",
    "month = month_true.astype(str).str.zfill(2)\n",
    "birth_fixed = year + month + day\n",
    "\n",
    "# Ausgabe der originalen und korrigierten Geburtsnummern der ersten fünf Kunden\n",
    "print(pd.DataFrame({\n",
    "    \"original\": client_df[\"birth_number\"].head(),\n",
    "    \"fixed\": birth_fixed.head()\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Korrektur des Jahres (Jahr wird mit \"19\" ergänzt)\n",
    "def correct_year(year):\n",
    "    return \"19\" + year\n",
    "\n",
    "# Korrigierte Geburtsnummer mit vollständigem Jahr (z.B. \"19930115\")\n",
    "birth_fixed_corrected = birth_fixed.apply(lambda x: correct_year(x[:2]) + x[2:])\n",
    "\n",
    "# Umwandlung in datetime-Format, fehlerhafte Werte werden als NaT gesetzt\n",
    "client_df[\"birth_number\"] = pd.to_datetime(birth_fixed_corrected, format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# Ausgabe der ersten Zeilen zur Kontrolle\n",
    "print(client_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548e773",
   "metadata": {},
   "source": [
    "Für das weitere Vorgehen, ist es nützlich eine Spalte einzuführen, welche das absolute Alter aufführt. Hierzu verwenden wir das aktuellste Datum, welches irgendwo in den Daten vorkommt. Dieses Datum verwenden wir dann als das Ende des Datensatzes an und berechnen hiermit das Alter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ec967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximaldatum aus bereits geladenen DataFrames\n",
    "max_accounts_date = accounts_df['date'].max()\n",
    "max_card_date = credit_card_df['issued'].max()\n",
    "\n",
    "# Nur Datumsspalte aus trans.csv einlesen\n",
    "trans_date = pd.read_csv(\"xselling_banking_data/trans.csv\", delimiter=\";\", usecols=[\"date\"], parse_dates=[\"date\"])\n",
    "max_trans_date = trans_date[\"date\"].max()\n",
    "\n",
    "# Nur Datumsspalte aus loan.csv einlesen\n",
    "loan_date = pd.read_csv(\"xselling_banking_data/loan.csv\", delimiter=\";\", usecols=[\"date\"], parse_dates=[\"date\"])\n",
    "max_loan_date = loan_date[\"date\"].max()\n",
    "\n",
    "# Gesamtmaximaldatum bestimmen\n",
    "latest_date = max(max_accounts_date, max_card_date, max_trans_date, max_loan_date)\n",
    "\n",
    "# Alter berechnen\n",
    "client_df['birth_number'] = pd.to_datetime(client_df['birth_number'], errors='coerce')\n",
    "client_df['age'] = (latest_date - client_df['birth_number']).dt.days // 365"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579b5ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gruppierung der Kunden nach Alter und Geschlecht mit Zählung der Personen pro Gruppe\n",
    "year_gender_counts = client_df.groupby(['age', 'gender']).size().reset_index(name='count')\n",
    "\n",
    "# Horizontales Balkendiagramm zur Verteilung des Alters nach Geschlecht\n",
    "fig = px.bar(\n",
    "    year_gender_counts, \n",
    "    x='count', \n",
    "    y='age', \n",
    "    color='gender', \n",
    "    orientation='h',\n",
    "    title='Distribution of Age by Gender',\n",
    "    labels={'age': 'Age', 'count': 'Count'},\n",
    "    color_discrete_map={\"M\": \"blue\", \"F\": \"red\"},\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Negative Werte für Männer (erste Gruppe) zum Plotten auf der linken Seite\n",
    "fig.data[0].x = -fig.data[0].x\n",
    "\n",
    "# Balkenkontur mit schwarzer Linie für bessere Abgrenzung\n",
    "fig.update_traces(\n",
    "    marker=dict(line=dict(width=2, color='black')) \n",
    ")\n",
    "\n",
    "# Keine Textbeschriftung auf den Balken\n",
    "fig.update_traces(textposition='none')\n",
    "\n",
    "# Layout-Anpassungen für Achsentitel, Legende und Tick-Intervalle\n",
    "fig.update_layout(\n",
    "    xaxis_title='Count',\n",
    "    yaxis_title='Age',\n",
    "    showlegend=True, \n",
    "    legend_title='Gender',  \n",
    "    xaxis=dict(tickmode='linear', dtick=10),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf2923",
   "metadata": {},
   "source": [
    "Kundenverteilung folgt einer typischen Bevölkerungsverteilung eines Industrielandes (breite Basis und abfallende Geburtenrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6fbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kundenanzahl pro Bezirk gruppieren und Prozentanteil berechnen\n",
    "clients_per_district = client_df.groupby('district_id').size().reset_index(name='count')\n",
    "clients_per_district['percentage'] = (clients_per_district['count'] / clients_per_district['count'].sum() * 100).round(1)\n",
    "\n",
    "clients_per_district['district_id'] = clients_per_district['district_id'].astype(str)\n",
    "clients_per_district_sorted = clients_per_district.sort_values(by='count', ascending=False)\n",
    "\n",
    "fig = px.bar(\n",
    "    clients_per_district_sorted, \n",
    "    x='district_id', \n",
    "    y='count',\n",
    "    title='Anzahl der Kunden pro Bezirk',\n",
    "    labels={'district_id': 'Bezirk', 'count': 'Anzahl der Kunden'},\n",
    "    color='percentage'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56858963",
   "metadata": {},
   "source": [
    "Die Verteilung der Kundenanzahl pro Bezirk zeigt eine starke Konzentration der Kunden im Bezirk mit der ID 1 (12 %), der mit über 600 Kunden den höchsten Anteil hat. Weitere Bezirke folgen mit deutlich geringeren Kundenanzahlen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492c61a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27232961",
   "metadata": {},
   "source": [
    "**Schlüsselstruktur und Datenmodell**\n",
    "\n",
    "- client_id ist Primärschlüssel\n",
    "- district_id ist Fremdschlüssel zum Datensatz district_df\n",
    "\n",
    "**Zusammenfassung der EDA Erkenntnisse**\n",
    "\n",
    "- Frauen und Männer sind ähnlich häufig verteilt.\n",
    "- Durchschnittsalter ist 45. Der jüngste Kunde ist 11 und der älteste ist 87.\n",
    "- Kundenverteilung folgt einer typischen Bevölkerungsverteilung eines Industrielandes (breite Basis und abfallende Geburtenrate)\n",
    "- Gegen hohes Alter sinken die Kundenbeobachtungen logischerweise.\n",
    "- Die Verteilung der Kundenanzahl pro Bezirk zeigt eine starke Konzentration der Kunden im Bezirk mit der ID 1 (12 %), der mit über 600 Kunden den höchsten Anteil hat. Weitere Bezirke folgen mit deutlich geringeren Kundenanzahlen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e4f23",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_2_'></a>[Dynamische Entitäten](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_2_1_'></a>[Permanent Order (`order_df`)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db35b2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a89c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daueraufträge einlesen mit spezifizierten Datentypen\n",
    "order_df = pd.read_csv(\n",
    "    \"xselling_banking_data/order.csv\",\n",
    "    sep=\";\",\n",
    "    dtype={\n",
    "        \"order_id\": \"int64\",\n",
    "        \"account_id\": \"int64\",\n",
    "        \"bank_to\": \"string\",\n",
    "        \"account_to\": \"string\",\n",
    "        \"amount\": \"float64\",\n",
    "        \"k_symbol\": \"category\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ca9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ebc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e02819",
   "metadata": {},
   "source": [
    "Die Datentypen wurden korrekt eingelesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea031e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ca8de",
   "metadata": {},
   "source": [
    "Die Daten weisen keine NaN Werte auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df95901",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df['order_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27319c3",
   "metadata": {},
   "source": [
    "Die Spalte order_id weist nur unique Werte auf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f228de",
   "metadata": {},
   "source": [
    "Die Datenbeschreibung gibt uns die Übersetzungen für die Werte der Variable k_symbol.\n",
    "\n",
    "- POJISTNE = insurance_payment\n",
    "- SIPO = household_payment\n",
    "- LEASING = leasing_payment\n",
    "- UVER = loan_payment\n",
    "\n",
    "Wir übersetzen bzw. nennen diese Werte nun um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58041865",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df['k_symbol'] = order_df['k_symbol'].map({\n",
    "    \"POJISTNE\": \"insurance_payment\",\n",
    "    \"SIPO\": \"household_payment\",\n",
    "    \"LEASING\": \"leasing_payment\",\n",
    "    \"UVER\" : \"loan_payment\"\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a2264",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3fcc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48af0b",
   "metadata": {},
   "source": [
    "- Der mittlere Wert einer permanent order beträgt 3281. Der Datensatz weist 6471 orders auf.\n",
    "- **bank_to**: Es gibt **13 verschiedene Banken**, wobei \"QR\" (10807258) am häufigsten vorkommt (531 Mal).\n",
    "- **k_symbol**: Dieser Code beschreibt den Verwendungszweck der Bestellung und ist häufig mit \"household_payment\" (3.502 Einträge) zu finden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Häufigkeiten und Prozentanteile der Kategorien in 'k_symbol'\n",
    "type_counts_k_symbol = order_df['k_symbol'].value_counts().reset_index(name='count')\n",
    "type_counts_k_symbol.columns = ['k_symbol', 'count']\n",
    "type_counts_k_symbol = type_counts_k_symbol.sort_values('k_symbol')\n",
    "type_counts_k_symbol['percent'] = (type_counts_k_symbol['count'] / type_counts_k_symbol['count'].sum() * 100).round(2)\n",
    "\n",
    "# Balkendiagramm der Verteilung der 'k_symbol' Kategorien\n",
    "fig = px.bar(\n",
    "    type_counts_k_symbol,\n",
    "    x='k_symbol',\n",
    "    y='percent',\n",
    "    text_auto=True,\n",
    "    hover_data={\"count\": True},\n",
    "    color='k_symbol'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='K Symbol',\n",
    "    yaxis_title='Percent',\n",
    "    xaxis=dict(type='category')\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6939c",
   "metadata": {},
   "source": [
    "household payments dominieren mit 69%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subplot mit Boxplot und Histogramm zur Verteilung der Auftragsbeträge erstellen\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    vertical_spacing=0.02)\n",
    "\n",
    "# Boxplot oben\n",
    "fig.add_trace(go.Box(x=order_df[\"amount\"], name=\"Boxplot\", marker_color=\"blue\", orientation='h'), row=1, col=1)\n",
    "\n",
    "# Histogramm unten\n",
    "fig.add_trace(go.Histogram(x=order_df[\"amount\"], nbinsx=100, name=\"Histogramm\", marker_color=\"blue\"), row=2, col=1)\n",
    "\n",
    "# Layout konfigurieren\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Distribution of Order amount\",\n",
    "    showlegend=True,\n",
    "    xaxis_title=\"Order Amount (CZK)\",\n",
    "    xaxis2_title=\"Order Amount (CZK)\",\n",
    "    bargap=0.05\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4c75b",
   "metadata": {},
   "source": [
    "Die Verteilung der Besellbeträge ist rechtsschief, mit einer Häufung der Bestellungen im Bereich von 1.000 bis 5.000 CZK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c7ed5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b9f1b",
   "metadata": {},
   "source": [
    "**Schlüsselstruktur und Datenmodell**\n",
    "\n",
    "- order_id ist Primärschlüssel\n",
    "- account_id ist Fremdschlüssel zu Datensatz account_df\n",
    "\n",
    "**Fazit EDA**\n",
    "\n",
    "- Die Verteilung der Besellbeträge ist rechtsschief, mit einer Häufung der Bestellungen im Bereich von 1.000 bis 5.000 CZK\n",
    "- household payments dominieren klar mit 69%. Die restlichen permanten order Gründe sind ziemlich uniform bei ca. 10% (+/- 4%). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_2_2_'></a>[Loan (`loan_df`)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e77df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Darlehensdaten mit spezifizierten Datentypen und Datumsformat\n",
    "loan_df = pd.read_csv(\n",
    "    \"xselling_banking_data/loan.csv\",\n",
    "    sep=\";\",\n",
    "    dtype={\n",
    "        \"loan_id\": \"int64\",\n",
    "        \"account_id\": \"int64\",\n",
    "        \"amount\": \"float64\",\n",
    "        \"duration\": \"int64\",\n",
    "        \"payments\": \"float64\",\n",
    "        \"status\": \"category\"\n",
    "    },\n",
    "    parse_dates=[\"date\"],\n",
    "    date_parser=lambda x: pd.to_datetime(x, format=\"%y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a487a9",
   "metadata": {},
   "source": [
    "Die Spaltentypen sehen korrekt aus, wie sie eingelesen wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbec43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d527d7c",
   "metadata": {},
   "source": [
    "Der Datensatz enthält keine Nullwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['loan_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3917e",
   "metadata": {},
   "source": [
    "- Die Spalte disp_id ist eindeutig und dient als Primärschlüssel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeacfc9",
   "metadata": {},
   "source": [
    "Um die Spalte status leichter interpretieren zu können, benennen wir sie um mit der korrekten Bedeutung.\n",
    "\n",
    "- A = finished_ok\n",
    "- B = finished_debts\n",
    "- C = running_ok\n",
    "- D = running_debts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['status'] = loan_df['status'].map({\n",
    "    \"A\" : \"finished_ok\",\n",
    "    \"B\" : \"finished_debts\",\n",
    "    \"C\" : \"running_ok\",\n",
    "    \"D\" : \"running_debts\"\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab0aa6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d478f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot mit Boxplot und Histogramm zur Verteilung der Darlehensbeträge erstellen\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    vertical_spacing=0.02)\n",
    "\n",
    "# Boxplot oben\n",
    "fig.add_trace(go.Box(x=loan_df[\"amount\"], name=\"Boxplot\", marker_color=\"red\", orientation='h'), row=1, col=1)\n",
    "\n",
    "# Histogramm unten\n",
    "fig.add_trace(go.Histogram(x=loan_df[\"amount\"], nbinsx=100, name=\"Histogramm\", marker_color=\"red\"), row=2, col=1)\n",
    "\n",
    "# Layout konfigurieren\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Distribution of Loan Amounts\",\n",
    "    showlegend=True,\n",
    "    xaxis_title=\"Loan Amount (CZK)\",\n",
    "    xaxis2_title=\"Loan Amount (CZK)\",\n",
    "    bargap=0.05\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25171458",
   "metadata": {},
   "source": [
    "\n",
    "- **Verteilung**: Die Kreditbeträge sind meist im unteren bis mittleren Bereich konzentriert, während nur wenige sehr grosse Kredite die Verteilung nach oben verzerren und so eine rechtsschiefe Verteilung erzeugen.\n",
    "\n",
    "\n",
    "- **Boxplot**:\n",
    "\n",
    "  - Der Median der Kreditbeträge liegt bei 116.928 CZK.\n",
    "  - 75 % der Kredite liegen unter 210.744 CZK.\n",
    "  - Der untere Whisker endet bei 4.980 CZK, was auf einen relativ niedrigen Bereich hinweist.\n",
    "  - Der obere Whisker endet bei 421.008 CZK und repräsentiert den oberen Bereich der normalen Kreditbeträge.\n",
    "  - Es gibt Ausreisser im oberen Bereich mit einem Maximum von 590.824 CZK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fe499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeiten und Prozentanteile der Darlehenslaufzeiten berechnen und sortieren\n",
    "type_counts_duration = loan_df['duration'].value_counts().reset_index(name='count')\n",
    "type_counts_duration.columns = ['duration', 'count']\n",
    "type_counts_duration = type_counts_duration.sort_values('duration')\n",
    "type_counts_duration['percent'] = (type_counts_duration['count'] / type_counts_duration['count'].sum() * 100).round(2)\n",
    "type_counts_duration['duration'] = type_counts_duration['duration'].astype(str)\n",
    "\n",
    "# Balkendiagramm zur Verteilung der Darlehenslaufzeiten\n",
    "fig = px.bar(\n",
    "    type_counts_duration,\n",
    "    x='duration',\n",
    "    y='percent',\n",
    "    text_auto=True,\n",
    "    hover_data={\"count\": True},\n",
    "    color='duration'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Loan duration',\n",
    "    yaxis_title='Percent %',\n",
    "    xaxis=dict(type='category')\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0d0d5",
   "metadata": {},
   "source": [
    "Die Loan duration weist 5 diskrete Werte auf, welche alle ziemlich uniform verteilt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot mit Boxplot und Histogramm zur Verteilung der Darlehenszahlungen erstellen\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    vertical_spacing=0.02)\n",
    "\n",
    "# Boxplot oben\n",
    "fig.add_trace(go.Box(x=loan_df[\"payments\"], name=\"Boxplot\", marker_color=\"blue\", orientation='h'), row=1, col=1)\n",
    "\n",
    "# Histogramm unten\n",
    "fig.add_trace(go.Histogram(x=loan_df[\"amount\"], nbinsx=100, name=\"Histogramm\", marker_color=\"blue\"), row=2, col=1)\n",
    "\n",
    "# Layout konfigurieren\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Distribution of Loan Payments\",\n",
    "    showlegend=True,\n",
    "    xaxis_title=\"Loan Payments (CZK)\",\n",
    "    xaxis2_title=\"Loan Payments (CZK)\",\n",
    "    bargap=0.05\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922aa9b",
   "metadata": {},
   "source": [
    "- Verteilung: Die meisten Kreditrückzahlungen liegen im Bereich von etwa 10.000 bis 150.000 CZK.\n",
    "Die Verteilung ist rechtsschief, mit einem langen Schwanz nach rechts, der höhere Rückzahlungen darstellt, die seltener auftreten.\n",
    "\n",
    "- **Boxplot Analyse:** \n",
    "    - Minimum: Der niedrigste Wert liegt bei etwa 304 CZK.\n",
    "    - Q1 (25%): Der erste Quartilwert liegt bei etwa 2.477 CZK.\n",
    "    - Median (50%): Der Medianwert beträgt etwa 3.934 CZK.\n",
    "    - Q3 (75%): Das dritte Quartil liegt bei etwa 5.814 CZK.\n",
    "    - Oberer Whisker (Upper Fence): Der obere Whisker endet bei etwa 9.910 CZK.\n",
    "    - Maximum: Der höchste Wert liegt bei etwa 9.910 CZK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeiten und Prozentanteile der Darlehensstatus berechnen und sortieren\n",
    "type_counts_status = loan_df['status'].value_counts().reset_index(name='count')\n",
    "type_counts_status.columns = ['status', 'count']\n",
    "type_counts_status = type_counts_status.sort_values('status')  # alphabetische Sortierung\n",
    "type_counts_status['percent'] = (type_counts_status['count'] / type_counts_status['count'].sum() * 100).round(2)\n",
    "\n",
    "# Balkendiagramm der Verteilung der Darlehensstatus\n",
    "fig = px.bar(\n",
    "    type_counts_status,\n",
    "    x='status',\n",
    "    y='percent',\n",
    "    text_auto=True,\n",
    "    hover_data={\"count\": True},\n",
    "    color='status'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Loan status',\n",
    "    yaxis_title='Percent',\n",
    "    xaxis=dict(type='category')\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c44e4",
   "metadata": {},
   "source": [
    "Die Loan status lassen sich in zwei sinnvolle Kategorien einteilen: OK (finished_ok und running_ok; machen zusammen 89% aus) und problemanfällig (finished_debts und running_debts; machen 11% aus). Also lässt sich sagen, dass jeder zehnte Loan ein Problem aufweist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b625c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479f0d9",
   "metadata": {},
   "source": [
    "**Schlüsselstruktur und Datenmodell**\n",
    "\n",
    "- `loan_id ist der Primärschlüssel.\n",
    "- `account_id` ist ein Fremdschlüssel, der auf den Datensatz **account_df** verweist.\n",
    "\n",
    "**Zusammenfassung der EDA-Erkenntnisse**\n",
    "\n",
    "\n",
    "- **Loan Amounts**: Der durchschnittliche Kreditbetrag liegt bei **117.000 CZK**. 75% der Kredite liegen bei einem Betrag von bis zu **210.000 CZK**.\n",
    "- **Duration**: Die Kreditlaufzeiten (durations) sind uniform verteilt und weisen keine signifikanten Abweichungen oder Spitzen auf.\n",
    "- **Payments**: Die meisten Kreditrückzahlungen liegen im Bereich von etwa 10.000 bis 150.000 CZK.\n",
    "Die Verteilung ist rechtsschief, mit einem langen Schwanz nach rechts, der höhere Rückzahlungen darstellt, die seltener auftreten.\n",
    "- **Loan Status**: Der Status der Kredite ist überwiegend positiv, mit **59%** der Kredite als `running_ok`. **30%** der Kredite sind `finished_ok`, und **10%** weisen Problemstellungen auf (entweder `running_debts` oder `finished_debts`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_2_3_'></a>[Transaction (`transaction_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d68e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Laden und Bereinigen der Daten**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Transaktionsdaten mit spezifizierten Datentypen und Datumsformat\n",
    "transaction_df = pd.read_csv(\"xselling_banking_data/trans.csv\", delimiter=\";\", dtype={\n",
    "    \"trans_id\": \"int64\",\n",
    "    \"account_id\": \"int64\",\n",
    "    \"type\": \"category\",\n",
    "    \"operation\": \"category\",\n",
    "    \"k_symbol\": \"category\",\n",
    "    \"bank\": \"string\",\n",
    "    \"account\": \"string\"\n",
    "}, parse_dates=[\"date\"],\n",
    "   date_format=\"%y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8b1d6",
   "metadata": {},
   "source": [
    "Die Spaltentypen sehen korrekt aus, wie sie eingelesen wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ddb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07692ac",
   "metadata": {},
   "source": [
    "Die tschechischen Bezeichnungen in den Spalten operation (Art der Transaktion) und k_symbol (Zweck der Zahlung) werden ins Englische übersetzt, um eine einheitliche und verständliche Benennung innerhalb der Analyse sicherzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df[\"type\"] = transaction_df[\"type\"].cat.rename_categories({\n",
    "    \"PRIJEM\": \"credit\",\n",
    "    \"VYDAJ\": \"withdrawal\"\n",
    "})\n",
    "\n",
    "transaction_df[\"operation\"] = transaction_df[\"operation\"].cat.rename_categories({\n",
    "    \"VYBER KARTOU\": \"credit_card_withdrawal\",\n",
    "    \"VKLAD\": \"credit_in_cash\",\n",
    "    \"PREVOD Z UCTU\": \"collection_from_another_bank\",\n",
    "    \"VYBER\": \"withdrawal_in_cash\",\n",
    "    \"PREVOD NA UCET\": \"remittance_to_another_bank\"\n",
    "})\n",
    "\n",
    "\n",
    "transaction_df[\"k_symbol\"] = transaction_df[\"k_symbol\"].cat.rename_categories({\n",
    "    \"POJISTNE\": \"insurance_payment\",\n",
    "    \"SLUZBY\": \"payment_for_statement\",\n",
    "    \"UROK\": \"interest_credited\",\n",
    "    \"SANKC. UROK\": \"sanction_interest_negative_balance\",\n",
    "    \"SIPO\": \"household_payment\",\n",
    "    \"DUCHOD\": \"oldage_pension\",\n",
    "    \"UVER\": \"loan_payment\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in transaction_df.select_dtypes(include='category'):\n",
    "    print(f\"\\nSpalte: {col}\")\n",
    "    print(f\"Anzahl eindeutiger Werte: {transaction_df[col].nunique()}\")\n",
    "    print(transaction_df[col].cat.categories.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee06b27",
   "metadata": {},
   "source": [
    "Die Umbenennung der Werte in den kategorialen Spalten hat grundsätzlich funktioniert.\n",
    "Allerdings fällt auf:\n",
    "\n",
    "- In der Spalte type ist der ursprüngliche tschechische Wert VYBER weiterhin vorhanden – dieser sollte weiter untersucht und gegebenenfalls korrigiert werden.\n",
    "\n",
    "- In der Spalte k_symbol existiert eine leere Kategorie (' '), die vermutlich auf fehlende oder nicht interpretierte Werte zurückzuführen ist. Auch diese sollte näher analysiert und bereinigt werden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50100a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.query(\"type == 'VYBER'\")[\"operation\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83888c6f",
   "metadata": {},
   "source": [
    "Bei genauer Analyse zeigt sich, dass alle 16'666 Transaktionen mit type == \"VYBER\" die Operation \"withdrawal_in_cash\" besitzen.\n",
    "Dies legt nahe, dass \"VYBER\" fälschlicherweise in der Spalte type verblieben ist und korrekterweise als \"withdrawal\" interpretiert werden sollte.\n",
    "Wir passen dies nun an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c7f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df[\"type\"] = transaction_df[\"type\"].replace(\"VYBER\" , \"withdrawal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in transaction_df.select_dtypes(include=\"category\"):\n",
    "    print(f\"\\nSpalte: {col}\")\n",
    "    print(f\"Anzahl eindeutiger Werte: {transaction_df[col].nunique()}\")\n",
    "    print(f\"Kategorien: {transaction_df[col].cat.categories.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0a42c",
   "metadata": {},
   "source": [
    "Die Korrektur   hat funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271acb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.isna().mean().mul(100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145a7ae",
   "metadata": {},
   "source": [
    "Die Analyse der fehlenden Werte zeigt, dass `bank` (74.1 %) und `account` (72.0 %) einen sehr hohen Anteil an NaN-Werten enthalten. Da sie nur wenig zur Analyse des Transaktionsverhaltens beitragen und keine zuverlässige Information liefern, werden diese Spalten aus dem Datensatz entfernt.\n",
    "\n",
    "Die Spalten `operation` und `k_symbol` hingegen sind relevant für das Transaktionsverhalten und werden daher beibehalten. Fehlende Werte werden dort mit der Kategorie \"unknown\" ersetzt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'unknown' als Kategorie in 'operation' hinzufügen, falls noch nicht vorhanden\n",
    "if \"unknown\" not in transaction_df['operation'].cat.categories:\n",
    "    transaction_df['operation'] = transaction_df['operation'].cat.add_categories(\"unknown\")\n",
    "\n",
    "# Leere Strings und NaN in 'operation' durch 'unknown' ersetzen\n",
    "transaction_df['operation'] = transaction_df['operation'].replace(' ', 'unknown')\n",
    "transaction_df['operation'] = transaction_df['operation'].fillna('unknown')\n",
    "\n",
    "# 'unknown' als Kategorie in 'k_symbol' hinzufügen, falls noch nicht vorhanden\n",
    "if \"unknown\" not in transaction_df['k_symbol'].cat.categories:\n",
    "    transaction_df['k_symbol'] = transaction_df['k_symbol'].cat.add_categories(\"unknown\")\n",
    "\n",
    "# Leere Strings und NaN in 'k_symbol' durch 'unknown' ersetzen\n",
    "transaction_df['k_symbol'] = transaction_df['k_symbol'].replace(' ', 'unknown')\n",
    "transaction_df['k_symbol'] = transaction_df['k_symbol'].fillna('unknown')\n",
    "\n",
    "# Entfernen der Spalten 'bank' und 'account', falls vorhanden\n",
    "transaction_df.drop(columns=['bank', 'account'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.isna().mean().mul(100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7e492",
   "metadata": {},
   "source": [
    "Alle fehlenden Werte wurden erfolgreich bereinigt:  \n",
    "Die Spalten `operation` und `k_symbol` enthalten nun die Ersatzkategorie `\"unknown\"`.  \n",
    "Die Spalten `bank` und `account` wurden aufgrund zu vieler fehlender Werte entfernt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a61e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in transaction_df.select_dtypes(include='category'):\n",
    "    print(f\"\\nSpalte: {col}\")\n",
    "    print(f\"Anzahl eindeutiger Werte: {transaction_df[col].nunique()}\")\n",
    "    print(transaction_df[col].cat.categories.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f9364",
   "metadata": {},
   "source": [
    "In `operation` und `k_symbol` wurde eine zusätzliche Spalte `\"unknown\"` ergänzt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205aea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df[\"amount\"].describe().apply(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f1fbc",
   "metadata": {},
   "source": [
    "In der Datenbeschreibung steht: type bezeichnet die Richtung der Transaktion – \"PRIJEM\" steht für Gutschrift (credit), \"VYDAJ\" für Abbuchung (withdrawal).\n",
    "\n",
    "Anhand der Auswertung der Spalte `amount` sehen wir jedoch, dass alle Beträge positiv gespeichert sind.\n",
    "\n",
    "Es ist somit nicht direkt erkennbar, ob es sich um eine Abhebung oder Einzahlung handelt, da das Vorzeichen nicht zwischen Credit (Gutschrift) und Withdrawal (Abhebung) unterscheidet.\n",
    "\n",
    " Zur Vorbereitung für die Modellierung setzen wir daher alle Abbuchungen (Withdrawal) in `amount` auf negative Werte, um Ein- und Ausgänge klar voneinander zu unterscheiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965327c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.loc[transaction_df[\"type\"] == \"withdrawal\", \"amount\"] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b387932",
   "metadata": {},
   "outputs": [],
   "source": [
    "(transaction_df.loc[transaction_df[\"type\"] == \"withdrawal\", \"amount\"] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041827d",
   "metadata": {},
   "source": [
    " Die Beträge aller withdrawal-Transaktionen wurden erfolgreich auf negative Werte gesetzt.\n",
    "Eine anschliessende Prüfung zeigt, dass nun sämtliche `amount`-Werte für Abbuchungen negativ sind.\n",
    "Damit ist sichergestellt, dass Ein- und Ausgänge im Datensatz korrekt unterschieden werden – eine wichtige Voraussetzung für die spätere Analyse und Modellierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df['trans_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a49e3",
   "metadata": {},
   "source": [
    "- Die Spalte `trans_id`ist eindeutig und dient als Primärschlüssel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c1eda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b23e9",
   "metadata": {},
   "source": [
    "- Der Datensatz enthält 1.056.320 Transaktionen, alle wichtigen Spalten sind vollständig.\n",
    "- In `amount` sind nun sowohl positive (Gutschrift) als auch negative Werte (Abbuchung) korrekt erfasst – bestätigt durch einen negativen Minimalwert (min: -87400).\n",
    "- Die Spalte date umfasst einen Zeitraum von 1993-01-01 bis 1998-12-31 \n",
    "-  Die Kategorie \"unknown\" wurde erfolgreich eingeführt, um fehlende Angaben in k_symbol und operation konsistent zu behandeln.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf520af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilung der Transaktionen in Einzahlungen (>0) und Abhebungen (<0)\n",
    "deposits_df = transaction_df[transaction_df['amount'] > 0]\n",
    "withdrawals_df = transaction_df[transaction_df['amount'] < 0]\n",
    "\n",
    "# Subplot mit Boxplot und Histogramm für Einzahlungen\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    row_heights=[0.2, 0.8],\n",
    "                    vertical_spacing=0.02)\n",
    "\n",
    "fig.add_trace(go.Box(x=deposits_df[\"amount\"], name=\"Boxplot\", marker_color=\"red\", orientation='h'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=deposits_df[\"amount\"], nbinsx=100, name=\"Histogramm\", marker_color=\"red\"), row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Deposits Distribution: Box Plot and Histogram\",\n",
    "    showlegend=True,\n",
    "    xaxis_title=\"Transaction Amount (CZK)\",\n",
    "    bargap=0.05\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Subplot mit Boxplot und Histogramm für Abhebungen (Beträge absolut)\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    row_heights=[0.2, 0.8],\n",
    "                    vertical_spacing=0.02)\n",
    "\n",
    "fig.add_trace(go.Box(x=withdrawals_df[\"amount\"].abs(), name=\"Boxplot\", marker_color=\"blue\", orientation='h'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=withdrawals_df[\"amount\"].abs(), nbinsx=100, name=\"Histogramm\", marker_color=\"blue\"), row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Withdrawals Distribution: Box Plot and Histogram (Absolute Values)\",\n",
    "    showlegend=True,\n",
    "    bargap=0.05\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Withdrawal Amount (CZK, abs.)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Withdrawal Amount (CZK, abs.)\", row=2, col=1)\n",
    "fig.update_xaxes(range=[0, withdrawals_df[\"amount\"].abs().max()], row=1, col=1)\n",
    "fig.update_xaxes(range=[0, withdrawals_df[\"amount\"].abs().max()], row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db9531",
   "metadata": {},
   "source": [
    "**Boxplot der Einzahlungen**\n",
    "\n",
    "Die Verteilung der Einzahlungshöhen ist **stark rechtsschief (linkssteil) **:\n",
    "\n",
    "- Der **Median** liegt bei **1'600 CZK**, was auf viele kleinere Einzahlungen hindeutet.\n",
    "- **75 %** der Einzahlungen liegen unter **12'088 CZK** (Q3), und der obere „Whisker“ endet bei **30'002 CZK**.\n",
    "- **Einzahlungen über 30'002 CZK** gelten als **Ausreisser** – mit einem **absoluten Maximum von 74'812 CZK**.\n",
    "\n",
    " **Boxplot der Abhebungen**\n",
    "\n",
    "Die Verteilung der Abhebungsbeträge ist stark **stark rechtsschief (linkssteil)**\n",
    "\n",
    "- Der **Median** liegt bei **–2'128 CZK**, was auf viele kleinere bis mittlere Abhebungen hindeutet.\n",
    "- **75 %** der Werte liegen über **–5'520 CZK**, während der „untere Whisker“ bei **–13'653 CZK** endet.\n",
    "- Abhebungen unter –13'653 CZK gelten als **Ausreisser**, mit einem **Minimum von –87'400 CZK**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prozentsatz der Häufigkeiten in der Spalte 'type' berechnen\n",
    "type_counts = transaction_df['type'].value_counts(normalize=True).reset_index(name='percent')\n",
    "type_counts['percent'] = (type_counts['percent'] * 100).round(2)\n",
    "type_counts.columns = ['type', 'percent']\n",
    "\n",
    "# Balkendiagramm für die Verteilung der Transaktionstypen\n",
    "fig_type = px.bar(\n",
    "    type_counts, \n",
    "    x='type', \n",
    "    y='percent', \n",
    "    title='Distribution of Transaction Types',\n",
    "    labels={'type': 'Transaction Type', 'percent': 'Percentage (%)'},\n",
    "    color='type',\n",
    "    text_auto=True,\n",
    ")\n",
    "fig_type.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "fig_type.show()\n",
    "\n",
    "# Prozentsatz der Häufigkeiten in der Spalte 'operation' berechnen\n",
    "operation_counts = transaction_df['operation'].value_counts(normalize=True).reset_index(name='percent')\n",
    "operation_counts['percent'] = (operation_counts['percent'] * 100).round(2)\n",
    "operation_counts.columns = ['operation', 'percent']\n",
    "\n",
    "# Balkendiagramm für die Verteilung der Operationstypen\n",
    "fig_operation = px.bar(\n",
    "    operation_counts, \n",
    "    x='operation', \n",
    "    y='percent', \n",
    "    title='Distribution of Operations',\n",
    "    labels={'operation': 'Operation Type', 'percent': 'Percentage (%)'},\n",
    "    color='operation',\n",
    "    text_auto=True,\n",
    ")\n",
    "fig_operation.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "fig_operation.show()\n",
    "\n",
    "# Subplot mit Boxplot und Histogramm zur Verteilung der Kontostände\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    row_heights=[0.2, 0.8],\n",
    "                    vertical_spacing=0.02)\n",
    "fig.add_trace(go.Box(x=transaction_df[\"balance\"], name=\"Boxplot\", marker_color=\"blue\", orientation='h'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=transaction_df[\"balance\"], nbinsx=100, name=\"Histogramm\", marker_color=\"blue\"), row=2, col=1)\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Balance Distribution: Box Plot and Histogram\",\n",
    "    showlegend=True,\n",
    "    bargap=0.05\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad21b5b",
   "metadata": {},
   "source": [
    "- **Transaction Type:**  \n",
    "  - Abhebungen (`withdrawal`) dominieren mit ca. 62 %, Einzahlungen (`credit`) machen ca. 38 % aus.\n",
    "\n",
    "- **Operation Type:**  \n",
    "  - Häufigste Kategorie ist `withdrawal_in_cash` (~41 %).  \n",
    "  - `remittance_to_another_bank`, `credit_in_cash` und `unknown` folgen.  \n",
    "  - `credit_card_withdrawal` kommt kaum vor.\n",
    "\n",
    "- **Analyse Balance**\n",
    "  - Die Verteilung ist **positiv rechtsschief**: Die meisten Salden liegen im positiven Bereich.\n",
    "  - **Min:** -41.125.7k CZK, **Max:** 209.637k CZK \n",
    "  - **Median:** 33.143.4k CZK, **Q1 (25%):** 22.409.5k CZK, **Q3 (75%):** 49.603.6k CZK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prozentsatz der Häufigkeiten in der Spalte 'k_symbol' berechnen\n",
    "type_counts = transaction_df['k_symbol'].value_counts(normalize=True).reset_index(name='percent')\n",
    "type_counts['percent'] = (type_counts['percent'] * 100).round(2)\n",
    "type_counts.columns = ['k_symbol', 'percent']\n",
    "\n",
    "# Balkendiagramm für die Verteilung der 'k_symbol'-Kategorien\n",
    "fig_type = px.bar(\n",
    "    type_counts, \n",
    "    x='k_symbol', \n",
    "    y='percent', \n",
    "    title='Distribution of K_Symbol: Characterization of Transaction',\n",
    "    labels={'k_symbol': 'Characterization of Transaction', 'percent': 'Percentage (%)'},\n",
    "    color='k_symbol',\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "fig_type.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\",\n",
    "    cliponaxis=False\n",
    ")\n",
    "\n",
    "fig_type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f3723",
   "metadata": {},
   "source": [
    "\n",
    "- **50%** der Transaktionen sind als `unknown` kategorisiert, was auf unklare oder nicht spezifizierte Transaktionen hinweist.\n",
    "- **`interest_credited`** folgt mit **ca. 20%**, was auf Zinsgutschriften hinweist.\n",
    "- **`payment_for_statement`** und **`household_payment`** machen **ca. 10-12%** aus, was Zahlungen im Zusammenhang mit Kontoauszügen und Haushaltszahlungen widerspiegelt.\n",
    "- Kategorien wie **`oldage_pension`**, **`insurance_payment`**, **`loan_payment`** und **`sanction_interest_negative_balance`** sind weniger häufig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'date' als datetime konvertieren\n",
    "transaction_df['date'] = pd.to_datetime(transaction_df['date'])\n",
    "\n",
    "# Jahr-Monat als Periodenstring extrahieren\n",
    "transaction_df['year_month'] = transaction_df['date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# Monatliche Summe der Transaktionsbeträge berechnen\n",
    "monthly_sum = transaction_df.groupby('year_month')['amount'].sum().reset_index()\n",
    "monthly_sum['year_month'] = pd.to_datetime(monthly_sum['year_month'])\n",
    "\n",
    "# Linienplot der monatlichen Transaktionssummen\n",
    "fig = px.line(monthly_sum,\n",
    "              x='year_month',\n",
    "              y='amount',\n",
    "              labels={'year_month': 'Monat', 'amount': 'Transaktionssumme (CZK)'},\n",
    "              title=\"Monatliche Transaktionssumme (1993–1998)\")\n",
    "\n",
    "# Achsen-Formatierung für bessere Lesbarkeit\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        dtick=\"M3\",             # Tick alle 3 Monate\n",
    "        tickformat=\"%b %Y\",     # Format: Jan 1993\n",
    "        tickangle=45            # Beschriftung schräg\n",
    "    ),\n",
    "    title_text=\"Monatliche Transaktionssumme (1993–1998)\",\n",
    "    yaxis_title=\"Betrag (CZK)\",\n",
    "    xaxis_title=\"Monat\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ec0a2",
   "metadata": {},
   "source": [
    " **Monatliche Transaktionssumme (1993–1998)**\n",
    "\n",
    "Die Visualisierung zeigt die aggregierten Transaktionsbeträge pro Monat über einen Zeitraum von sechs Jahren.\n",
    "\n",
    "- Es ist ein **regelmässig wiederkehrendes Muster** erkennbar: Fast jedes Jahr zeigt sich im **Dezember oder Januar** ein starker Rückgang der Transaktionssumme (negativer Peak), gefolgt von einer raschen Erholung.\n",
    "- Insgesamt zeigt sich ein **positiver Trend**: Die durchschnittlichen monatlichen Transaktionssummen nehmen über die Jahre tendenziell zu.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8732ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fazit und nächste Schritte**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baefd5c2",
   "metadata": {},
   "source": [
    "**Schlüsselstruktur und Datenmodell**\n",
    "\n",
    "- trans_id ist Primärschlüssel\n",
    "- district_id ist Fremdschlüssel zu Datensatz district_df\n",
    "\n",
    "**Zusammenfassung der EDA Erkenntnisse**\n",
    "\n",
    "- Die Analyse der fehlenden Werte zeigte, dass `bank` (74.1 %) und `account` (72.0 %) einen sehr hohen Anteil an NaN-Werten enthalten. Da sie nur wenig zur Analyse des Transaktionsverhaltens beitragen und keine zuverlässige Information liefern, wurden diese Spalten aus dem Datensatz entfernt.\n",
    "- Die Spalten `operation` und `k_symbol` enthalten nun die Ersatzkategorie `\"unknown\"`.  Diese Spalten sind für die Analyse des Transaktionsverhaltens relevant und bleiben im Datensatz.\n",
    "- **50%** der Transaktionen sind als `unknown` kategorisiert, was auf unklare oder nicht spezifizierte Transaktionen hinweist.\n",
    "- Der Transaktionstyp  withdrawal (Abhebung) dominiert mit ca. 62 %, während  Einzahlungen (credit) nur etwa 38 % ausmachen.\n",
    "- Die Balance-Verteilung ist rechtsschief: Die meisten Konten weisen kleinere bis mittlere Salden auf, mit wenigen extrem hohen Salden, die die Verteilung nach oben verzerren. Der Median der Balance liegt bei ca. 33.143,4 CZK, und 75 % der Salden liegen unter 49.603,6 CZK. Es gibt Ausreisser im oberen Bereich mit Salden bis zu 200.000 CZK.\n",
    "- Es wurde ein regelmässiges, saisonales Muster in den monatlichen Transaktionssummen identifiziert. Fast jedes Jahr zeigt sich im Dezember oder Januar ein starker Rückgang der Transaktionssumme, gefolgt von einer schnellen Erholung im folgenden Monat. Diese saisonalen Schwankungen könnten durch Faktoren wie Jahresabschlüsse oder Urlaubs-/Feiertagsausgaben bedingt sein. Die saisonalen Ausschläge nehmen in der Stärke der Ausschläge tendenziell über die beobachtete Zeit zu.\n",
    "- Ein- und Auszahlungen sind stark rechtsschief. Der Median der Einzahlungen liegt bei 1.600 CZK, 75 % der Einzahlungen liegen unter 12.088 CZK. Der Median der Auszahlungen liegt bei -2.128 CZK, 75 % der Abhebungen liegen über -5.520 CZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_3_'></a>[Konto 14 und 18](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15ce6c",
   "metadata": {},
   "source": [
    "Wir untersuchen die Konten 14 und 18."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773d0fc",
   "metadata": {},
   "source": [
    "Wir filtern die Transaktionen für die Konten 14 und 18 heraus, erstellen daraus eine eigene Kopie und ergänzen diese um eine Spalte, die den Monat der jeweiligen Buchung als Periodenwert enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531cbd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maske für die Konten 14 und 18 erstellen\n",
    "mask = transaction_df[\"account_id\"].isin([14, 18])\n",
    "\n",
    "# Gefilterte Kopie mit neuer Spalte 'month' als Jahr-Monat-Periode\n",
    "tx_sel = (\n",
    "    transaction_df.loc[mask]\n",
    "                  .copy()  # Eigene Kopie, um SettingWithCopyWarning zu vermeiden\n",
    "                  .assign(month=lambda df: df[\"date\"].dt.to_period(\"M\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ecfbc9",
   "metadata": {},
   "source": [
    "Jetzt aggregieren wir die monatlichen Umsätze als Saldo und ermitteln den Monatsend-Kontostand. Zusätzlich berechnen wir Ein- und Auszahlungen getrennt, fügen diese dem Monats-DataFrame hinzu um eine vollständige Übersicht pro Konto und Monat zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678db674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umsatz (saldiert)\n",
    "umsatz = (\n",
    "    tx_sel.groupby([\"account_id\", \"month\"])[\"amount\"]\n",
    "          .sum()\n",
    "          .rename(\"umsatz\")\n",
    ")\n",
    "\n",
    "# Monatsend-Saldo\n",
    "saldo = (\n",
    "    tx_sel.sort_values(\"date\")\n",
    "         .groupby([\"account_id\", \"month\"])[\"balance\"]\n",
    "         .last()\n",
    "         .rename(\"saldo\")\n",
    ")\n",
    "\n",
    "# Zusammenführen der Salden und Umsätze\n",
    "monthly = pd.concat([umsatz, saldo], axis=1).reset_index()\n",
    "\n",
    "# Umsätze getrennt positiv und negativ berechnen\n",
    "umsatz_pos = (\n",
    "    tx_sel[tx_sel[\"amount\"] > 0]\n",
    "    .groupby([\"account_id\", \"month\"])[\"amount\"]\n",
    "    .sum()\n",
    "    .rename(\"umsatz_pos\")\n",
    ")\n",
    "\n",
    "umsatz_neg = (\n",
    "    tx_sel[tx_sel[\"amount\"] < 0]\n",
    "    .groupby([\"account_id\", \"month\"])[\"amount\"]\n",
    "    .sum()\n",
    "    .abs()\n",
    "    .rename(\"umsatz_neg\")\n",
    ")\n",
    "\n",
    "# Alle in monthly einfügen\n",
    "monthly = monthly.set_index([\"account_id\", \"month\"])\n",
    "monthly = monthly.join([umsatz_pos, umsatz_neg]).fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca15214",
   "metadata": {},
   "source": [
    "Zuerst verschaffen wir uns mit diesen drei Plots einen Überblick über den gesamten Beobachtungszeitraum. Dabei visualisieren wir das monatliche Monatsend-Vermögen sowie die monatlichen Ein- und Auszahlungen getrennt für die Konten 14 und 18. So erkennen wir Trends und Schwankungen in den Geldbewegungen und im Kontostand über die Zeit hinweg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Plot Monatsend-Saldo\n",
    "plt.figure(figsize=(12,5))\n",
    "for acc_id, grp in monthly.groupby(\"account_id\"):\n",
    "    plt.plot(grp[\"month\"].dt.to_timestamp(), grp[\"saldo\"], label=f\"Saldo Konto {acc_id}\")\n",
    "plt.title(\"Monatsend-Saldo\")\n",
    "plt.xlabel(\"Monat\")\n",
    "plt.ylabel(\"Saldo (CZK)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Plot nur Einzahlungen\n",
    "plt.figure(figsize=(12,5))\n",
    "for acc_id, grp in monthly.groupby(\"account_id\"):\n",
    "    plt.plot(grp[\"month\"].dt.to_timestamp(), grp[\"umsatz_pos\"], label=f\"Einzahlungen Konto {acc_id}\")\n",
    "plt.title(\"Monatliche Einzahlungen\")\n",
    "plt.xlabel(\"Monat\")\n",
    "plt.ylabel(\"Betrag (CZK)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Plot nur Auszahlungen\n",
    "plt.figure(figsize=(12,5))\n",
    "for acc_id, grp in monthly.groupby(\"account_id\"):\n",
    "    plt.plot(grp[\"month\"].dt.to_timestamp(), grp[\"umsatz_neg\"], label=f\"Auszahlungen Konto {acc_id}\")\n",
    "plt.title(\"Monatliche Auszahlungen\")\n",
    "plt.xlabel(\"Monat\")\n",
    "plt.ylabel(\"Betrag (CZK)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a705198",
   "metadata": {},
   "source": [
    "Die Plots zeigen, dass Konto 18 deutlich höhere Schwankungen bei Saldo, Einzahlungen und Auszahlungen aufweist als Konto 14. Konto 14 ist erst ab kurz vor 1997 im Datensatz vertreten und zeigt seitdem einen relativ stabilen und langsamen Anstieg im Saldo. Dagegen gibt es bei Konto 18 starke Peaks, die auf unregelmässige oder grössere Transaktionen hindeuten.\n",
    "\n",
    "Diese Übersicht über den gesamten Zeitraum gibt einen guten ersten Eindruck von den Kontobewegungen.\n",
    "\n",
    "Im nächsten Schritt konzentrieren wir uns auf das Jahr 1997, um typische monatliche Verläufe detaillierter zu analysieren und saisonale Muster über das Jahr hinweg zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "example_year = 1997\n",
    "# Filtere Transaktionen für das Beispieljahr\n",
    "tx_1997 = tx_sel[tx_sel[\"date\"].dt.year == example_year]\n",
    "\n",
    "# Formatter zur Anzeige nur des Monats als Zahl auf der x-Achse\n",
    "def month_formatter(x, pos=None):\n",
    "    dt = mdates.num2date(x)\n",
    "    return str(dt.month)\n",
    "\n",
    "for acc_id in [14, 18]:\n",
    "    data = tx_1997[tx_1997[\"account_id\"] == acc_id]\n",
    "    \n",
    "    # Tagesweise Summen berechnen\n",
    "    daily_sum = data.groupby(\"date\")[\"amount\"].sum()\n",
    "    daily_pos = data[data[\"amount\"] > 0].groupby(\"date\")[\"amount\"].sum()\n",
    "    daily_neg = data[data[\"amount\"] < 0].groupby(\"date\")[\"amount\"].sum().abs()\n",
    "    daily_balance = data.groupby(\"date\")[\"balance\"].last()  # Tagesend-Vermögen\n",
    "    \n",
    "    for vals, label, color in [\n",
    "        (daily_sum, \"Gesamtumsatz\", None),\n",
    "        (daily_pos, \"Einzahlungen\", \"green\"),\n",
    "        (daily_neg, \"Auszahlungen\", \"red\"),\n",
    "        (daily_balance, \"Tägliches Vermögen\", \"blue\"),\n",
    "    ]:\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.plot(vals.index, vals.values, label=label, color=color)\n",
    "        plt.title(f\"{label} Konto {acc_id} im Jahr {example_year}\")\n",
    "        plt.xlabel(\"Monat\")\n",
    "        plt.ylabel(\"Betrag (CZK)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(month_formatter))\n",
    "        \n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a83a3",
   "metadata": {},
   "source": [
    "Konto 14 zeigt regelmässige, zyklische Muster bei Ein- und Auszahlungen. Es besteht ein Sparsockel von ca. 30000 CZK. Das Geld wird hier fast immer komplett ausgegeben bis auf den Sparsockel, was für ein Privatkonto typisch ist. Konto 18 ist deutlich volatiler, mit grösseren Schwankungen und Spitzen, was auf ein mögliches Durchlaufskonto hindeutet. Ab Mitte des Jahres steigt das Vermögen stark an, beginnt jedoch wieder zu senken zu Anfang des neuen Jahres. Logischerweise korrelieren Ein- und Auszahlungen auf beiden Konten mit dem Saldoverlauf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263cd2e",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Kombination und EDA der statischen Entitäten](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_1_'></a>[Kombination (`merged_df_static`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a64861",
   "metadata": {},
   "source": [
    "\n",
    "**Vorgehen zur Kombination der dynamischen und statischen Entitäten**\n",
    "\n",
    "Die Kombination der Daten erfolgt auf Basis der Unterscheidung in **statische und dynamische Entitäten** (siehe Erläuterungen hierzu in Kapitel 2.1 Einleitung) in drei klar abgegrenzten Schritten:\n",
    "\n",
    "\n",
    "1. **Erzeugung einer stabilen Kundenbasis:**  \n",
    "     Zuerst werden die statischen Entitäten (`Client`, `Disposition`, `Account`, `Credit Card`) miteinander verbunden, um pro Kunde eine umfassende und konstante Beschreibung zu erstellen. Dies bildet die Basis für die spätere Integration der dynamischen Entitäten.\n",
    "\n",
    "2. **Aggregation dynamischer Daten im Rollup-Fenster:**  \n",
    "   Anschliessend werden für jede Person die Daten aus den dynamischen Entitäten (`Transaction`, `Loan`, `Permanent Order`) über ein festgelegtes Zeitfenster von 13 Monaten aggregiert.  \n",
    "   - **Hinweis:** Der Join der dynamischen Entitäten ist eine **notwendige Voraussetzung für die Erstellung des Rollup-Fensters**. Ohne diesen Join können keine konsistenten und vergleichbaren aggregierten Merkmale berechnet werden.  \n",
    "   - Das Rollup-Fenster ermöglicht die Erfassung saisonaler Effekte und stellt sicher, dass für alle Kunden ein gleichlanges Beobachtungsfenster vorliegt.\n",
    "\n",
    "3. **Zusammenführung beider Ebenen:**  \n",
    "     Im letzten Schritt werden die aggregierten Verhaltensdaten aus Schritt 2 mit den  Kundendaten aus Schritt 1 zusammengeführt. Dadurch entsteht ein vollständiger Datensatz pro Kunde, der sowohl **statische Merkmale (z. B. Alter, Geschlecht, Region)** als auch **dynamische Merkmale (z. B. durchschnittliche Transaktionssumme, Kreditvolumen)** umfasst und als Grundlage für die Modellierung dient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab733f7",
   "metadata": {},
   "source": [
    "Nun wollen wir die einzelnen Datensätze so sinnvoll miteinander joinen, sodass eine spätere Identifikation von Kreditkarten-Halter und Kreditkarten-Nichthalter ermöglicht wird. Wir wollen hierzu so viele Variablen bzw. Spalten erhalten wie für die Ermittlung nötig und redundante Informationen nicht berücksichtigen. Wir gehen nach folgendem Merge-Schema vor: Client <- Disposition <- Client <- Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19292776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgangstabelle: client_df\n",
    "merged_df_static = client_df\n",
    "\n",
    "# Verbindung mit der disposition_df-Tabelle per 'client_id'\n",
    "merged_df_static = pd.merge(merged_df_static, disposition_df, on='client_id', how='left')\n",
    "\n",
    "# Verbindung mit der credit_card_df-Tabelle per 'disp_id'\n",
    "merged_df_static = pd.merge(merged_df_static, credit_card_df, on='disp_id', how='left')\n",
    "\n",
    "# Verbindung mit der account_df-Tabelle per 'account_id'\n",
    "merged_df_static = pd.merge(merged_df_static, accounts_df, on='account_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56039570",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86581c0a",
   "metadata": {},
   "source": [
    "Wir sehen, dass nach dem Merge die Spalten `district_id` und `type` umbenannt wurden, da sie doppelt vorkommen.\n",
    "Wir benennen diese nun wieder sinnvoll um, damit diese besser interpretierbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.rename(columns={'district_id_x': 'district_id_client', 'district_id_y': 'disctrict_id_account', 'type_x': 'disposition_type', 'type_y': 'credit_card_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa23c6b",
   "metadata": {},
   "source": [
    "Die Umbenennung hat funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindung mit district_df basierend auf client_df\n",
    "# Merge basierend auf der 'client_district_id'\n",
    "merged_df_static = pd.merge(\n",
    "    merged_df_static, \n",
    "    district_df, \n",
    "    left_on='district_id_client', \n",
    "    right_on='district_id', \n",
    "    how='left', \n",
    "    suffixes=('', '_client')\n",
    ")\n",
    "\n",
    "# Lösche die redundante district_id Spalte\n",
    "merged_df_static.drop('district_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c85481",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52919949",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_2_'></a>[Unterteilung von Käufern und Nichtkäufern](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fb1b2",
   "metadata": {},
   "source": [
    "Nun wollen wir die Unterteilung von Käufern und Nichtkäufern machen. Wir erstellen hierzu aus dem gemergten Ausgangsdatensatz, zwei Datensätze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afa490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_owner_df = merged_df_static[merged_df_static['card_id'].notna()]\n",
    "\n",
    "non_cc_owner_df = merged_df_static[merged_df_static['card_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efa22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_owner_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c879273",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cc_owner_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af760d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenfassung der Anzahl von Kreditkarteninhabern und Nichtinhabern mit Prozentanteilen\n",
    "cc_summary_df = pd.DataFrame({\n",
    "    \"Group\": [\"Kreditkarten-Inhaber\", \"Kreditkarten-Nichtinhaber\"],\n",
    "    \"Count\": [len(cc_owner_df), len(non_cc_owner_df)],\n",
    "})\n",
    "cc_summary_df[\"percent\"] = ((cc_summary_df[\"Count\"] / cc_summary_df[\"Count\"].sum()) * 100).round(2)\n",
    "\n",
    "# Balkendiagramm zur Verteilung der beiden Kundengruppen\n",
    "fig = px.bar(\n",
    "    cc_summary_df,\n",
    "    x='Group',\n",
    "    y='percent', \n",
    "    text='percent',  \n",
    "    title='Verteilung der Kreditkartenkäufer und Nicht-Käufer',\n",
    "    labels={'Group': 'Kundengruppe', 'percent': 'Prozent'},\n",
    "    color='Group',  \n",
    "    text_auto=True\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textfont_size=12,\n",
    "    textangle=0,\n",
    "    textposition=\"outside\", \n",
    "    cliponaxis=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde642ab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Der Plot zeigt, dass die Mehrheit der Bankkunden keine Kreditkartenbesitzer (83 %) sind. Damit ist die Zielvariable stark unausgeglichen. Da wir ein Modell trainieren wollen, das Kreditkartenkäufer zuverlässig identifiziert, müssen wir mit einer unbalancierten Klassifikation umgehen. Dies hat Auswirkungen auf die Wahl geeigneter Modellmetriken (z. B. Precision, Recall) und kann Massnahmen wie Resampling oder Gewichtung der Klassen erforderlich machen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_3_'></a>[Klärung entitätenspezifischer Fragestellungen (aus Kapitel 2)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa521d",
   "metadata": {},
   "source": [
    "**Frage**\n",
    "\n",
    "Besitzen ausschliesslich `owner`-Einträge (nicht `Disponent`) in der `disposition`-Tabelle eine Kreditkarte? Dies stellt sicher, dass nur tatsächlich entscheidungsbefugte Personen (Kontoinhaber:innen) in die Modellierungsbasis einbezogen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c4ce7",
   "metadata": {},
   "source": [
    "**Antwort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113837e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filteren OWNER Einträge\n",
    "owner_df = merged_df_static[merged_df_static['disposition_type'] == 'OWNER']\n",
    "num_owners = len(owner_df)\n",
    "print(f'Anzahl Typ OWNER: {num_owners}')\n",
    "\n",
    "# Filtern OWNER mit Kreditkarte\n",
    "owner_with_card_df = owner_df[owner_df['card_id'].notna()]\n",
    "num_owner_with_cc=len(owner_with_card_df)\n",
    "print(f'Anzahl Typ OWNER mit Kreditkarte: {num_owner_with_cc}')\n",
    "\n",
    "print(f\"Besitzen alle 'owner' eine Kreditkarte? {num_owners == num_owner_with_cc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der OWNER ohne Kreditkarte\n",
    "owner_without_card_df = owner_df[owner_df['card_id'].isna()]\n",
    "num_owner_without_cc = len(owner_without_card_df)\n",
    "\n",
    "owner_card_counts = pd.DataFrame({\n",
    "    'category': ['With Credit Card', 'Without Credit Card'],\n",
    "    'count': [num_owner_with_cc, num_owner_without_cc]\n",
    "})\n",
    "\n",
    "owner_card_counts['percent'] = (owner_card_counts['count'] / owner_card_counts['count'].sum() * 100).round(1)\n",
    "\n",
    "fig = px.bar(\n",
    "    owner_card_counts, \n",
    "    x='category', \n",
    "    y='percent', \n",
    "    text_auto='.1f', \n",
    "    color='category', \n",
    "    hover_data=['count'],\n",
    "    title='Distribution of OWNER with and without Credit Card',\n",
    "    labels={'category': 'Category', 'percent': 'Percentage (%)'}\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7b9ed",
   "metadata": {},
   "source": [
    "Es gibt 4500 OWNER. Von diesen 4500 gibt es 892 (20%), welche eine Kreditkarte besitzen.\n",
    "\n",
    "Folgend wollen wir noch untersuchen ob es DISPONENT Einträge gibt, welche eine Kreditkarten-ID als Eintrag aufweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filteren DISPONENT Einträge\n",
    "disponent_df = merged_df_static[merged_df_static['disposition_type'] == 'DISPONENT']\n",
    "num_disponents = len(disponent_df)\n",
    "print(f'Anzahl Typ DISPONENT: {num_disponents}')\n",
    "\n",
    "# Filtern DISPONENT mit Kreditkarte\n",
    "disponent_with_card_df = disponent_df[disponent_df['card_id'].notna()]\n",
    "num_disponent_with_cc = len(disponent_with_card_df)\n",
    "print(f'Anzahl Typ DISPONENT mit Kreditkarte: {num_disponent_with_cc}')\n",
    "\n",
    "print(f\"Besitzen DISPONENT Einträge eine Kreditkarte? {num_disponents == num_disponent_with_cc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170914a2",
   "metadata": {},
   "source": [
    "**Implikationen für die Modellentwicklung** \n",
    "\n",
    "Die Analyse bestätigt, dass ausschliesslich Einträge mit dem disposition_type `OWNER` eine Kreditkarte besitzen können. Kein DISPONENT ist mit einer Kreditkarte verknüpft.\n",
    "DISPONENTs müssen aus der Analyse ausgeschlossen werden, da sie nicht als potenzielle Kreditkartenkäufer in Betracht kommen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef717b81",
   "metadata": {},
   "source": [
    "**Frage**\n",
    "\n",
    "Filtern von Junior-Karten: Ab welchem Alter werden keine Junior-Karten mehr vergeben? Altersbasierter Ausschluss von Kreditkarten, die nicht zur Zielgruppe der Kampagne gehören."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac0479",
   "metadata": {},
   "source": [
    "**Antwort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dfde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtern nach Junior-Karten\n",
    "junior_cards_df = merged_df_static[merged_df_static['credit_card_type'] == 'junior']\n",
    "\n",
    "fig = px.histogram(\n",
    "    junior_cards_df, \n",
    "    x='age', \n",
    "    title='Age Distribution of Junior Credit Cards',\n",
    "    labels={'age': 'Age'},\n",
    "    nbins=20  # Anzahl der Bins anpassen, wenn nötig\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92234e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximalalter Besitzer junior-Card: {max(junior_cards_df['age'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832a9f4",
   "metadata": {},
   "source": [
    "**Implikationen für die Modellentwicklung** \n",
    "\n",
    "Wir haben den Threshold für das Alter eines junior-Card Besitzers mit 24 Jahren ermittelt. D.h. ab 25 Jahren wird keine junior-Card mehr vergeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21f73b",
   "metadata": {},
   "source": [
    "**Frage**\n",
    "\n",
    "Bleibt `client_id` auch nach dem Zusammenführen mit anderen Tabellen eindeutig? In der aktuellen `disposition`-Tabelle kommt jede `client_id` nur einmal vor, was auf eine 1:1-Beziehung zwischen Kund:innen und Konten hindeutet. Für die weitere Datenverarbeitung ist zu prüfen, ob diese Eindeutigkeit auch nach dem Join mit anderen Tabellen (z. B. `card`, `client`, `account`) erhalten bleibt, oder ob ein Kunde mehreren Konten oder Rollen zugeordnet ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1810007",
   "metadata": {},
   "source": [
    "**Antwort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vor dem Mergen\n",
    "unique_clients_before = len(disposition_df['client_id'].unique())\n",
    "print(f\"Einzigartige client_id im disposition_df vor dem Merge: {unique_clients_before}\")\n",
    "\n",
    "# Nach dem Meergen\n",
    "unique_clients_after = len(merged_df_static['client_id'].unique())\n",
    "print(f\"Einzigartige client_id nach dem Merge: {unique_clients_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc307d01",
   "metadata": {},
   "source": [
    "**Implikationen für die Modellentwicklung** \n",
    "\n",
    "Die Überprüfung zeigt, dass die `client_id` auch nach dem Zusammenführen der Tabellen eindeutig bleibt. Das bedeutet, dass jeder Kunde genau eine Zeile in der kombinierten statischen Entitätentabelle (`merged_df_static`) erhält. Dadurch ist gewährleistet, dass alle Features und Zielvariablen eindeutig pro Person zugeordnet werden können – eine zentrale Voraussetzung für die spätere Modellierung auf Kundenebene.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571068ba",
   "metadata": {},
   "source": [
    "**Frage**\n",
    "\n",
    "Gibt es pro `account_id` mehrere `disp_id`-Einträge mit verschiedenen Rollen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2133e92",
   "metadata": {},
   "source": [
    "**Antwort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der eindeutigen disposition_types pro client_id\n",
    "multi_roles_per_client = merged_df_static.groupby('client_id')['disposition_type'].nunique()\n",
    "\n",
    "# Clients mit mehr als einer Rolle\n",
    "clients_with_multiple_roles = multi_roles_per_client[multi_roles_per_client > 1]\n",
    "\n",
    "# Anzahl der Clients mit einer Rolle\n",
    "clients_with_single_role = multi_roles_per_client[multi_roles_per_client == 1]\n",
    "\n",
    "print(f\"Anzahl der Clients mit mehreren Rollen: {len(clients_with_multiple_roles)}\")\n",
    "print(f\"Anzahl der Clients mit einer Rolle: {len(clients_with_single_role)}\")\n",
    "\n",
    "# Zusammenfassung der Ergebnisse\n",
    "role_counts_client = pd.Series(\n",
    "    {\n",
    "        'Single Role': len(clients_with_single_role),\n",
    "        'Multiple Roles': len(clients_with_multiple_roles)\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "role_counts_client.columns = ['Role Type', 'Count']\n",
    "role_counts_client['Percent'] = (role_counts_client['Count'] / role_counts_client['Count'].sum() * 100).round(1)\n",
    "\n",
    "# Visualisierung\n",
    "fig = px.bar(\n",
    "    role_counts_client, \n",
    "    x='Role Type', \n",
    "    y='Percent', \n",
    "    text='Percent',  \n",
    "    title='Distribution of Clients with Single and Multiple Roles',\n",
    "    labels={'Role Type': 'Role Type', 'Percent': 'Percentage (%)'},\n",
    "    color='Role Type',\n",
    "    color_discrete_map={'Single Role': 'blue', 'Multiple Roles': 'red'},\n",
    "    hover_data={'Role Type': True, 'Count': True}\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside', cliponaxis=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc5003",
   "metadata": {},
   "source": [
    "**Implikationen für die Modellentwicklung** \n",
    "\n",
    "Die Analyse zeigt, dass alle Clients ausschliesslich eine Rolle (OWNER oder DISPONENT) haben. Es gibt keine Clients mit mehreren Rollen. Dies bedeutet:\n",
    "Für die Modellierung der Zielvariable (has_credit_card) ist keine weitere Unterscheidung nach Mehrfachrollen erforderlich, da ein Client entweder OWNER oder DISPONENT ist, aber nicht beides gleichzeitig.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213fa6d6",
   "metadata": {},
   "source": [
    "**Frage**\n",
    "\n",
    "Regionale Differenzierung: Gibt es Distrikte, die signifikant mehr (oder weniger) Kreditkarten besitzen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dbbecd",
   "metadata": {},
   "source": [
    "**Antwort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbb23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Kreditkarten als Prozent der Einwohner\n",
    "cards_per_district = (\n",
    "    merged_df_static.groupby('district_id_client')\n",
    "    .agg(num_cards=('card_id', 'count'), n_inhabitants=('n_inhabitants', 'first'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Berechnung des Anteils der Kreditkarten an der Einwohnerzahl in Prozent\n",
    "cards_per_district['cards_per_inhabitants_percent'] = (\n",
    "    cards_per_district['num_cards'] / cards_per_district['n_inhabitants'].replace(0, np.nan)\n",
    ") * 100\n",
    "\n",
    "# Visualisierung - als Prozent\n",
    "fig = px.bar(\n",
    "    cards_per_district, \n",
    "    x='district_id_client', \n",
    "    y='cards_per_inhabitants_percent', \n",
    "    title='Percentage of Credit Cards relative to Inhabitants by District',\n",
    "    labels={\n",
    "        'district_id_client': 'District ID',\n",
    "        'cards_per_inhabitants_percent': 'Percentage of Credit Cards (%)'\n",
    "    },\n",
    "    color='cards_per_inhabitants_percent', \n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='District ID',\n",
    "    yaxis_title='Percentage of Credit Cards relative to Inhabitants (%)',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b893e",
   "metadata": {},
   "source": [
    "**Implikationen für die Modellentwicklung** \n",
    "\n",
    "- Geringe Differenzierung: Die Kreditkartendichte zeigt keine ausgeprägten regionalen Unterschiede. Dies deutet darauf hin, dass diese Variable keinen starken Einflussfaktor für die Zielvariable (Kreditkartenbesitz) darstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be369ceb",
   "metadata": {},
   "source": [
    "**Frage**\n",
    "\n",
    "Nutzungsfrequenz-Analyse: Besteht ein Zusammenhang zwischen Frequenzgruppen (`weekly`, `yearly`) und der Anzahl Kredit-Karten?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b868889",
   "metadata": {},
   "source": [
    "**Antwort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03712a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporäre Kopie mit Spalten 'frequency' und 'card_id'\n",
    "temp_df = merged_df_static[['frequency', 'card_id']].copy()\n",
    "\n",
    "# Neue Spalte: 1 wenn Karte vorhanden, sonst 0\n",
    "temp_df['has_card'] = temp_df['card_id'].notna().astype(int)\n",
    "\n",
    "# Gruppierung nach 'frequency' und 'has_card' mit Zählung\n",
    "frequency_card_counts = temp_df.groupby(['frequency', 'has_card']).size().reset_index(name='count')\n",
    "\n",
    "# Prozentuale Anteile innerhalb jeder Frequenzgruppe berechnen\n",
    "frequency_card_counts['percent'] = (\n",
    "    frequency_card_counts['count'] / frequency_card_counts.groupby('frequency')['count'].transform('sum') * 100\n",
    ").round(1)\n",
    "\n",
    "# Lesbare Beschriftung der Kreditkartenbesitz-Status\n",
    "frequency_card_counts['card_status'] = frequency_card_counts['has_card'].map({1: 'Yes', 0: 'No'})\n",
    "\n",
    "# Gruppiertes Balkendiagramm nach Frequenz und Kreditkartenbesitz\n",
    "fig = px.bar(\n",
    "    frequency_card_counts, \n",
    "    x='frequency', \n",
    "    y='percent', \n",
    "    color='card_status',\n",
    "    title='Credit Card Ownership by Frequency Group',\n",
    "    labels={'frequency': 'Frequency', 'percent': 'Percentage (%)', 'card_status': 'Has Credit Card'},\n",
    "    barmode='group', \n",
    "    color_discrete_map={'Yes': 'green', 'No': 'red'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e6176",
   "metadata": {},
   "source": [
    "Die Analyse zeigt, dass es keinen signifikanten Unterschied in der Kreditkartenbesitzrate zwischen den verschiedenen Frequenzgruppen gibt. Unabhängig von der Häufigkeit der Kartennutzung (monatlich, wöchentlich oder nach Transaktionen) besitzen fast alle Accounts keine Kreditkarte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125e7bb",
   "metadata": {},
   "source": [
    "**Implikationen für die Modellentwicklung** \n",
    "\n",
    "Da die Frequenz der Kartennutzung keinen Unterschied im Besitz von Kreditkarten macht, ist es möglich, dass dieses Feature nicht viel zur Vorhersage beiträgt --> entfernen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03584d",
   "metadata": {},
   "source": [
    "- **Verknüpfung mit anderen Tabellen**  \n",
    "Zusammenführen der Kontodaten (`account_df`) mit `district_df` (z. B. soziodemografische Merkmale) und ggf. `card_df`, um das Zusammenspiel von Distrikt, Frequenz und Kartenbesitz zu analysieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0ca6a",
   "metadata": {},
   "source": [
    "hier weitere Fragestellungen aufführen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_4_'></a>[Implikationen für die Modellentwicklung (Zusammenfassungen)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3596fb",
   "metadata": {},
   "source": [
    "\n",
    "**Ausschluss Disponenten**\n",
    "- Die Analyse bestätigt, dass ausschliesslich Einträge mit dem disposition_type `OWNER` eine Kreditkarte besitzen können. Kein DISPONENT ist mit einer Kreditkarte verknüpft.\n",
    "DISPONENTs müssen aus der Analyse ausgeschlossen werden, da sie nicht als potenzielle Kreditkartenkäufer in Betracht kommen.\n",
    "\n",
    "**Altersgrenze bei Junior-Karten**\n",
    "- Der Schwellenwert für die Vergabe einer Junior-Card wurde bei **24 Jahren** identifiziert. Ab einem Alter von **25 Jahren** werden keine Junior-Cards mehr ausgestellt.\n",
    "\n",
    "**Eindeutigkeit der Kundenzuordnung**\n",
    "- Nach dem Zusammenführen der statischen Entitäten bleibt die **`client_id` eindeutig**.  \n",
    "  Jede:r Kund:in ist durch genau **eine Zeile in `merged_df_static`** repräsentiert. Das gewährleistet eine saubere Feature- und Zielzuordnung auf Kundenebene – eine essenzielle Voraussetzung für das Klassifikationsmodell.\n",
    "\n",
    "**Account-Rollenverteilung**\n",
    "- Die Analyse zeigt, dass alle Clients ausschliesslich eine Rolle (OWNER oder DISPONENT) haben. Es gibt keine Clients mit mehreren Rollen. Dies bedeutet:\n",
    "Für die Modellierung der Zielvariable (has_credit_card) ist keine weitere Unterscheidung nach Mehrfachrollen erforderlich, da ein Client entweder OWNER oder DISPONENT ist, aber nicht beides gleichzeitig.\n",
    "\n",
    "\n",
    "**Regionale Unterschiede**\n",
    "- Geringe Differenzierung: Die Kreditkartendichte zeigt keine ausgeprägten regionalen Unterschiede. Dies deutet darauf hin, dass diese Variable keinen starken Einflussfaktor für die Zielvariable (Kreditkartenbesitz) darstellt.\n",
    "\n",
    "**Kartennutzungsfrequenz**\n",
    "- Die Frequenz der Kartennutzung (monatlich, wöchentlich etc.) zeigt **keinen klaren Zusammenhang** mit dem Besitz einer Kreditkarte.  \n",
    "  Dieses Merkmal wird daher **vorläufig beibehalten**, aber im Modellvalidierungsprozess kritisch geprüft.\n",
    "\n",
    "---\n",
    "\n",
    "**Hinweis zur Weiterverwendung der Erkenntnisse**\n",
    "\n",
    "- Alle  gewonnenen Erkenntnisse fliessen gezielt in Kapitel 6 (Modellentwicklung) ein.  Eine endgültige Auswahl oder Bereinigung von Merkmalen erfolgt jedoch **nicht an dieser Stelle**, sondern im Rahmen der **Modellvalidierung in Kapitel 6**. Auch vermeintlich schwache Merkmale werden zunächst beibehalten und später  geprüft.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a65b5",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Bereinigung Grundmenge](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_1_'></a>[Entfernen der Junior-Karten](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static = merged_df_static[merged_df_static['age'] >= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0de826",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aead36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_owners = (merged_df_static['disposition_type'] == 'OWNER').all()\n",
    "print(f\"Alle Zeilen sind OWNER: {all_owners}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_2_'></a>[Ausschluss Disponenten](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nur Zeilen mit 'disposition_type' == 'OWNER' behalten\n",
    "merged_df_static = merged_df_static[merged_df_static['disposition_type'] == 'OWNER']\n",
    "\n",
    "# Kategorien auf tatsächlich vorhandene Werte einschränken\n",
    "merged_df_static['disposition_type'] = merged_df_static['disposition_type'].cat.remove_unused_categories()\n",
    "\n",
    "print(merged_df_static['disposition_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_static.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ecc01",
   "metadata": {},
   "source": [
    "Wir haben nun alle DIsponenten ausgeschlossen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b9b63",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Modellkonstruktion](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Definitionen Kreditkarten-Käufer](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edb36c",
   "metadata": {},
   "source": [
    "Vorab erstellen wir die Zielvariable `has_cc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zielvariable erstellen (1 für Kreditkartenbesitzer, 0 für Nicht-Käufer)\n",
    "merged_df_static['has_cc'] = merged_df_static['card_id'].notna().astype(int)\n",
    "\n",
    "# Überprüfen der neuen Zielvariablen\n",
    "merged_df_static.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_1_'></a>[Käufer (`buyers_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1c67e",
   "metadata": {},
   "source": [
    "Wir erstellen ein Dataframe für die Käufer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd09e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_df = merged_df_static[merged_df_static['has_cc'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_2_'></a>[Kaufdatum](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e66bbe",
   "metadata": {},
   "source": [
    "Für die bessere Interpretation, bennen wir die Variable `issued` in `cc_purchase_date` um und löschen die Spalte `issued`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ffcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_df['cc_purchase_date'] = buyers_df['issued']\n",
    "buyers_df.drop(columns=['issued'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40915cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_3_'></a>[Rollup-Fenster (`buyers_event_info_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83bcfc",
   "metadata": {},
   "source": [
    "\n",
    "Ein Rollup-Fenster ist eine Methode, um Daten über einen bestimmten Zeitraum zu aggregieren, der sich mit jeder neuen Beobachtung verschiebt. Typischerweise werden die Transaktionen eines Kunden über einen Zeitraum von 13 Monaten zusammengefasst, um Kennzahlen wie die Summe der Transaktionen, die Transaktionshäufigkeit oder den durchschnittlichen Transaktionsbetrag zu berechnen.\n",
    "\n",
    "In unserem Fall verwenden wir jedoch 13 Monate statt der üblichen 12 Monate. Der zusätzliche Monat ist notwendig, da die vollständigen Daten eines Monats erst mit einem gewissen Zeitverzug zur Verfügung stehen (typischerweise mit einem Monat Verzögerung), bis sie an die zuständige Abteilung übermittelt wurden (Quelle: Folien \"Product Affinity Modeling\"). Durch die Nutzung von 13 Monaten stellen wir sicher, dass sowohl der aktuelle Monat (lag 0) als auch der Vergleichsmonat vom Vorjahr (lag -12) vollständig und konsistent vorliegen.\n",
    "\n",
    "Bevor jedoch das Rollup-Fenster erstellt werden kann, müssen die dynamischen Entitäten (Transaction, Loan, Order) mit den statischen Kundendaten (Account, Client) verknüpft werden. Dadurch entsteht ein vollständiger, zusammengeführter Datensatz, in dem alle relevanten Transaktions- und Kontoinformationen auf Kundenebene vorliegen.\n",
    "\n",
    "Folgend werden wir ein Rollup-Fenster mit einer Dauer von 13 Monaten erstellen, um den Vergleich mit dem Vorjahresmonat zu ermöglichen. Dies erlaubt es uns, nicht nur die historischen Transaktionsdaten zu aggregieren, sondern auch Veränderungen und saisonale Effekte gezielt zu analysieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c05e2",
   "metadata": {},
   "source": [
    "Wir fügen die `client_id` dem Dynamic DF hinzu. Dies ist für die Erstellung des Rollup-Fensters erforderlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dynamic = accounts_df\n",
    "\n",
    "merged_df_dynamic = pd.merge(merged_df_dynamic, order_df, on='account_id', how='left')\n",
    "merged_df_dynamic = pd.merge(merged_df_dynamic, transaction_df, on='account_id', how='left')\n",
    "merged_df_dynamic = pd.merge(merged_df_dynamic, loan_df, on='account_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dynamic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dynamic.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a033c2c",
   "metadata": {},
   "source": [
    "Wir sehen, dass nach dem Merge die Spalten `date`, `amount` und `k_symbol` umbenannt wurden, da sie doppelt vorkommen.\n",
    "Wir benennen diese nun wieder sinnvoll um, damit diese besser interpretierbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dynamic.rename(columns={\n",
    "    'date_x': 'account_date', \n",
    "    'date_y': 'trans_date', \n",
    "    'date': 'loan_date', \n",
    "    'amount_x': 'order_amount', \n",
    "    'amount_y': 'trans_amount',\n",
    "    'amount': 'loan_amount',\n",
    "    'k_symbol_x': 'k_symbol_order',\n",
    "    'k_symbol_y': 'k_symbol_trans'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dynamic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dynamic.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Füge 'client_id' aus merged_df_static hinzu, basierend auf 'account_id'\n",
    "merged_df_dynamic = merged_df_dynamic.merge(merged_df_static[['account_id', 'client_id']], \n",
    "                                            on='account_id', \n",
    "                                            how='left')\n",
    "\n",
    "# Überprüfe, ob 'client_id' jetzt in merged_df_dynamic vorhanden ist\n",
    "print(merged_df_dynamic.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f059450",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_dynamic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a562bf2",
   "metadata": {},
   "source": [
    "**Prüfung der Stabilität der Merkmale im Rollup-Fenster**\n",
    "\n",
    "Um sicherzustellen, dass für jeden Account innerhalb des Rollup-Fensters zeitinvariante Merkmale konsistent sind, wurde für alle relevanten Spalten geprüft, ob pro `account_id` mehr als ein einzigartiger Wert vorkommt.\n",
    "\n",
    "Variablen, bei denen mehrere Werte pro Account auftreten, werden gesondert behandelt. Für kategorische Merkmale wird dabei in der Regel der Modus (der häufigste Wert) verwendet, um eine eindeutige und konsistente Repräsentation pro Account zu gewährleisten.\n",
    "\n",
    "Neben den aggregierten Kennzahlen werden zudem Variablen berücksichtigt, die nicht in die Kennzahlenberechnung einfliessen. Diese weisen keine offensichtliche Korrelation (Kollinearität) mit den aggregierten Merkmalen auf und liefern somit unabhängige Informationen. Die Einbeziehung dieser Variablen erweitert den Informationsgehalt des Modells, ohne die Gefahr von Redundanzen oder multikollinearen Effekten zu erhöhen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für alle Spalten aussser 'account_id' prüfen, ob mehr als ein unique Wert pro account_id existiert\n",
    "unique_counts = merged_df_dynamic.groupby('account_id').nunique()\n",
    "\n",
    "# Für jede Spalte prüfen, ob es mindestens einen account_id gibt mit mehr als einem unique Wert\n",
    "for col in unique_counts.columns:\n",
    "    if (unique_counts[col] > 1).any():\n",
    "        print(f\"Es gibt Accounts mit mehr als einem einzigartigen Wert in der Spalte '{col}'.\")\n",
    "    else:\n",
    "        print(f\"Alle Accounts haben nur einen einzigartigen Wert in der Spalte '{col}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c622e",
   "metadata": {},
   "source": [
    "**Rollup-Fenster: Aggregation der Kundenkennzahlen**\n",
    "\n",
    "Die Funktion `get_rollup_window_data` aggregiert für jeden Kunden (`client_id`) und dessen Kaufdatum (`cc_purchase_date`) verschiedene Kennzahlen aus Transaktionen, Krediten und Daueraufträgen innerhalb eines definierten Rollup-Zeitfensters von **13 Monaten**.\n",
    "\n",
    "---\n",
    "\n",
    "**Ziel und Vorgehen**\n",
    "\n",
    "- Für jeden Kunden und das zugehörige Kaufdatum wird ein rückblickendes Zeitfenster von **13 Monaten** definiert (ca. 395 Tage).  \n",
    "- Alle Transaktionen, Kredite und Daueraufträge, die innerhalb dieses Zeitfensters liegen, werden gefiltert und für die Aggregation ausgewählt.  \n",
    "- Aus diesen Daten werden statistische Kennzahlen sowie dominante Kategorien berechnet, um ein kompaktes Kundenprofil zu erstellen.\n",
    "\n",
    "---\n",
    "\n",
    "**Berechnete Kennzahlen**\n",
    "\n",
    "**Transaktionen**\n",
    "\n",
    "- **total_spent**: Summe aller Transaktionsbeträge  \n",
    "- **num_transactions**: Anzahl der Transaktionen  \n",
    "- **avg_balance**, **max_balance**, **min_balance**, **std_balance**: Statistiken zum Kontostand  \n",
    "- **avg_trans_amount**, **med_trans_amount**, **max_trans_amount**, **min_trans_amount**, **std_trans_amount**: Statistiken zu den Transaktionsbeträgen  \n",
    "- **balance_before_cc**: Kontostand der letzten Transaktion vor dem Kaufdatum  \n",
    "- **transaction_type**, **transaction_operation**, **trans_k_symbol**: Dominante (modale) Kategorien aus Transaktionsart, Operation und Symbol  \n",
    "\n",
    "**Kredite**\n",
    "\n",
    "- **num_loans**: Anzahl der Kredite im Zeitfenster (höchstens 1 pro Kunde)  \n",
    "- **loan_amount**, **loan_duration**, **loan_payments**, **loan_status**: Kennzahlen und Status des Kredits  \n",
    "\n",
    "**Daueraufträge**\n",
    "\n",
    "- **num_perm_orders**: Anzahl der Daueraufträge  \n",
    "- **total_order_amount**, **avg_order_amount**: Summe und Durchschnitt der Dauerauftragsbeträge  \n",
    "- **order_k_symbol**: Dominantes Symbol der Daueraufträge  \n",
    "\n",
    "---\n",
    "\n",
    "**Datenqualität**\n",
    "\n",
    "- Fehlende numerische Werte werden mit **0** ersetzt.  \n",
    "- Fehlende kategoriale Werte werden mit **'missing'** gefüllt.\n",
    "- Für kategoriale Merkmale werden keine Dummy-Variablen erstellt. Stattdessen wird pro Merkmal die dominante (modale) Kategorie als String gespeichert, um die Anzahl der Features überschaubar zu halten und eine prägnante Repräsentation zu ermöglichen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a07cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollup_window_data(\n",
    "        df: pd.DataFrame,\n",
    "        buy_dates: pd.DataFrame,\n",
    "        rollup_window: int,\n",
    "        date_column: str = \"cc_purchase_date\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregiert Kennzahlen je (client_id, Kaufdatum) innerhalb eines Rollup-Fensters.\n",
    "\n",
    "    Enthalten sind:\n",
    "    - Transaktions-, Kredit- und Dauerauftrags-Kennzahlen\n",
    "    - KEINE Dummy-Variablen für Kategorien, sondern je Gruppe eine String-Spalte mit dominierender Klasse\n",
    "      (transaction_type, transaction_operation, trans_k_symbol, order_k_symbol)\n",
    "    - Höchstens ein Kredit pro Kunde/Fenster\n",
    "    - Rückgabe ohne NaN: numerische Werte mit 0, Strings mit 'missing' (oder leer zu 'missing')\n",
    "    \"\"\"\n",
    "\n",
    "    # Alias-Spaltennamen\n",
    "    TRANS_DATE, LOAN_DATE = \"trans_date\", \"loan_date\"\n",
    "    TRANS_AMT,  LOAN_AMT  = \"trans_amount\", \"loan_amount\"\n",
    "    ORDER_AMT             = \"order_amount\"\n",
    "    BALANCE               = \"balance\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, br in buy_dates.iterrows():\n",
    "        cid, buy_dt = br[\"client_id\"], br[date_column]\n",
    "        start_dt = buy_dt - pd.Timedelta(days=rollup_window)\n",
    "        sub = df[df[\"client_id\"] == cid]\n",
    "\n",
    "        # Transaktionen im Fenster filtern\n",
    "        t = sub[\n",
    "            sub[TRANS_DATE].notna() &\n",
    "            sub[TRANS_DATE].between(start_dt, buy_dt, inclusive=\"left\")\n",
    "        ]\n",
    "\n",
    "        # Transaktionskennzahlen berechnen\n",
    "        t_stats = {\n",
    "            \"total_spent\"      : t[TRANS_AMT].sum(),\n",
    "            \"num_transactions\" : len(t),\n",
    "            \"avg_balance\"      : t[BALANCE].mean(),\n",
    "            \"max_balance\"      : t[BALANCE].max(),\n",
    "            \"min_balance\"      : t[BALANCE].min(),\n",
    "            \"std_balance\"      : t[BALANCE].std(ddof=0),\n",
    "            \"avg_trans_amount\" : t[TRANS_AMT].mean(),\n",
    "            \"med_trans_amount\" : t[TRANS_AMT].median(),\n",
    "            \"max_trans_amount\" : t[TRANS_AMT].max(),\n",
    "            \"min_trans_amount\" : t[TRANS_AMT].min(),\n",
    "            \"std_trans_amount\" : t[TRANS_AMT].std(ddof=0),\n",
    "            \"balance_before_cc\": (\n",
    "                t.sort_values(TRANS_DATE).iloc[-1][BALANCE] if not t.empty else 0\n",
    "            ),\n",
    "            # Modus als dominante Kategorie (leere als \"\")\n",
    "            \"transaction_type\"      : t[\"type\"].mode().iat[0] if not t[\"type\"].dropna().empty else \"\",\n",
    "            \"transaction_operation\" : t[\"operation\"].mode().iat[0] if not t[\"operation\"].dropna().empty else \"\",\n",
    "            \"trans_k_symbol\"        : t[\"k_symbol_trans\"].mode().iat[0] if not t[\"k_symbol_trans\"].dropna().empty else \"\",\n",
    "        }\n",
    "\n",
    "        # Kredite (max 1) filtern und Kennzahlen setzen\n",
    "        loans = sub[\n",
    "            sub[LOAN_DATE].notna() &\n",
    "            sub[LOAN_DATE].between(start_dt, buy_dt, inclusive=\"left\")\n",
    "        ]\n",
    "        if \"loan_id\" in loans.columns:\n",
    "            loans = loans.drop_duplicates(\"loan_id\")\n",
    "\n",
    "        if loans.empty:\n",
    "            l_stats = dict(num_loans=0, loan_amount=0, loan_duration=0, loan_payments=0, loan_status=\"\")\n",
    "        else:\n",
    "            lo = loans.iloc[0]\n",
    "            l_stats = dict(\n",
    "                num_loans=1,\n",
    "                loan_amount=lo[LOAN_AMT],\n",
    "                loan_duration=lo[\"duration\"],\n",
    "                loan_payments=lo[\"payments\"],\n",
    "                loan_status=str(lo[\"status\"])\n",
    "            )\n",
    "\n",
    "        # Daueraufträge filtern und Kennzahlen setzen\n",
    "        orders = sub[sub[ORDER_AMT].notna()]\n",
    "        if \"order_id\" in orders.columns:\n",
    "            orders = orders.drop_duplicates(\"order_id\")\n",
    "\n",
    "        o_stats = {\n",
    "            \"num_perm_orders\": len(orders),\n",
    "            \"total_order_amount\": orders[ORDER_AMT].sum(),\n",
    "            \"avg_order_amount\": orders[ORDER_AMT].mean(),\n",
    "            \"order_k_symbol\": (\n",
    "                orders[\"k_symbol_order\"].mode().iat[0] if not orders[\"k_symbol_order\"].dropna().empty else \"\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Ergebniszeile sammeln\n",
    "        rows.append({\"client_id\": cid, **t_stats, **l_stats, **o_stats})\n",
    "\n",
    "    # DataFrame erstellen und NaN-Werte ersetzen\n",
    "    rollup_df = pd.DataFrame(rows)\n",
    "\n",
    "    num_cols = rollup_df.select_dtypes(include=\"number\").columns\n",
    "    obj_cols = rollup_df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "    rollup_df[num_cols] = rollup_df[num_cols].fillna(0)\n",
    "    rollup_df[obj_cols] = rollup_df[obj_cols].fillna(\"missing\")\n",
    "    rollup_df[obj_cols] = rollup_df[obj_cols].replace('', 'missing')\n",
    "\n",
    "    return rollup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_df = buyers_df.reset_index(drop=True)\n",
    "\n",
    "# Rollup-Daten für Kreditkartenkäufer im definierten Fenster erzeugen\n",
    "buyers_event_info_df = get_rollup_window_data(\n",
    "    df=merged_df_dynamic,\n",
    "    buy_dates=buyers_df[['client_id', 'cc_purchase_date']],\n",
    "    rollup_window=395,\n",
    "    date_column='cc_purchase_date'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_event_info_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90711649",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_event_info_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156066d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers_event_info_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648780f5",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Definitionen Kreditkarten-Nichtkäufer](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Nichtkäufer (`non_buyers_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93542cb0",
   "metadata": {},
   "source": [
    "Wir erstellen ein Dataframe für die Nichtkäufer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20873567",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_buyers_df = merged_df_static[merged_df_static['has_cc'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Kaufdatum](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06787f20",
   "metadata": {},
   "source": [
    "Um Kreditkartenkäufer und Nicht-Käufer fair vergleichen zu können, ist es entscheidend, dass beide Gruppen im Vorfeld des Ereignisses – dem Kreditkartenkauf – denselben externen Marktbedingungen ausgesetzt waren. Dazu zählen etwa saisonale Effekte oder wirtschaftliche Rahmenbedingungen, die sich auf das Kundenverhalten auswirken können.\n",
    "\n",
    "Da bei den Nicht-Käufern kein tatsächlicher Kaufzeitpunkt existiert, wurde jedem Nicht-Käufer ein **„Pseudo-Kaufdatum“** zugewiesen. Dieses Datum wurde zufällig aus der Verteilung der echten Kaufzeitpunkte der Käufer gezogen. So wird sichergestellt, dass die für die Modellerstellung verwendeten Transaktionshistorien auf vergleichbaren Zeiträumen basieren.\n",
    "\n",
    "Das ermöglicht es, für beide Gruppen **ein 13-monatiges Rollup-Fenster** vor dem (echten oder pseudo) Kaufzeitpunkt zu analysieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Schritt 1: Liste der echten Kaufdaten\n",
    "purchchase_dates = buyers_df['cc_purchase_date'].dropna()\n",
    "\n",
    "purchchase_dates.head()\n",
    "\n",
    "# Schritt 2 : Zufälliges Sample für Nicht-Käufer ziehen\n",
    "non_buyers_df = non_buyers_df.copy()\n",
    "non_buyers_df['pseudo_purchase_date'] = np.random.choice(purchchase_dates, size=len(non_buyers_df), replace=True)\n",
    "\n",
    "# Schritt 2: Zufälliges Sample für Nicht-Käufer ziehen\n",
    "non_buyers_df = non_buyers_df.copy()\n",
    "non_buyers_df['pseudo_purchase_date'] = np.random.choice(purchchase_dates, size=len(non_buyers_df), replace=True)\n",
    "\n",
    "non_buyers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbbe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich der Verteilung der tatsächlichen Kaufdaten von Käufern mit Pseudo-Kaufdaten von Nicht-Käufern\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Buyers - Purchase Date\", \"Non-Buyers - Pseudo Purchase Date\"))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=buyers_df['cc_purchase_date'],\n",
    "    name=\"Buyers\",\n",
    "    histnorm='probability'  \n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=non_buyers_df['pseudo_purchase_date'],\n",
    "    name=\"Non-Buyers\",\n",
    "    histnorm='probability'  \n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='overlay',\n",
    "    showlegend=False,\n",
    "    title_text=\"Histogram of Purchase Dates: Buyers vs. Non-Buyers with Pseudo Purchase Dates\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a830f7",
   "metadata": {},
   "source": [
    "- **Links**: Verteilung der tatsächlichen Kaufzeitpunkte (`cc_purchase_date`) von Kreditkartenkäufern.\n",
    "- **Rechts**: Verteilung der zugewiesenen Pseudo-Kaufzeitpunkte (`pseudo_purchase_date`) für Nicht-Käufer.\n",
    "\n",
    "Die Pseudo-Kaufzeitpunkte wurden durch zufällige Ziehung (mit Zurücklegen / replace = True ) aus der Verteilung der echten Kaufdaten generiert. Das ist wichtig, weil wir mehr Nicht-Käufer haben, weniger Käufer  – also müssen Wiederholungen erlaubt sein, sonst reicht die Menge nicht.\n",
    "\n",
    "Ziel war es, sicherzustellen, dass Käufer und Nicht-Käufer im gleichen zeitlichen Kontext analysiert werden – insbesondere in Hinblick auf externe Marktbedingungen wie Saisonalität oder wirtschaftliche Zyklen.\n",
    "\n",
    "Die Verteilung der beiden Gruppen ist nahezu identisch. Das zeigt, dass die Methode zur Erzeugung der Pseudo-Kaufdaten funktioniert hat: Die zeitliche Streuung der Kaufentscheidungen wurde erfolgreich auf die Nicht-Käufer übertragen. Somit können beide Gruppen auf vergleichbarer zeitlicher Grundlage analysiert werden.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_3_'></a>[Rollup-Fenster (`non_buyers_event_info_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollup-Daten für Nicht-Käufer basierend auf pseudo Kaufdatum erstellen\n",
    "non_buyers_event_info_df = get_rollup_window_data(\n",
    "    df=merged_df_dynamic,\n",
    "    buy_dates=non_buyers_df[['client_id', 'pseudo_purchase_date']],\n",
    "    rollup_window=395,\n",
    "    date_column='pseudo_purchase_date'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516eac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_buyers_event_info_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_buyers_event_info_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_buyers_event_info_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[EDA Käufer/Nichtkäufer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e724ac8",
   "metadata": {},
   "source": [
    "Nun vergleichen wir die Daten der Käufer und Nichtkäufer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c612c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Käufer: 'month' anhand von Kaufdatum (cc_purchase_date) mappen und in datetime umwandeln\n",
    "buyers_event_info_df['month'] = pd.to_datetime(\n",
    "    buyers_event_info_df['client_id'].map(\n",
    "        buyers_df.set_index('client_id')['cc_purchase_date']\n",
    "    )\n",
    ")\n",
    "# Nur Einträge mit gültigem Datum behalten\n",
    "buyers_event_info_df = buyers_event_info_df[buyers_event_info_df['month'].notna()]\n",
    "# Perioden in Timestamp konvertieren (erster Tag des Monats)\n",
    "buyers_event_info_df['month'] = buyers_event_info_df['month'].dt.to_period('M').dt.to_timestamp()\n",
    "# Monatliche Mittelwerte berechnen für ausgewählte numerische Features\n",
    "buyers_monthly = buyers_event_info_df.groupby('month')[['avg_balance', 'avg_trans_amount', 'avg_order_amount']].mean().reset_index()\n",
    "buyers_monthly['group'] = 'Buyer'  # Label zur Gruppenzuordnung\n",
    "\n",
    "# Nicht-Käufer: 'month' anhand von Pseudo-Kaufdatum mappen und konvertieren\n",
    "non_buyers_event_info_df['month'] = pd.to_datetime(\n",
    "    non_buyers_event_info_df['client_id'].map(\n",
    "        non_buyers_df.set_index('client_id')['pseudo_purchase_date']\n",
    "    )\n",
    ")\n",
    "non_buyers_event_info_df = non_buyers_event_info_df[non_buyers_event_info_df['month'].notna()]\n",
    "non_buyers_event_info_df['month'] = non_buyers_event_info_df['month'].dt.to_period('M').dt.to_timestamp()\n",
    "non_buyers_monthly = non_buyers_event_info_df.groupby('month')[['avg_balance', 'avg_trans_amount', 'avg_order_amount']].mean().reset_index()\n",
    "non_buyers_monthly['group'] = 'Non-Buyer'\n",
    "\n",
    "# Käufer- und Nicht-Käufer-Daten zusammenführen\n",
    "combined = pd.concat([buyers_monthly, non_buyers_monthly])\n",
    "\n",
    "# Liste der Features, die einzeln geplottet werden sollen\n",
    "features_to_plot = ['avg_balance', 'avg_trans_amount', 'avg_order_amount']\n",
    "\n",
    "# Für jede Kennzahl einen separaten Zeitreihenplot erstellen\n",
    "for feature in features_to_plot:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=combined, x='month', y=feature, hue='group', marker=\"o\")\n",
    "    plt.title(f'Zeitverlauf {feature} (Käufer vs. Nicht-Käufer)')\n",
    "    plt.xlabel('Zeitverlauf')\n",
    "    plt.ylabel(f'{feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2819d",
   "metadata": {},
   "source": [
    "**avg_balance**\n",
    "\n",
    "- Käufer haben über die gesamte Zeitspanne hinweg deutlich ein höheres durchschnittliches Guthaben als Nicht-Käufer.\n",
    "- Bei den Käufern bleibt das Guthaben relativ stabil und auf einem hohen Niveau, während das Guthaben der Nicht-Käufer im Verlauf langsam ansteigt, aber deutlich darunter bleibt.\n",
    "\n",
    "**avg_trans_amount**\n",
    "\n",
    "- Anfangs sind die durchschnittlichen Transaktionsbeträge bei Käufern meist höher als bei Nicht-Käufern.\n",
    "- Im Zeitverlauf nähern sich die Werte an und wechseln teilweise die Führung, was auf eine ähnliche Nutzung oder Transaktionshöhe bei beiden Gruppen in späteren Jahren hinweist.\n",
    "- Die Schwankungen sind bei beiden Gruppen deutlich, was auf unregelmässige Transaktionen schliessen lässt.\n",
    "\n",
    "**avg_order_amount**\n",
    "\n",
    "- Die Bestellwerte sind anfangs bei Käufern und Nicht-Käufern sehr schwankend.\n",
    "- Im weiteren Verlauf tendieren die Bestellwerte der Käufer etwas höher zu bleiben, jedoch nähern sich die Werte im Zeitverlauf an und verlaufen recht ähnlich.\n",
    "- Beide Gruppen zeigen eine zunehmende Stabilisierung der Bestellwerte im Zeitverlauf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ffcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste wichtiger numerischer Features für Dichtevergleiche\n",
    "num_features = ['total_spent', 'avg_balance', 'num_transactions', 'loan_amount']\n",
    "\n",
    "# Für jedes Feature KDE-Plots erstellen, um Verteilungen von Käufern und Nicht-Käufern zu vergleichen\n",
    "for feature in num_features:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.kdeplot(data=buyers_event_info_df, x=feature, label='Buyer', fill=True)       # Käufer\n",
    "    sns.kdeplot(data=non_buyers_event_info_df, x=feature, label='Non-Buyer', fill=True) # Nicht-Käufer\n",
    "    plt.title(f'Verteilung von {feature}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda14298",
   "metadata": {},
   "source": [
    "**total_spent**\n",
    "\n",
    "- Käufer (Buyer) tendieren dazu, im Durchschnitt mehr auszugeben als Nicht-Käufer.\n",
    "- Die Verteilung der Käufer ist etwas breiter und zeigt rechtsseitige Ausreisser (grössere Ausgaben).\n",
    "- Nicht-Käufer haben eine schmalere, konzentrierte Verteilung, überwiegend im unteren Ausgabenbereich.\n",
    "\n",
    "**avg_balance**\n",
    "\n",
    "- Käufer haben tendenziell ein höheres durchschnittliches Guthaben.\n",
    "- Die Verteilungen der beiden Gruppen unterscheiden sich deutlich: Käufer haben einen Peak bei höheren Guthabenwerten, Nicht-Käufer bei niedrigeren.\n",
    "- Die Überlappung ist gering, was auf eine gute Trennbarkeit dieser Variable hinweist.\n",
    "\n",
    "**num_transactions**\n",
    "\n",
    "- Beide Gruppen haben eine ähnliche Verteilung bei der Anzahl Transaktionen.\n",
    "- Es gibt eine leichte Verschiebung: Käufer haben tendenziell mehr Transaktionen.\n",
    "- Die Verteilungen sind aber sehr ähnlich und stark überlappend.\n",
    "\n",
    "**loan_amount**\n",
    "\n",
    "- Die Verteilungen der Käufer und Nicht-Käufer sind nahezu identisch, beide sehr stark links konzentriert (niedrige Beträge). Wobei die Konzentration bei Nicht-Käufern höher ist.\n",
    "- Es gibt nur wenige Ausreisser mit sehr hohen Darlehensbeträgen, und diese sind bei beiden Gruppen ähnlich verteilt.\n",
    "- Diese Variable scheint wenig Unterschied zwischen den Gruppen zu zeigen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33d478",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Feature Engineering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[Kombination eventbezogener Informationen (`combined_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14df7f",
   "metadata": {},
   "source": [
    "Wir führen nun die beiden Dataframes zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombinieren der DataFrames für Käufer und Nicht-Käufer\n",
    "combined_event_info_df = pd.concat([buyers_event_info_df, non_buyers_event_info_df], ignore_index=True)\n",
    "\n",
    "# Überprüfen des kombinierten DataFrames\n",
    "combined_event_info_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_event_info_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_event_info_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31bead",
   "metadata": {},
   "source": [
    "Nun mergen wir den Datensatz mit den Infos des Rollupfensters der Käufer/Nichtkäufer zusammen mit den statischen Entitäten auf Basis der `client_id` zum finalen Datensatz `main_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_event_info_df.merge(merged_df_static, on=\"client_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0378d",
   "metadata": {},
   "source": [
    "Wir benennen nun noch sämtliche demografischen Variablen um damit klar ist, dass diese sich auf den Kunden beziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.rename(columns={\n",
    "    'district_name': 'client_district_name',\n",
    "    'region': 'client_region',\n",
    "    'n_inhabitants': 'client_n_inhabitants',\n",
    "    'n_municipals_lower_499': 'client_n_municipals_lower_499',\n",
    "    'n_municipals_between_500_1999': 'client_n_municipals_between_500_1999',\n",
    "    'n_municipals_between_2000_9999': 'client_n_municipals_between_2000_9999',\n",
    "    'n_municipals_higher_10000': 'client_n_municipals_higher_10000',\n",
    "    'n_cities': 'client_n_cities',\n",
    "    'ratio_urban_inhabitants': 'client_ratio_urban_inhabitants',\n",
    "    'avg_salary': 'client_avg_salary',\n",
    "    'unemployment_rate_1995': 'client_unemployment_rate_1995',\n",
    "    'unemployment_rate_1996': 'client_unemployment_rate_1996',\n",
    "    'n_enterpreneurs_per_1k_inhabitants': 'client_n_enterpreneurs_per_1k_inhabitants',\n",
    "    'n_crimes_1995': 'client_n_crimes_1995',\n",
    "    'n_crimes_1996': 'client_n_crimes_1996'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04571732",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[Bereinigung und EDA (`final_df`)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e292e7",
   "metadata": {},
   "source": [
    "Wir prüfen den Datensatz auf final auf fehlende Werte oder Duplikate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isna().sum().sort_values(ascending=False) # Anzahl der fehlenden Werte pro Spalte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f5eb6",
   "metadata": {},
   "source": [
    "Wir sehen bei den Variablen `card_id`, `credit_card_type` und `issued` (somit bei sämtlichen kreditkartenspezfischen Informationen), dass die Anzahl der NA-Werte exakt der Anzahl Nichtkäufer entspricht. Zusätzlich fehlen bei den Variablen `client_n_crimes_1995` und `client_unemployment_rate_1995` jeweils 44 Werte.  Die fehlenden Werte in den beiden demografischen Variablen werden wir im weiteren Verlauf im Rahmen des Modelltrainings mit einer geeigneten **Imputationsstrategie** (Median) behandeln, um ihre Informationen nutzbar zu machen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ceefe",
   "metadata": {},
   "source": [
    "Wir entfernen die Spalten `birth_number`, `account_id`, `disp_id`, `district_id_client`, `client_id`, `disposition_type` und `district_id_account` aus dem Datensatz, da diese für das Training keinen Mehrwert bringen.\n",
    "\n",
    "Zudem müssen wir unbedingt die Spalten `card_id`, `credit_card_type` und `issued` entfernen. Dies sind 'leakende' Features. Heisst, diese verraten direkt den Wert der Zielvariable `has_cc` (Data Leakage!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32019b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = combined_df.drop(columns=['birth_number', \n",
    "                                     'account_id', \n",
    "                                     'disp_id', \n",
    "                                     'district_id_client', \n",
    "                                     'client_id', \n",
    "                                     'disposition_type', \n",
    "                                     'disctrict_id_account', \n",
    "                                     'card_id', \n",
    "                                     'credit_card_type', \n",
    "                                     'issued'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80278b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36716242",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fe59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f7ffb",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Modellentwicklung](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[Partitionierung Trainings- und Testdaten und NaN Imputation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62083b43",
   "metadata": {},
   "source": [
    "Wir unterteilen den Datensatz in 80% Trainingsdaten und 20% Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Aufteilung der Daten in Trainings- und Testsets\n",
    "# - 80% Training, 20% Test\n",
    "# - stratified, um Klassenverteilung in Zielvariable beizubehalten\n",
    "target = \"has_cc\"                      # Zielvariable: Kreditkartenbesitz\n",
    "X = final_df.drop(columns=[target])   # Features ohne Zielvariable\n",
    "y = final_df[target]                   # Zielvariable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(X_train.columns)  # Spaltenübersicht der Trainingsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf31c4",
   "metadata": {},
   "source": [
    "Wir untersuchen nun die Trainings- und Testdaten auf NaN-Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fehlende Werte in Trainingsdaten prüfen ---\n",
    "print(\"Fehlende Werte im TRAININGSDATENSATZ:\")\n",
    "\n",
    "# Gesamtzahl der fehlenden Werte und prozentualer Anteil an allen Zellen\n",
    "total_nans_train = X_train.isnull().sum().sum()\n",
    "total_cells_train = X_train.shape[0] * X_train.shape[1]\n",
    "nan_percentage_train = (total_nans_train / total_cells_train) * 100\n",
    "print(f\"Fehlende Werte: {total_nans_train} von {total_cells_train} Zellen ({nan_percentage_train:.2f}%)\")\n",
    "\n",
    "# Detaillierte Übersicht: Anzahl und Prozentsatz fehlender Werte je Spalte, sortiert nach Anteil\n",
    "nan_summary_train = X_train.isnull().sum().to_frame(name=\"NaN Anzahl\")\n",
    "nan_summary_train[\"Gesamt\"] = len(X_train)\n",
    "nan_summary_train[\"NaN %\"] = (nan_summary_train[\"NaN Anzahl\"] / nan_summary_train[\"Gesamt\"]) * 100\n",
    "nan_summary_train = nan_summary_train[nan_summary_train[\"NaN Anzahl\"] > 0]\n",
    "nan_summary_train = nan_summary_train.sort_values(\"NaN %\", ascending=False)\n",
    "print(nan_summary_train)\n",
    "\n",
    "# --- Fehlende Werte in Testdaten prüfen ---\n",
    "print(\"\\nFehlende Werte im TESTDATENSATZ:\")\n",
    "\n",
    "total_nans_test = X_test.isnull().sum().sum()\n",
    "total_cells_test = X_test.shape[0] * X_test.shape[1]\n",
    "nan_percentage_test = (total_nans_test / total_cells_test) * 100\n",
    "print(f\"Fehlende Werte: {total_nans_test} von {total_cells_test} Zellen ({nan_percentage_test:.2f}%)\")\n",
    "\n",
    "nan_summary_test = X_test.isnull().sum().to_frame(name=\"NaN Anzahl\")\n",
    "nan_summary_test[\"Gesamt\"] = len(X_test)\n",
    "nan_summary_test[\"NaN %\"] = (nan_summary_test[\"NaN Anzahl\"] / nan_summary_test[\"Gesamt\"]) * 100\n",
    "nan_summary_test = nan_summary_test[nan_summary_test[\"NaN Anzahl\"] > 0]\n",
    "nan_summary_test = nan_summary_test.sort_values(\"NaN %\", ascending=False)\n",
    "print(nan_summary_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8dc8c",
   "metadata": {},
   "source": [
    "Dass die Variablen `client_unemployment_rate_1995` und `client_n_crimes_1995` NaN Werte haben ist klar: Wir haben beim Einlesen des Datensatzes Fragezeichen-Werte mit NaN ersetzt.\n",
    "\n",
    "Nun ersetzen wir die fehlenden Werte in den Features `client_unemployment_rate_1995` und `client_n_crimes_1995` durch den Median der jeweiligen Spalte. Dabei wird der Median ausschliesslich auf Basis der Trainingsdaten (X_train) berechnet, um ein Datenleck zu vermeiden. Das heisst, es sollen keine Informationen aus dem Testdatensatz (X_test) in das Modelltraining einfliessen. Anschliessend wird dieser auf dem Training berechnete Median auch auf die Testdaten angewendet, um die Daten konsistent und vollständig für die Modellierung vorzubereiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ca979",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_impute = [\"client_unemployment_rate_1995\", \"client_n_crimes_1995\"]\n",
    "\n",
    "# Median nur aus Training berechnen\n",
    "medians = X_train[cols_to_impute].median()\n",
    "\n",
    "# Auf beide Datensätze anwenden\n",
    "X_train[cols_to_impute] = X_train[cols_to_impute].fillna(medians)\n",
    "X_test[cols_to_impute]  = X_test[cols_to_impute].fillna(medians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3209768",
   "metadata": {},
   "source": [
    "Wir prüfen nochmals die NaN-Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fehlende Werte im TRAININGSDATENSATZ:\")\n",
    "\n",
    "total_nans_train = X_train.isnull().sum().sum()\n",
    "total_cells_train = X_train.shape[0] * X_train.shape[1]\n",
    "nan_percentage_train = (total_nans_train / total_cells_train) * 100\n",
    "\n",
    "print(f\"Fehlende Werte: {total_nans_train} von {total_cells_train} Zellen ({nan_percentage_train:.2f}%)\")\n",
    "\n",
    "nan_summary_train = X_train.isnull().sum().to_frame(name=\"NaN Anzahl\")\n",
    "nan_summary_train[\"Gesamt\"] = len(X_train)\n",
    "nan_summary_train[\"NaN %\"] = (nan_summary_train[\"NaN Anzahl\"] / nan_summary_train[\"Gesamt\"]) * 100\n",
    "nan_summary_train = nan_summary_train[nan_summary_train[\"NaN Anzahl\"] > 0]\n",
    "nan_summary_train = nan_summary_train.sort_values(\"NaN %\", ascending=False)\n",
    "\n",
    "print(nan_summary_train)\n",
    "\n",
    "\n",
    "print(\"\\nFehlende Werte im TESTDATENSATZ:\")\n",
    "\n",
    "total_nans_test = X_test.isnull().sum().sum()\n",
    "total_cells_test = X_test.shape[0] * X_test.shape[1]\n",
    "nan_percentage_test = (total_nans_test / total_cells_test) * 100\n",
    "\n",
    "print(f\"Fehlende Werte: {total_nans_test} von {total_cells_test} Zellen ({nan_percentage_test:.2f}%)\")\n",
    "\n",
    "nan_summary_test = X_test.isnull().sum().to_frame(name=\"NaN Anzahl\")\n",
    "nan_summary_test[\"Gesamt\"] = len(X_test)\n",
    "nan_summary_test[\"NaN %\"] = (nan_summary_test[\"NaN Anzahl\"] / nan_summary_test[\"Gesamt\"]) * 100\n",
    "nan_summary_test = nan_summary_test[nan_summary_test[\"NaN Anzahl\"] > 0]\n",
    "nan_summary_test = nan_summary_test.sort_values(\"NaN %\", ascending=False)\n",
    "\n",
    "print(nan_summary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7403b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66347a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Zeilen in Ausgaben erlauben\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "display(\n",
    "    X_train.dtypes\n",
    "           .reset_index()\n",
    "           .rename(columns={'index': 'column', 0: 'dtype'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed14700",
   "metadata": {},
   "source": [
    "## <a id='toc7_2_'></a>[Baseline-Modell (Logistic Regression)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_2_1_'></a>[Training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa537c",
   "metadata": {},
   "source": [
    "Da die Zielvariable \"has_cc\" unbalanciert ist, untersuchen wir, ob Sampling-Strategien einen positiven Einfluss auf die Modellleistung der logistischen Regression haben. Neben der Standardvariante mit `class_weight='balanced'` vergleichen wir zwei alternative Ansätze:\n",
    "\n",
    "- **Synthetic Minority Over-sampling Technique (SMOTE)**: synthetisches Oversampling der Minderheitsklasse\n",
    "- **Random Undersampling**: zufällige Reduktion der Mehrheitsklasse\n",
    "\n",
    "Alle drei Varianten werden in einheitlichen Pipelines implementiert und mit identischem Preprocessing versehen. Anschliessend evaluieren wir die Modelle per 5-facher Cross-Validation anhand gängiger Klassifikationsmetriken (Accuracy, Precision, Recall, F1, ROC-AUC), um festzustellen, ob sich ein Samplingverfahren gegenüber der Standardgewichtung als vorteilhaft erweist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db598bff",
   "metadata": {},
   "source": [
    "Wir extrahieren die relevanten Variablen für die Bildung des Baselinemodells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = ['age', 'gender', 'client_region', 'total_spent', 'balance_before_cc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) Aufteilen der Features in numerische und kategoriale Spalten\n",
    "#    Numerische Spalten werden standardisiert, kategoriale one-hot-kodiert\n",
    "num_cols = ['age', 'total_spent', 'balance_before_cc']\n",
    "cat_cols = ['gender', 'client_region']\n",
    "\n",
    "# 2) Aufbau des Preprocessing mit ColumnTransformer\n",
    "#    Numerische Features: StandardScaler (Mittelwert 0, Varianz 1)\n",
    "#    Kategoriale Features: OneHotEncoder mit Drop-First, um Multikollinearität zu vermeiden\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first'), cat_cols)\n",
    "])\n",
    "\n",
    "# 3) Definition von drei Modell-Pipelines mit unterschiedlichen Methoden zur Behandlung von Klassenungleichgewicht\n",
    "\n",
    "# Baseline mit LogisticRegression und class_weight='balanced' zur Gewichtung seltener Klassen\n",
    "baseline_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=10_000,\n",
    "        class_weight='balanced',\n",
    "        solver='lbfgs',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Pipeline mit SMOTE (Synthetic Minority Over-sampling Technique) zur künstlichen Erzeugung von Minderheitsklasse\n",
    "pipe_smote = ImbPipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=10_000,\n",
    "        solver='lbfgs',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Pipeline mit RandomUnderSampler zur Reduktion der Mehrheitsklasse\n",
    "pipe_undersample = ImbPipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('undersample', RandomUnderSampler(random_state=42)),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=10_000,\n",
    "        solver='lbfgs',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 4) Cross-Validation mit stratified KFold (5 Splits), um die Klassenzusammensetzung stabil zu halten\n",
    "#    Evaluierung mit mehreren Metriken: accuracy, precision, recall, f1, roc_auc\n",
    "pipelines = {\n",
    "    \"Baseline (class_weight)\": baseline_pipeline,\n",
    "    \"SMOTE\": pipe_smote,\n",
    "    \"Undersampling\": pipe_undersample\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "cv_results = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    # Cross-Validation für jedes Pipeline-Modell durchführen\n",
    "    scores = cross_validate(pipe, X_train[num_cols + cat_cols], y_train,\n",
    "                            cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    # Mittelwerte der Metriken sammeln\n",
    "    cv_results[name] = {m: scores[f'test_{m}'].mean() for m in scoring}\n",
    "\n",
    "# 5) Ergebnisse in einem DataFrame übersichtlich zusammenfassen und auf drei Dezimalstellen runden\n",
    "results_df = pd.DataFrame(cv_results).T.round(3)\n",
    "\n",
    "# Tabelle anzeigen\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720059c1",
   "metadata": {},
   "source": [
    "Alle drei Varianten der logistischen Regression – mit class_weight='balanced', SMOTE und Undersampling – liefern nahezu identische Ergebnisse über alle bewerteten Metriken hinweg.\n",
    "Oversampling (SMOT) und Undersampling bringen im Vergleich zu class_weight='balanced' keine signifikanten Performancegewinne.\n",
    "Daher ist die einfachste Variante (class_weight) weiterhin gut vertretbar – insbesondere aufgrund ihrer Effizienz und Robustheit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b566e",
   "metadata": {},
   "source": [
    "Nun trainieren wir das Modell auf dem gesamten Trainingsdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b58bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit auf gesamtem Trainingsdatensatz:\n",
    "baseline_pipeline.fit(X_train[baseline_features], y_train)\n",
    "\n",
    "# Vorhersagen & Wahrscheinlichkeiten\n",
    "y_pred_test = baseline_pipeline.predict(X_test[baseline_features])\n",
    "y_proba_test = baseline_pipeline.predict_proba(X_test[baseline_features])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_2_2_'></a>[Evaluation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf31c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikationsbericht\n",
    "print(\"\\n=== Klassifikationsbericht (Test-Set) – Baseline Logistic Regression (Pipeline) ===\")\n",
    "print(classification_report(y_test, y_pred_test, digits=3))\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_proba_test)\n",
    "print(f\"ROC-AUC (Test-Set): {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fdcb3",
   "metadata": {},
   "source": [
    "**Klasse 0 (Negativ-Fälle, also Nicht-Käufer)**\n",
    "- **Präzision:** `0.92`  \n",
    "  → Sehr wenige False Positives, d.h. kaum Fehlalarme unter den vorhergesagten Negativen.\n",
    "- **Recall:** `0.79`  \n",
    "  → Rund 79 % der tatsächlichen Negativ-Fälle werden korrekt erkannt. Etwa jeder fünfte Fall bleibt unentdeckt.\n",
    "\n",
    "**Klasse 1 (Positiv-Fälle, also Käufer)**\n",
    "- **Präzision:** `0.43`  \n",
    "  → Nur etwa 43 % der als Käufer vorhergesagten Personen sind tatsächlich Käufer. Es gibt also viele Fehlalarme.\n",
    "- **Recall:** `0.71`  \n",
    "  → Immerhin 71 % der tatsächlichen Käufer werden erkannt – ein ordentlicher Wert, aber rund ein Drittel bleibt unentdeckt.\n",
    "\n",
    "**Gesamtmetriken**\n",
    "\n",
    "**Gesamtmetriken**\n",
    "\n",
    "- **Accuracy:** `0.775`  \n",
    "  → Auf den ersten Blick solide, aber durch das Klassenungleichgewicht (ca. 82 % Klasse 0) wenig aussagekräftig.\n",
    "- **Macro-F1:** `0.694`  \n",
    "  → Durchschnittlicher F1-Score über beide Klassen – zeigt die Modellleistung unabhängig von der Klassenverteilung.\n",
    "- **Weighted-F1:** `0.794`  \n",
    "  → Gewichteter F1-Score nach Klassenhäufigkeit. Fällt höher aus, da die dominante Klasse 0 stark ins Gewicht fällt.\n",
    "- **ROC-AUC:** `0.837`  \n",
    "  → Das Modell trennt Käufer und Nicht-Käufer solide – ein guter Ausgangspunkt für Verbesserungen.\n",
    "\n",
    "\n",
    "**Fazit**\n",
    "Das Baseline-Modell erzielt eine ordentliche Trennschärfe (**ROC-AUC 0.837**), hat aber klare Schwächen bei der Vorhersage der Minderheitsklasse.  Insbesondere die **Präzision für Klasse 1 ist niedrig**, was zu vielen Fehlalarmen bei potenziellen Zielkunden führt.  Hier besteht **deutliches Verbesserungspotenzial** durch Feature Engineering oder alternative Modelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657026c5",
   "metadata": {},
   "source": [
    "Wir visualisieren nun die Fehlerarten mit einer Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75535ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix – Test-Set\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_pred_test, normalize='true')\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_test, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Confusion Matrix (Test-Set) – Baseline Logistic Regression\")\n",
    "plt.xlabel(\"Vorhergesagt\")\n",
    "plt.ylabel(\"Tatsächlich\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91f6f6",
   "metadata": {},
   "source": [
    "Wir zeigen die ROC-Kurve zur Beurteilung der Trennschärfe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC-Kurve Werte berechnen\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_test)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve mit markierten Thresholds\")\n",
    "\n",
    "# Schwellenwerte einzeichnen\n",
    "for i in range(0, len(thresholds), len(thresholds)//10):\n",
    "    plt.annotate(f\"{thresholds[i]:.2f}\", (fpr[i], tpr[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345691ae",
   "metadata": {},
   "source": [
    "**ROC-Kurven-Analyse – Test-Set**\n",
    "\n",
    "Die ROC-Kurve zeigt die Modellgüte über verschiedene Schwellenwerte hinweg anhand des Verhältnisses von:\n",
    "\n",
    "- **True Positive Rate (TPR / Recall):**  \n",
    "  Anteil korrekt erkannter Käufer (Sensitivität)\n",
    "- **False Positive Rate (FPR):**  \n",
    "  Anteil fälschlich als Käufer klassifizierter Nicht-Käufer (Fehlalarme)\n",
    "\n",
    "---\n",
    "\n",
    "**Was misst die ROC-AUC?**\n",
    "\n",
    "- **AUC (Area Under Curve)** ist die Fläche unter der ROC-Kurve.\n",
    "- Sie liegt zwischen:\n",
    "  - **1.0** → perfektes Modell (immer korrekt sortiert)\n",
    "  - **0.5** → reines Raten (keine Trennfähigkeit)\n",
    "  - **< 0.5** → schlechter als Zufall (systematisch falsch sortiert)\n",
    "\n",
    "Die AUC misst die Wahrscheinlichkeit, dass das Modell einem zufällig gewählten Käufer eine höhere Wahrscheinlichkeit zuweist als einem zufällig gewählten Nicht-Käufer.\n",
    "\n",
    "\n",
    "**Beobachtungen zum Modell (Test-Set)**\n",
    "\n",
    "- Die AUC beträgt **0.84**, was auf eine **gute Trennschärfe** des Modells hinweist. In **84 % aller möglichen Vergleichspaare** aus einem zufälligen Käufer und einem zufälligen Nicht-Käufer weist das Modell dem Käufer eine **höhere Wahrscheinlichkeit (Score)** zu als dem Nicht-Käufer.\n",
    "- Bei einer **False Positive Rate von ca. 20 %** erkennt das Modell etwa **70 % der tatsächlichen Käufer (Recall)**.\n",
    "- Ab einer **FPR von ca. 40 %** steigt der Recall nur noch minimal – es kommen viele neue Fehlalarme, aber kaum neue echte Treffer dazu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b9c6f7",
   "metadata": {},
   "source": [
    "## <a id='toc7_3_'></a>[Verbesserung Baseline-Modells durch Feature-Selektion (LogReg-Modell VIF-basiert)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_3_1_'></a>[Multikollinearität reduzieren: VIF](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57286189",
   "metadata": {},
   "source": [
    "Zur Verbesserung der Robustheit und Interpretierbarkeit unseres logistischen Regressionsmodells analysieren wir die Prädiktoren hinsichtlich Multikollinearität. Hohe Korrelationen zwischen erklärenden Variablen können zu instabilen oder verzerrten Regressionskoeffizienten führen und die Modellinterpretation erschweren.\n",
    "\n",
    "Zur Identifikation multikollinearer Merkmale verwenden wir den **Variance Inflation Factor (VIF)**.\n",
    "In der Praxis wird häufig ein Schwellenwert von **5 oder 10** verwendet, um problematische Multikollinearität über den **Variance Inflation Factor (VIF)** zu identifizieren. Während ein VIF > 5 bereits auf moderate Korrelationen hinweisen kann, gilt ein VIF > 10 als **robuster Indikator** für starke lineare Abhängigkeiten zwischen Prädiktoren.\n",
    "\n",
    "Für dieses Projekt wurde bewusst ein Schwellenwert von **10** gewählt, und zwar aus folgenden Gründen:\n",
    "\n",
    "- Ein Schwellenwert von 10 reduziert das Risiko, **informative Prädiktoren unnötig auszuschliesssen**, die zwar gewisse Korrelationen aufweisen, aber dennoch einen eigenständigen Beitrag zur Vorhersagekraft leisten.\n",
    "- Die Wahl eines konservativeren Schwellenwerts (z. B. 5) würde die Variablenauswahl **stärker einschränken** und könnte potenziell **zu Informationsverlust** führen – insbesondere bei realweltlichen Daten mit natürlichen Korrelationen (z. B. zwischen Vermögen und Umsatz).\n",
    "\n",
    "Der Wert von 10 stellt somit einen **ausgewogenen Kompromiss** zwischen Stabilität, Interpretierbarkeit und dem Erhalt relevanter Merkmale dar – wie er auch in der Fachliteratur empfohlen wird.\n",
    "\n",
    "\n",
    "Die Analyse erfolgt **iterativ** und umfasst sowohl numerische als auch kategoriale Merkmale:\n",
    "\n",
    "- Kategoriale Variablen werden mittels **One-Hot-Encoding** in binäre Dummy-Variablen umgewandelt. Dabei wird pro Feature-Gruppe eine Referenzkategorie ausgelassen (`drop=\"first\"`), um vollständige Linearabhängigkeiten zu vermeiden.\n",
    "- Anschliessssend berechnen wir für alle resultierenden Spalten den VIF.\n",
    "- Wird der Grenzwert von **10.0** überschritten, entfernen wir das Merkmal mit dem höchsten VIF.\n",
    "- Dieser Prozess wird wiederholt, bis alle verbleibenden Merkmale einen VIF ≤ 10 aufweisen.\n",
    "\n",
    "Das resultierende Merkmalsset ist weitgehend frei von redundanten Informationen und bildet eine stabilere Grundlage für das anschliesssende Modelltraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b30f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsdaten auf kontinuierliche numerische Spalten beschränken\n",
    "#    (Float- und Integer-Spalten, keine kategorischen oder Dummy-Variablen)\n",
    "\n",
    "num_base_cols = (\n",
    "    X_train\n",
    "      .select_dtypes(include=[\"float64\", \"int64\"])\n",
    "      .columns\n",
    ")\n",
    "X_vif = X_train[num_base_cols].copy()\n",
    "\n",
    "# Spalten mit konstanter Varianz (Standardabweichung 0) entfernen\n",
    "X_vif = X_vif.loc[:, X_vif.std() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_vif_matrix(X):\n",
    "    \"\"\"Berechnet den Variance Inflation Factor (VIF) für alle Spalten in X.\"\"\"\n",
    "    vif_data = []\n",
    "    for i in range(X.shape[1]):\n",
    "        try:\n",
    "            vif_val = variance_inflation_factor(X.values, i)\n",
    "        except Exception:\n",
    "            vif_val = np.nan\n",
    "        vif_data.append(vif_val)\n",
    "    return pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"VIF\": vif_data\n",
    "    })\n",
    "\n",
    "def stepwise_vif_reduction(X_df, threshold=10.0, verbose=True, plot_progress=True):\n",
    "    \"\"\"\n",
    "    Iterative Reduktion von Features basierend auf VIF.\n",
    "    Entfernt jeweils die Variable mit dem höchsten VIF über dem Schwellenwert.\n",
    "\n",
    "    Rückgabe:\n",
    "        dict mit initialem und finalem VIF, entfernten Features, Verlauf und Vergleich.\n",
    "    \"\"\"\n",
    "    # 1. Datenkopie anlegen und Liste für entfernte Features initialisieren\n",
    "    X = X_df.copy()\n",
    "    removed = []\n",
    "    vif_history = []\n",
    "\n",
    "    # 2. Nur numerische Spalten behalten und konstante Spalten entfernen\n",
    "    X = X.select_dtypes(include=\"number\")\n",
    "    X = X.loc[:, X.std() > 0]\n",
    "\n",
    "    # 3. VIF vor Reduktion berechnen\n",
    "    initial_vif = calculate_vif_matrix(X)\n",
    "\n",
    "    while True:\n",
    "        vif_df = calculate_vif_matrix(X)\n",
    "        max_vif = vif_df[\"VIF\"].max()\n",
    "        mean_vif = vif_df[\"VIF\"].mean()\n",
    "        num_critical = (vif_df[\"VIF\"] > threshold).sum()\n",
    "\n",
    "        # Verlauf speichern\n",
    "        vif_history.append({\n",
    "            \"iteration\": len(removed),\n",
    "            \"max_vif\": max_vif,\n",
    "            \"mean_vif\": mean_vif,\n",
    "            \"num_critical\": num_critical\n",
    "        })\n",
    "\n",
    "        # Stoppen, wenn alle VIF-Werte unter dem Schwellenwert liegen\n",
    "        if max_vif <= threshold:\n",
    "            break\n",
    "\n",
    "        # Feature mit höchstem VIF entfernen\n",
    "        drop_feature = vif_df.sort_values(\"VIF\", ascending=False).iloc[0][\"feature\"]\n",
    "        removed.append(drop_feature)\n",
    "        if verbose:\n",
    "            print(f\"Entferne '{drop_feature}' mit VIF={max_vif:.2f}\")\n",
    "        X = X.drop(columns=[drop_feature])\n",
    "\n",
    "    # 4. Finalen VIF berechnen und Vergleich mit initialem erstellen\n",
    "    final_vif = calculate_vif_matrix(X)\n",
    "    comparison = pd.merge(\n",
    "        initial_vif, final_vif, on=\"feature\", how=\"inner\", suffixes=(\"_before\", \"_after\")\n",
    "    )\n",
    "    comparison[\"ΔVIF\"] = comparison[\"VIF_after\"] - comparison[\"VIF_before\"]\n",
    "\n",
    "    # 5. Optional: Verlauf der VIF-Werte plotten\n",
    "    if plot_progress:\n",
    "        progress_df = pd.DataFrame(vif_history)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(progress_df[\"iteration\"], progress_df[\"max_vif\"], label=\"Max VIF\")\n",
    "        plt.plot(progress_df[\"iteration\"], progress_df[\"mean_vif\"], label=\"Mean VIF\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"VIF\")\n",
    "        plt.title(\"VIF-Reduktionsverlauf\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"initial_vif\": initial_vif.sort_values(\"VIF\", ascending=False).reset_index(drop=True),\n",
    "        \"final_vif\": final_vif.sort_values(\"VIF\", ascending=False).reset_index(drop=True),\n",
    "        \"removed_features\": removed,\n",
    "        \"comparison\": comparison.sort_values(\"ΔVIF\", ascending=False),\n",
    "        \"vif_progress\": pd.DataFrame(vif_history),\n",
    "        \"n_initial\": initial_vif.shape[0],\n",
    "        \"n_final\": final_vif.shape[0],\n",
    "        \"n_removed\": len(removed)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02206c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stepwise-VIF-Reduktion\n",
    "vif_result = stepwise_vif_reduction(\n",
    "    X_vif,\n",
    "    threshold=10.0,\n",
    "    verbose=True,\n",
    "    plot_progress=True           # Kurve wird angezeigt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ursprüngliche Anzahl Features: {vif_result['n_initial']}\")\n",
    "print(f\"Verbleibende Features:         {vif_result['n_final']}\")\n",
    "print(f\"Entfernte Features:            {vif_result['n_removed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVergleichstabelle:\")\n",
    "vif_result[\"comparison\"].sort_values(\"ΔVIF\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_3_2_'></a>[Training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren der reduzierten Featureliste aus dem VIF-Ergebnis für die Modellbildung\n",
    "num_reduced = vif_result[\"final_vif\"][\"feature\"].tolist()\n",
    "\n",
    "# Kontrolle, ob nur numerische Spalten in den reduzierten Features sind\n",
    "non_num_cols = [col for col in num_reduced if not np.issubdtype(X_train[col].dtype, np.number)]\n",
    "print(\"Nicht-numerische Spalten im VIF-Ergebnis:\", non_num_cols)\n",
    "\n",
    "# Kategorische Spalten im Trainingsdatensatz ermitteln\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns.tolist()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Definition des Preprocessing mit ColumnTransformer\n",
    "#    Numerische Features: StandardScaler (Median-Impute optional ergänzt)\n",
    "#    Kategorische Features: OneHotEncoder mit Behandlung unbekannter Kategorien, Drop-First zur Vermeidung von Dummy-Variablen-Falle\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"sc\", StandardScaler())\n",
    "        ]), num_reduced),\n",
    "\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse_output=False), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Pipeline mit Preprocessing und Logistischer Regression (ausgeglichen durch class_weight)\n",
    "baseline_pipeline_reduced = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=10_000,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"lbfgs\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5b719",
   "metadata": {},
   "source": [
    "Wir überprüfen nun ob auch wirklich alle kategorialen Variablen korrekt codiert wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43de20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation der Trainingsdaten mit dem Preprocessing-Teil der Pipeline\n",
    "X_train_transformed = baseline_pipeline_reduced.named_steps['prep'].fit_transform(X_train)\n",
    "\n",
    "# Numerische Feature-Namen aus der reduzierten Featureliste\n",
    "num_features = num_reduced\n",
    "\n",
    "# One-Hot-kodierte kategorische Feature-Namen aus dem ColumnTransformer extrahieren\n",
    "cat_features = baseline_pipeline_reduced.named_steps['prep']\\\n",
    "    .named_transformers_['cat'].get_feature_names_out(cat_cols)\n",
    "\n",
    "# Alle Feature-Namen zusammenfügen (numerisch + kategorisch)\n",
    "all_features = list(num_features) + list(cat_features)\n",
    "\n",
    "# DataFrame mit den transformierten Trainingsdaten und passenden Spaltennamen erstellen\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=all_features)\n",
    "\n",
    "# Anzeigeoption einstellen, um alle Spalten im DataFrame sichtbar zu machen\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# DataFrame mit den ersten fünf Zeilen anzeigen\n",
    "display(X_train_transformed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ab571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "\n",
    "cv_result_reduced = cross_validate(\n",
    "    baseline_pipeline_reduced,\n",
    "    X_train,                  # <-- jetzt gesamtes X_train (numerisch + kategorial)\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"=== Ø-Scores (5-CV) ===\")\n",
    "for m in scoring:\n",
    "    print(f\"{m:<9s}: {cv_result_reduced[f'test_{m}'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_3_3_'></a>[Evaluation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163d57b",
   "metadata": {},
   "source": [
    "Vergleich Baseline vs. Reduziertes Modell (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9003764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der zu vergleichenden Metriken\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "# Sampling-Modelle in gewünschter Reihenfolge (optional)\n",
    "model_names = [\"Baseline (class_weight)\", \"SMOTE\", \"Undersampling\"]\n",
    "\n",
    "# Vergleichstabelle erstellen:\n",
    "# - Spalte 1: Mittelwerte aus Cross-Validation für das ursprüngliche Baseline-Modell\n",
    "# - Spalte 2: Mittelwerte für das reduzierte Modell (nach VIF-Selektion)\n",
    "# - Spalte 3: Differenz der Mittelwerte (Reduced − Baseline), um Verbesserungen oder Einbuss$sen zu erkennen\n",
    "\n",
    "# Vergleichstabelle erstellen\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Baseline (class_weight)': [cv_results[\"Baseline (class_weight)\"][m] for m in metrics],\n",
    "    'VIF-reduziert (mean)':    [cv_result_reduced[f'test_{m}'].mean() for m in metrics],\n",
    "    'Δ (VIF − Baseline)': [\n",
    "        cv_result_reduced[f'test_{m}'].mean() - cv_results[\"Baseline (class_weight)\"][m]\n",
    "        for m in metrics\n",
    "    ]\n",
    "}, index=metrics).round(3)\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64ae3d",
   "metadata": {},
   "source": [
    "Das **Baseline-Modell** verwendet nur **5 ausgewählte Features**   und zeigt eine **vergleichbare oder sogar leicht bessere Performance** als das komplexere VIF-Modell mit 17 verbleibenden Features:\n",
    "\n",
    "- **Accuracy** und **ROC-AUC** ist fast gleichwertig (−0.01).\n",
    "- **Recall** , **Precision** und **F1-Score**  sind beim Baseline-Modell minimal besser.\n",
    "\n",
    "Die starke Feature-Reduktion auf nur 5 Merkmale im Baseline-Modell führt zu **keinem signifikanten Leistungsverlust**.  \n",
    "Stattdessen bietet es ein **einfacheres, leichter erklärbares und stabileres Modell** – ideal für Kommunikation mit Fachabteilungen oder produktiven Einsatz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_4_'></a>[Vorbereitung Kandidatenmodelle für den Modellvergleich](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7254e",
   "metadata": {},
   "source": [
    "Nach der Optimierung des Baseline-Modells auf Basis der logistischen Regression werden in diesem Abschnitt leistungsfähigere Klassifikationsverfahren vorbereitet, um deren Eignung für die Vorhersage von Kreditkartenkäufen zu evaluieren.\n",
    "\n",
    "Im Fokus stehen baumbasierte Modelle, die sich durch eine höhere Modellkomplexität und Robustheit gegenüber Multikollinearität auszeichnen. Die folgenden Kandidatenmodelle werden auf dem vollständigen, nicht VIF-reduzierten Feature-Set definiert:\n",
    "\n",
    "- **Random Forest** (mit `class_weight='balanced'`)\n",
    "- **Balanced Random Forest** (mit internem Resampling pro Baum )\n",
    "- **XGBoost** (mit `scale_pos_weight`)\n",
    "- **HistGradientBoosting**\n",
    "\n",
    "Alle Modelle werden in einheitliche Pipelines integriert, die ein gemeinsames Preprocessing (StandardScaler für numerische, OneHotEncoder für kategoriale Variablen) enthalten. Damit sind die Kandidatenmodelle konsistent vorbereitet und bereit für den strukturierten Vergleich im nächsten Kapitel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303087cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) Spalten nach Datentypen trennen: numerisch vs. kategorisch\n",
    "num_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]\n",
    "cat_cols = [col for col in X_train.columns if X_train[col].dtype in ['object', 'string', 'category']]\n",
    "\n",
    "# Kategorische Spalten als String casten (für Pipeline-Kompatibilität)\n",
    "X_train[cat_cols] = X_train[cat_cols].astype(str)\n",
    "X_test[cat_cols] = X_test[cat_cols].astype(str)\n",
    "\n",
    "# 2) Preprocessing Pipeline definieren\n",
    "# Numerische Spalten: fehlende Werte mit Mittelwert auffüllen, dann skalieren\n",
    "# Kategorische Spalten: fehlende Werte mit \"missing\" auffüllen, dann OneHot-Encoding\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ]), num_cols),\n",
    "    \n",
    "    (\"cat\", Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"encode\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# Positives Klassenverhältnis für XGBoost (Wichtung der Minderheitsklasse)\n",
    "pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "# Klassifikatoren definieren mit spezifischen Parametern\n",
    "rf = RandomForestClassifier(n_estimators=400, class_weight='balanced', random_state=42)\n",
    "\n",
    "brf = BalancedRandomForestClassifier(n_estimators=400, random_state=42)\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    scale_pos_weight=pos_weight,\n",
    "    n_estimators=400,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Funktion zum Umwandeln von sparse Matrix in dichten Array (für XGBoost und HGB)\n",
    "to_dense = FunctionTransformer(lambda X: X.toarray(), accept_sparse=True)\n",
    "\n",
    "# Pipelines für die verschiedenen Modelle zusammenstellen\n",
    "pipelines = {\n",
    "    \"Random Forest\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"clf\", rf)\n",
    "    ]),\n",
    "    \"Balanced RF\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"clf\", brf)\n",
    "    ]),\n",
    "    \"XGBoost\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"dense\", to_dense),   # Konvertierung nötig für XGBoost\n",
    "        (\"clf\", xgb_clf)\n",
    "    ]),\n",
    "    \"HistGradBoost\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"dense\", to_dense),   # Konvertierung nötig für HistGradientBoosting\n",
    "        (\"clf\", hgb)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Beispiel: XGBoost-Pipeline auswählen und trainieren (fit)\n",
    "pipe = pipelines[\"XGBoost\"]\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Zugriff auf den ColumnTransformer und OneHotEncoder für Kategorische Features\n",
    "column_transformer = pipe.named_steps[\"prep\"]\n",
    "ohe = column_transformer.named_transformers_[\"cat\"].named_steps[\"encode\"]\n",
    "\n",
    "# Ausgabe der Kategorien pro kategorischer Spalte\n",
    "print(\"\\nKategorische Spalten und erkannte Kategorien:\")\n",
    "for col, cats in zip(cat_cols, ohe.categories_):\n",
    "    print(f\"{col}: {list(cats)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61023806",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Modellvergleich, -selektion und -optimierung](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[Vergleich Modellperformance](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e6d07",
   "metadata": {},
   "source": [
    "Im Anschluss an die Entwicklung der Kandidatenmodelle folgt nun deren Vergleich im Hinblick auf die Klassifikationsqualität. Ziel ist es, das Modell mit der besten Performance zu identifizieren, das anschliessend für die finale Evaluation auf dem Testdatensatz sowie für den Einsatz in der geschäftlichen Anwendung verwendet werden soll.\n",
    "\n",
    "Die Bewertung erfolgt mittels **5-facher stratifizierter Cross-Validation** auf den Trainingsdaten. Aufgrund der **unausgeglichenen Zielklassen** (nur ein kleiner Teil der Kunden kauft eine Kreditkarte), kommen dabei insbesondere zwei Metriken zum Einsatz:\n",
    "\n",
    "- **ROC AUC**: Diese misst die Trennschärfe des Modells über alle möglichen Schwellenwerte hinweg. Sie zeigt, wie gut das Modell Käufer und Nicht-Käufer voneinander unterscheidet.\n",
    "- **F1-Score**: Dieser harmonisiert Precision und Recall und ist besonders hilfreich, wenn sowohl False Positives als auch False Negatives relevant sind – wie im Cross-Selling-Kontext, wo falsch positive Vorhersagen zu ineffizienten Marketingausgaben führen können.\n",
    "\n",
    "Die Verwendung dieser beiden Metriken ermöglicht eine robuste und pragmatische Einschätzung der Modellleistung, auch ohne vollständige Betrachtung von Precision-Recall- oder Lift-Kurven.\n",
    "\n",
    "Alle Modelle wurden mit identischem Preprocessing trainiert, um einen fairen Vergleich zu gewährleisten. Die Ergebnisse der Cross-Validation sind in der folgenden Tabelle zusammengefasst und bilden die Entscheidungsgrundlage für die Auswahl des besten Modells (**Roadmap-Schritt 13**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60166e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation mit stratified 5-Fold für verschiedene Modelle durchführen\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    # Cross-Validate für jedes Modell mit mehreren Metriken parallel berechnen\n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    # Mittelwerte der Testergebnisse sammeln\n",
    "    results[name] = {m: scores[f'test_{m}'].mean() for m in scoring}\n",
    "\n",
    "# Ergebnisse in DataFrame umwandeln und runden\n",
    "results_df = pd.DataFrame(results).T.round(3)\n",
    "\n",
    "# Ergebnis des VIF-reduzierten LogReg-Modells als neue Zeile hinzufügen\n",
    "reduced_entry = {\n",
    "    m: cv_result_reduced[f'test_{m}'].mean()\n",
    "    for m in scoring\n",
    "}\n",
    "results_df.loc[\"LogReg (VIF reduced)\"] = reduced_entry\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "results_df = results_df.round(3)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd079e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Plots initialisieren\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# ROC-Kurve plot erstellen\n",
    "plt.subplot(1, 3, 1)\n",
    "for name, pipe in pipelines.items():\n",
    "    y_proba = cross_val_predict(pipe, X_train, y_train, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_train, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={roc_auc:.2f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data vorbereiten: melt() erzeugt ein long-format DataFrame\n",
    "df_melted = results_df.reset_index().melt(id_vars='index')\n",
    "df_melted.columns = ['Modell', 'Metrik', 'Wert']\n",
    "\n",
    "# Plot mit Plotly\n",
    "fig = px.bar(\n",
    "    df_melted,\n",
    "    x=\"Metrik\",\n",
    "    y=\"Wert\",\n",
    "    color=\"Modell\",\n",
    "    barmode=\"group\",\n",
    "    text_auto='.2f',\n",
    "    title=\"Modellvergleich\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    xaxis_title=\"Metrik\",\n",
    "    yaxis_title=\"Score\",\n",
    "    legend_title=\"Modell\",\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_1_1_'></a>[Auswahl des besten Modells](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4afc5",
   "metadata": {},
   "source": [
    "Die ROC AUC-Werte liegen bei den Ensemble-Modellen (Random Forest, Balanced RF, XGBoost, HistGradientBoost) auf einem **vergleichbar hohen Niveau (≈ 0.89–0.90)**. Diese Metrik allein erlaubt daher **keine eindeutige Differenzierung** zwischen den Modellen.\n",
    "\n",
    "Zur genaueren Beurteilung wird deshalb der **F1-Score** als sekundäres Kriterium herangezogen. Der F1-Score vereint **Precision** und **Recall** zu einem harmonisierten Mass, das besonders bei unausgeglichenen Klassenverhältnissen aussagekräftig ist – wie es im Kontext der Kreditkarten-Cross-Selling-Kampagne der Fall ist.\n",
    "\n",
    "Das Modell **Balanced Random Forest** erreicht mit **F1 = 0.62** den höchsten Wert unter allen Modellen, bei gleichzeitig hohem Recall und akzeptabler Precision. Obwohl andere Modelle ähnliche ROC-Werte erzielen, zeigt Balanced RF **die insgesamt ausgewogenste Performance** im Hinblick auf relevante Zielmetriken.\n",
    "\n",
    "**Fazit:**  \n",
    "> Aufgrund der Kombination aus hoher Trennschärfe (ROC AUC = 0.90) und bestmöglicher Balance zwischen Precision und Recall (F1 = 0.62) wird **Balanced Random Forest** als das leistungsstärkste Modell ausgewählt. Es bildet die Grundlage für die finale Evaluation auf dem Testdatensatz und die spätere geschäftliche Anwendung.\n",
    "\n",
    "**Hinweis zur Lift-Kurve**\n",
    "\n",
    "Die Lift-Kurve und Top-N-Analyse (z. B. Top 5 %) sind zentrale Instrumente zur Bewertung der praktischen Marketingwirkung. Da der Fokus hier auf der generellen Modellgüte liegt (ROC AUC, F1), wird die Lift-Analyse bewusst zu einem späteren Zeitpunkt (Roadmap-Schritt 17) nachgeholt, um dann gezielt die Kundenpriorisierung und Zielgruppenkonsistenz zu untersuchen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_2_'></a>[Vergleich Top-N-Kundenlisten](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellnamen definieren (Baseline + Kandidaten)\n",
    "model_names = [\"LogReg (VIF reduced)\", \"Random Forest\", \"Balanced RF\", \"XGBoost\"]\n",
    "\n",
    "# Dictionary zur Speicherung der trainierten Modelle\n",
    "trained_models = {}\n",
    "\n",
    "# Alle Modelle aus dem Pipeline-Dictionary auf Trainingsdaten fitten\n",
    "for name, pipe in pipelines.items():\n",
    "    print(f\"Fitting Modell: {name}\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "    trained_models[name] = pipe\n",
    "\n",
    "# VIF-reduziertes LogReg-Modell separat fitten\n",
    "print(\"Fitting LogReg (VIF reduced)\")\n",
    "baseline_pipeline_reduced.fit(X_train[num_reduced + cat_cols], y_train)\n",
    "trained_models[\"LogReg (VIF reduced)\"] = baseline_pipeline_reduced\n",
    "\n",
    "# Analyse der Überlappung der Top-N Kunden (z.B. Top 5% und 10%)\n",
    "top_percentages = [0.05, 0.10]  # 5% und 10%\n",
    "overlap_results = []\n",
    "\n",
    "for p in top_percentages:\n",
    "    n_top = int(len(X_test) * p)\n",
    "    print(f\"\\nTop {int(p*100)}% Kunden: jeweils {n_top} Kunden\")\n",
    "\n",
    "    top_customers = {}\n",
    "    for name, model in trained_models.items():\n",
    "        # Für LogReg reduzierte Features verwenden, sonst alle Features\n",
    "        if name == \"LogReg (VIF reduced)\":\n",
    "            proba = model.predict_proba(X_test[num_reduced + cat_cols])[:, 1]\n",
    "        else:\n",
    "            proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        top_idx = np.argsort(proba)[-n_top:]\n",
    "        top_customers[name] = set(X_test.iloc[top_idx].index)\n",
    "\n",
    "    # Jaccard-Index und Schnittmenge für alle Modellpaare berechnen\n",
    "    for i, name1 in enumerate(model_names):\n",
    "        for j, name2 in enumerate(model_names):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            set1 = top_customers[name1]\n",
    "            set2 = top_customers[name2]\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            overlap_results.append({\n",
    "                \"Top (%)\": int(p*100),\n",
    "                \"Modell 1\": name1,\n",
    "                \"Modell 2\": name2,\n",
    "                \"Intersection\": intersection,\n",
    "                \"Jaccard\": jaccard\n",
    "            })\n",
    "\n",
    "# DataFrame mit den Überlappungs-Metriken erstellen\n",
    "overlap_df = pd.DataFrame(overlap_results)\n",
    "\n",
    "# Funktion zur Hervorhebung des Jaccard-Werts mit Farben\n",
    "def highlight_jaccard(val):\n",
    "    if val >= 0.8:\n",
    "        return 'background-color: #2ecc71; color: white;'  # Grün: sehr hoch\n",
    "    elif val >= 0.6:\n",
    "        return 'background-color: #27ae60; color: white;'  # Dunkelgrün\n",
    "    elif val >= 0.4:\n",
    "        return 'background-color: #f1c40f; color: black;'  # Gelb: mittel\n",
    "    elif val >= 0.2:\n",
    "        return 'background-color: #e67e22; color: black;'  # Orange: niedrig\n",
    "    else:\n",
    "        return 'background-color: #e74c3c; color: white;'  # Rot: sehr niedrig\n",
    "\n",
    "# Tabelle mit absoluten Top-N Kunden pro Prozentwert erstellen\n",
    "n_total = len(X_test)\n",
    "cutoff_info = pd.DataFrame({\n",
    "    \"Top (%)\": [int(p*100) for p in top_percentages],\n",
    "    \"Top N\"  : [int(n_total*p) for p in top_percentages]\n",
    "})\n",
    "print(cutoff_info)\n",
    "\n",
    "# Gestylte Tabelle mit farblicher Hervorhebung der Jaccard-Index-Werte anzeigen\n",
    "styled_df = overlap_df.style.format({\"Jaccard\": \"{:.2f}\"}).applymap(highlight_jaccard, subset=['Jaccard'])\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9334f8",
   "metadata": {},
   "source": [
    "Die Tabelle zeigt die paarweise Überschneidung (Jaccard-Index) der Top-Kunden-Listen der Modelle für die obersten 5% und 10% der Kunden.\n",
    "Der Jaccard-Index misst, wie ähnlich sich zwei Mengen sind, und wird als Anteil der gemeinsamen Elemente an der Gesamtheit aller Elemente beider Mengen berechnet. Werte nahe 1 bedeuten grosse Übereinstimmung, Werte nahe 0 wenig bis keine.\n",
    "\n",
    "- **Hohe Überschneidung** (> 0.68) besteht vor allem zwischen den Ensemble-Modellen `Random Forest` und `Balanced RF`, was auf eine sehr ähnliche Auswahl an Top-Kunden hindeutet.\n",
    "- Die **logistische Regression (VIF reduziert)** unterscheidet sich stark von den anderen Modellen, mit deutlich geringeren Überschneidungen (Jaccard meist < 0.3). Dies zeigt, dass sie andere Kunden priorisiert.\n",
    "- Für **grössere Top-N Mengen (10%)** nimmt die Überschneidung zwischen den Modellen tendenziell zu, da mehr Kunden in allen Listen auftauchen.\n",
    "- Insgesamt verdeutlicht die Analyse, dass Ensemble-Modelle in der Kundenauswahl konsistenter sind als das Baseline-LogReg-Modell.\n",
    "- Diese Erkenntnisse unterstützen eine fundierte Modellwahl unter Berücksichtigung von Kundenkonsistenz bei der gezielten Marketingansprache.\n",
    "\n",
    "**Fazit:** Modelle mit hoher Jaccard-Übereinstimmung sind besonders geeignet für konsistente, wiederholbare Kampagnen. Modelle mit geringer Übereinstimmung (wie LogReg) können hingegen alternative oder ergänzende Zielgruppen identifizieren und zur Diversifikation der Ansprache beitragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_3_'></a>[Hyperparameter-Tuning und Test-Set-Evaluation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28ef2d",
   "metadata": {},
   "source": [
    "\n",
    "Im letzten Schritt wurde das leistungsstärkste Modell aus dem Vergleich  – der **Balanced Random Forest** – mithilfe einer Hyperparameteroptimierung weiter verbessert. Hierzu wurde `RandomizedSearchCV` auf dem Trainingsset durchgeführt, um optimale Einstellungen für Tiefe, Baumanzahl, Blätteranzahl und Splittingstrategien zu identifizieren. `RandomizedSearchCV` wurde verwendet, um eine Stichprobe von 100 Hyperparameter-Kombinationen aus dem definierten Parameterraum zufällig zu evaluieren. Dies ermöglicht eine effizientere Suche bei gleichzeitig hoher Vielfalt, ohne alle theoretisch möglichen Kombinationen durchzuspielen. Bei 5-facher Cross-Validation ergibt dies insgesamt 500 Fits (100 Kombinationen × 5 Folds).\n",
    "\n",
    "Anschliessend wurde das **optimierte Modell** auf dem **Testdatensatz evaluiert**, um seine Generalisierungsfähigkeit zu überprüfen. Die finale Bewertung umfasst Standardmetriken wie Accuracy, Precision, Recall, F1-Score sowie die ROC-AUC. Zusätzlich wurden die ROC-Kurve und die normalisierte Confusion-Matrix dargestellt.\n",
    "\n",
    "Um das Entscheidungsverhalten des Modells praxisnah weiter zu verbessern, wurde zusätzlich ein **Threshold-Tuning** durchgeführt. Dabei wurde die Entscheidungsschwelle systematisch variiert (z. B. von 0.1 bis 0.9), um zu analysieren, wie sich Precision, Recall und F1-Score abhängig vom gewählten Schwellenwert verändern. Dies erlaubt eine datenbasierte Feinjustierung des Klassifikationsverhaltens in Abhängigkeit vom Businessziel – etwa, ob mehr Wert auf eine hohe Erkennungsrate (Recall) oder eine präzisere Ansprache (Precision) gelegt werden soll.\n",
    "\n",
    "Die Ergebnisse dieser Evaluation bilden die Grundlage für die Schlussfolgerung hinsichtlich der Modellgüte und der praktischen Einsetzbarkeit im Rahmen einer gezielten Kundenansprache im Cross-Selling-Kontext.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fa54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Mehrere Metriken für RandomizedSearchCV definieren\n",
    "scoring = {\n",
    "    \"f1\": \"f1\",            # Optimiert nach F1-Score\n",
    "    \"roc_auc\": \"roc_auc\",  # Auch ROC-AUC als wichtige Metrik\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\"\n",
    "}\n",
    "\n",
    "# 1) Parameterraum für BalancedRandomForestClassifier\n",
    "param_dist = {\n",
    "    \"clf__n_estimators\"     : [25, 50, 100, 200, 300, 400, 500],\n",
    "    \"clf__max_depth\"        : [None, 5, 10, 15, 20],\n",
    "    \"clf__min_samples_split\": randint(2, 11),\n",
    "    \"clf__min_samples_leaf\" : randint(1, 11),\n",
    "    \"clf__max_features\"     : [\"sqrt\", \"log2\", None, 0.3, 0.5, 0.7],\n",
    "    \"clf__bootstrap\"        : [True, False],\n",
    "    \"clf__criterion\"        : [\"gini\", \"entropy\"],\n",
    "    \"clf__sampling_strategy\": [\"auto\", 0.5, 0.75],\n",
    "    \"clf__replacement\"      : [True, False],\n",
    "}\n",
    "\n",
    "# 2) Pipeline mit Preprocessing + BalancedRandomForestClassifier\n",
    "brf_pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", BalancedRandomForestClassifier(random_state=42, n_jobs=1))\n",
    "])\n",
    "\n",
    "# 3) RandomizedSearchCV konfigurieren und fitten\n",
    "brf_search = RandomizedSearchCV(\n",
    "    estimator=brf_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,               # Anzahl zufälliger Parameterkombinationen\n",
    "    scoring=scoring,          # Mehrere Metriken zur Evaluierung\n",
    "    refit=\"roc_auc\",              # Nach F1 Score wird das beste Modell ausgewählt\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "brf_search.fit(X_train, y_train)\n",
    "\n",
    "# 4) Bestes Modell aus RandomizedSearch extrahieren\n",
    "best_brf = brf_search.best_estimator_\n",
    "\n",
    "# 5) Evaluation auf Trainingsdaten\n",
    "y_train_pred = best_brf.predict(X_train)\n",
    "y_train_proba = best_brf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "print(\"\\n=== Klassifikationsbericht (Trainings-Set) ===\")\n",
    "print(classification_report(y_train, y_train_pred, digits=3))\n",
    "print(f\"ROC-AUC (Trainings-Set): {roc_auc_score(y_train, y_train_proba):.3f}\")\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred, normalize=\"true\")\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm_train, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Confusion Matrix (Trainings-Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6) Evaluation auf Testdaten\n",
    "y_test_pred = best_brf.predict(X_test)\n",
    "y_test_proba = best_brf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== Klassifikationsbericht (Test-Set) ===\")\n",
    "print(classification_report(y_test, y_test_pred, digits=3))\n",
    "print(f\"ROC-AUC (Test-Set): {roc_auc_score(y_test, y_test_proba):.3f}\")\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm_test, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Confusion Matrix (Test-Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7) Threshold-Tuning: Precision, Recall und F1-Score für verschiedene Entscheidungsschwellen\n",
    "\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 17)  # Schwellen von 0.1 bis 0.9 in 0.05-Schritten\n",
    "metrics = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_thresh = (y_test_proba >= t).astype(int)\n",
    "    precision = precision_score(y_test, y_thresh)\n",
    "    recall = recall_score(y_test, y_thresh)\n",
    "    f1 = f1_score(y_test, y_thresh)\n",
    "    metrics.append((t, precision, recall, f1))\n",
    "\n",
    "df_thresh = pd.DataFrame(metrics, columns=[\"threshold\", \"precision\", \"recall\", \"f1\"])\n",
    "display(df_thresh)\n",
    "\n",
    "df_thresh.set_index(\"threshold\").plot(figsize=(8, 5), marker=\"o\")\n",
    "plt.title(\"Precision, Recall und F1-Score in Abhängigkeit vom Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Entscheidungsschwelle\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8) Ausgabe der besten Parameterkombination\n",
    "print(\"\\nBeste Parameterkombination:\")\n",
    "print(brf_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der ROC-Kurve und Schwellenwerte\n",
    "y_proba = best_brf.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC={roc_auc:.2f})', color='blue', linewidth=2)\n",
    " \n",
    "# Anzahl der Schwellenwerte, die du anzeigen möchtest\n",
    "num_thresholds = 7\n",
    "# Indizes gleichmässig auswählen\n",
    "indices = np.linspace(0, len(thresholds) - 1, num=num_thresholds, dtype=int)\n",
    " \n",
    "for i in indices:\n",
    "    # Punkt markieren\n",
    "    plt.scatter(fpr[i], tpr[i], color='red', s=70, zorder=5)\n",
    "    # Text etwas oberhalb und rechts vom Punkt\n",
    "    plt.text(fpr[i] + 0.02, tpr[i] - 0.03, f'{thresholds[i]:.2f}',\n",
    "             fontsize=12, color='red', fontweight='bold', zorder=6)\n",
    " \n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title(\"ROC-Kurve (Test-Set) mit Schwellenwerten\", fontsize=16)\n",
    " \n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_3_1_'></a>[Evaluation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697ea14",
   "metadata": {},
   "source": [
    "**Trainingsdaten**\n",
    "\n",
    "Das Modell zeigt auf dem Trainingsdatensatz eine sehr hohe Gesamtgüte:\n",
    "\n",
    "- Accuracy liegt bei 82.3 %, die ROC-AUC beträgt perfekte 1.000.\n",
    "\n",
    "- Für Klasse 0 (Nicht-Käufer:innen) wird eine Precision von 1.00 erreicht – das Modell macht keine Fehler bei der Vorhersage dieser Klasse. Allerdings liegt der Recall nur bei 0.78, was bedeutet, dass etwa 22 % der tatsächlichen Nicht-Käufer:innen fälschlicherweise als Käufer:innen klassifiziert werden.\n",
    "\n",
    "- Für Klasse 1 (Käufer:innen) ergibt sich ein umgekehrtes Bild: Der Recall liegt bei 1.00, es werden also alle Käufer:innen korrekt erkannt. Die Precision liegt jedoch bei 0.51, d. h. etwa jede zweite positive Vorhersage ist korrekt.\n",
    "\n",
    "Diese idealisierte Leistung ist auf das interne Balancing des Balanced Random Forest zurückzuführen, das das Verhältnis der Klassen künstlich ausgleicht. Dadurch entsteht ein vereinfachtes Lernproblem, das das Modell sehr gut lösen kann allerdings mit der Gefahr des Overfittings, was sich in der perfekten AUC zeigt.\n",
    "\n",
    "**Testdaten**\n",
    "\n",
    "Die Ergebnisse auf dem Test-Set fallen realistischer aus und zeigen, dass das Modell trotz Overfitting-Tendenz im Training eine gute Generalisierungsfähigkeit besitzt:\n",
    "\n",
    "- Die Accuracy auf dem Test-Set liegt bei 77 %, die ROC-AUC beträgt 0.886 was ein sehr guter Wert zur Trennung der beiden Klassen ist.\n",
    "\n",
    "- Der Recall ist weiterhin hoch für beide Klassen: 0.735 für Klasse 0 und 0.928 für Klasse 1. Das bedeutet, dass das Modell nur rund 7 % der tatsächlichen Käufer:innen verpasst.\n",
    "\n",
    "- Die Precision liegt bei 0.978 für Klasse 0 und 0.438 für Klasse 1. Gerade bei Klasse 1 bedeutet dies, dass etwa 44 % der als Käufer:innen vorhergesagten Personen tatsächlich auch kaufen.\n",
    "\n",
    "- Der F1-Score für Klasse 1 liegt bei 0.595, was angesichts des stark unausgeglichenen Klassenverhältnisses als solide zu bewerten ist.\n",
    "\n",
    "Die Ergebnisse zeigen, dass das Modell besonders gut darin ist, potenzielle Käufer:innen zu erkennen, auch wenn dies zu gewissen Fehlklassifikationen führt (False Positives). Diese sind in einem Cross-Selling-Kontext oft weniger problematisch als False Negatives.\n",
    "\n",
    "**Threshold-Tuning**\n",
    "\n",
    "- Der optimale F1-Score liegt bei 0.50 – hier erreichen wir den besten Kompromiss zwischen Precision und Recall mit einem F1-Score von 0.60.\n",
    "\n",
    "- Standardmässig wird der Threshold auf 0.50 gesetzt, was auch in vielen Klassifikationsmodellen der Fall ist.\n",
    "\n",
    "- Für das Cross-Selling-Szenario (bei dem das Risiko, Käufer zu verpassen, höher ist als das Risiko von zusätzlichen False Positives) bleibt der Threshold 0.50 optimal.\n",
    "\n",
    "Business-Impact:\n",
    "\n",
    "- Recall 0.93 bei Threshold 0.50: Etwa 93 % der tatsächlichen Käufer werden erreicht.\n",
    "\n",
    "- Precision 0.44: Es wird ungefähr jede zweite angesprochene Person ein Käufer sein (False Positives beeinflussen das Kontakt-Budget).\n",
    "\n",
    "Empfehlung:\n",
    "\n",
    "- Der Threshold sollte auf 0.50 gesetzt werden, um den besten F1-Score zu erzielen.\n",
    "\n",
    "- Überwachen der Precision und des Recall im Realbetrieb, um sicherzustellen, dass die richtige Balance zwischen beiden gefunden wird.\n",
    "\n",
    "- Feintuning im Bereich 0.50-0.55 kann in Betracht gezogen werden, wenn der Fokus stärker auf der Minimierung von Kontaktkosten liegt.\n",
    "\n",
    "**Fazit**\n",
    "\n",
    "Das finale Modell überzeugt durch hohe Erkennungsrate, gute Trennleistung und eine ausgewogene Performance bei realistischen Entscheidungsschwellen. Damit ist es praxisnah einsetzbar zur gezielten Identifikation potenzieller Kreditkartenkund:innen im Rahmen von Cross-Selling-Massnahmen.\n",
    "\n",
    "Die Kombination aus:\n",
    "\n",
    "- robuster Modellwahl (Balanced Random Forest)\n",
    "- Hohe Trennschärfe (ROC-AUC = 0.886)\n",
    "- gezieltem Hyperparameter-Tuning\n",
    "- optionalem Schwellen-Tuning\n",
    "\n",
    "liefert eine verlässliche Lösung für das Ungleichgewichtsproblem und eine gute Entscheidungsbasis für das operative Marketing.\n",
    "\n",
    "Die finalen Modellparameter lauten:\n",
    "\n",
    "```\n",
    "{\n",
    "    'clf__bootstrap':        False,\n",
    "    'clf__criterion':        'entropy',\n",
    "    'clf__max_depth':        None,\n",
    "    'clf__max_features':     0.7,\n",
    "    'clf__min_samples_leaf': 3,\n",
    "    'clf__min_samples_split': 2,\n",
    "    'clf__n_estimators':     200,\n",
    "    'clf__replacement':      False,\n",
    "    'clf__sampling_strategy':'auto'\n",
    "}\n",
    "\n",
    "````\n",
    "Diese Konfiguration setzt auf eine grosse Anzahl von Entscheidungsbäumen (200), keine Wiederholung von Stichproben (replacement = False), eine moderate Begrenzung der Blatttiefe durch min_samples_leaf, sowie die Verwendung der Entropie als Kriterium für die Entscheidungsregeln. Das gewählte Setup zeigt gute Trennleistung bei gleichzeitig kontrollierter Komplexität."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f5228",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Modellerklärung und -reduktion](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_1_'></a>[Globale Prädiktorwichtigkeit mittels PFI und PDP](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059990ba",
   "metadata": {},
   "source": [
    "Zur Bewertung der globalen Bedeutung der Merkmale wurde die Permutation Feature Importance (PFI) eingesetzt. Dabei wird für jedes Merkmal geprüft, wie stark die Modellleistung (gemessen mit ROC-AUC) sinkt, wenn die Werte dieses Merkmals zufällig permutiert werden. Ein grösserer Leistungsverlust zeigt eine höhere Relevanz des Merkmals für das Modell an.\n",
    "\n",
    "Da PFI bei stark korrelierten Merkmalen die Wichtigkeit unterschätzen kann (weil ähnliche Merkmale sich gegenseitig kompensieren), wurde der Fokus auf die Top-Features mit der höchsten PFI gelegt. Zur tieferen Interpretation der Wirkung dieser Merkmale wurden zusätzlich Partial Dependence Plots (PDP) erzeugt, um den Einfluss einzelner Variablen auf die Modellvorhersage besser zu verstehen.\n",
    "\n",
    "Dieses Vorgehen ermöglicht eine nachvollziehbare und gut interpretierbare Darstellung der wichtigsten Einflussgrössen im Modell, die auch für nicht-technische Stakeholder verständlich ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bestes Modell aus RandomizedSearchCV auswählen\n",
    "best_model = brf_search.best_estimator_\n",
    "\n",
    "# Permutation Feature Importance (PFI) berechnen:\n",
    "# Messen, wie stark die Modellleistung (ROC-AUC) bei zufälligem Permutieren eines Features sinkt\n",
    "result = permutation_importance(\n",
    "    best_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring='roc_auc',\n",
    "    n_repeats=10,         # Anzahl Wiederholungen zur Stabilität\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Feature-Namen aus Trainingsdaten extrahieren\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Indizes der Features nach absteigender mittlerer Wichtigkeit sortieren\n",
    "sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "\n",
    "top_n = 10\n",
    "top_idx = sorted_idx[:top_n]\n",
    "\n",
    "# Feature-Namen bereinigen für bessere Lesbarkeit (Unterstriche durch Leerzeichen ersetzen, erste Buchstaben gross)\n",
    "feature_names_clean = [name.replace('_', ' ').title() for name in feature_names]\n",
    "\n",
    "# Horizontaler Balkendiagrammplot der Top-N Features mit mittlerer Wichtigkeit und Standardabweichung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    np.array(feature_names_clean)[top_idx],\n",
    "    result.importances_mean[top_idx],\n",
    "    xerr=result.importances_std[top_idx],\n",
    "    color='salmon',\n",
    "    align='center'\n",
    ")\n",
    "plt.xlabel(\"Mean decrease in ROC-AUC after permutation\")\n",
    "plt.title(f\"Permutation Feature Importance (Top {top_n} Features)\")\n",
    "plt.gca().invert_yaxis()  # Wichtigste Features oben anzeigen\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0dbab3",
   "metadata": {},
   "source": [
    "Die Visualisierung zeigt die zehn wichtigsten Merkmale gemessen anhand der Permutation Feature Importance, wobei die mittlere Abnahme der ROC-AUC nach Permutation der einzelnen Merkmalswerte dargestellt ist. Je stärker die Abnahme, desto stärker ist das Modell von diesem Merkmal abhängig.\n",
    "\n",
    "- Avg Balance ist mit grossem Abstand das wichtigste Merkmal. Die Permutation dieses Merkmals führt zu einem signifikanten Rückgang der ROC-AUC, was darauf hindeutet, dass die durchschnittliche finanzielle Situation der Kund:innen ein zentraler Prädiktor für die Kreditkartenaffinität ist.\n",
    "\n",
    "- Balance Before Cc (Kontostand vor Kreditkartenvergabe) ist das zweitwichtigste Merkmal, wenn auch mit deutlich geringerem Effekt. Es legt nahe, dass nicht nur die langfristige finanzielle Stabilität, sondern auch der konkrete Stand vor der Kreditkartenentscheidung relevant ist.\n",
    "\n",
    "- Merkmale wie Avg Trans Amount und Max Balance haben nur noch einen sehr geringen Einfluss, zeigen aber dennoch, dass das Transaktionsverhalten eine gewisse Zusatzinformation liefert.\n",
    "\n",
    "- Klassische demografische Merkmale wie Age (Alter) sowie aggregierte Kennzahlen wie Total Spent oder Std Trans Amount spielen nur eine untergeordnete Rolle. Ihr Einfluss auf die Modellleistung ist gering, was darauf hindeutet, dass sie in diesem Datenset keine starke Trennkraft für die Klassifikation bieten.\n",
    "\n",
    "Die Kreditkartenaffinität scheint stark durch den durchschnittlichen Kontostand bestimmt zu sein. Vermutlich ein Indikator für langfristige finanzielle Stabilität oder Kaufkraft. Verhalten (Transaktionen),  demografische Merkmale und aggregierte Kennzahlen liefern zusätzliche, aber deutlich schwächere Signale. Das Modell stützt sich somit primär auf monetäre Mittel, weniger auf Verhalten oder Alter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originalnamen der Top-n Features extrahieren\n",
    "top_features = [feature_names[i] for i in top_idx]\n",
    "\n",
    "# Layout mit 3x4 Subplots für bis zu 12 Features\n",
    "fig, axs = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Partial Dependence Plots (PDP) für die Top-Features erzeugen\n",
    "PartialDependenceDisplay.from_estimator(best_brf, X_train, top_features, ax=axs[:len(top_features)])\n",
    "\n",
    "for ax in axs[:len(top_features)]:\n",
    "    # X-Achse mit Tausender-Trennung formatieren\n",
    "    ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('Partial dependence', fontsize=10, fontweight='bold')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=9)\n",
    "\n",
    "# Überflüssige Achsen entfernen, falls weniger Features als Subplots\n",
    "for ax in axs[len(top_features):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.suptitle(\"Partial Dependence Plots (PDP) für Top 10 Features\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148aefc",
   "metadata": {},
   "source": [
    "Die PDPs zeigen den Einfluss der wichtigsten Merkmale auf die Vorhersagewahrscheinlichkeit für einen Kreditkartenkauf.\n",
    "\n",
    "- Avg Balance zeigt den mit Abstand stärksten Effekt: Ab einem durchschnittlichen Kontostand von ca. 30’000 steigt die Kaufwahrscheinlichkeit sprunghaft von unter 10 % auf über 60 %. Es handelt sich um ein klares Schwellenverhalten, das auf eine hohe Relevanz finanzieller Stabilität hinweist.\n",
    "\n",
    "- Balance Before CC und Max Balance haben ebenfalls einen positiven, aber moderateren Effekt. Auch hier deuten hohe Werte auf eine erhöhte Kreditkartenaffinität hin – allerdings mit flacheren Kurvenverläufen.\n",
    "\n",
    "- Transaktionsbezogene Variablen wie avg_trans_amount, max_trans_amount, std_trans_amount und med_trans_amount zeigen einen leichten positiven Einfluss, allerdings ohne deutliche Knicke oder Schwellen. Sie liefern ergänzende Informationen zum Zahlungsverhalten, sind aber weniger trennstark.\n",
    "\n",
    "- Age, total_spent und min_balance weisen nahezu flache Verläufe auf. Diese Merkmale haben kaum Einfluss auf die Modellprognose.\n",
    "\n",
    "Der durchschnittliche Kontostand (avg_balance) ist der zentrale Prädiktor mit klarem Schwellenverhalten. Weitere finanzielle Merkmale unterstützen die Vorhersage, während demografische oder verhaltensbasierte Kennzahlen nur geringe zusätzliche Erklärungskraft besitzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_2_'></a>[Feature-Importance-Vergleich: Baseline, Kandidatenmodelle und Bestmodell](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fe46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optimiertes Modell in pipelines einfügen \n",
    "pipelines[\"Balanced RF (Optimiert)\"] = brf_search.best_estimator_\n",
    "\n",
    "# Baseline Logistic VIF hinzufügen\n",
    "pipelines[\"Baseline Logistic Regression (VIF Reduced)\"] = baseline_pipeline_reduced\n",
    "\n",
    "# Auswahl der Modelle für den Plot\n",
    "pipelines_subset = {\n",
    "    \"Balanced RF (Optimiert)\": pipelines[\"Balanced RF (Optimiert)\"],\n",
    "    \"Random Forest\": pipelines[\"Random Forest\"],\n",
    "    \"HistGradBoost\": pipelines[\"HistGradBoost\"],\n",
    "    \"XGBoost\": pipelines[\"XGBoost\"],\n",
    "    \"Baseline Logistic Regression (VIF Reduced)\": pipelines[\"Baseline Logistic Regression (VIF Reduced)\"]\n",
    "}\n",
    "\n",
    "# Farben für Modelle (Plotly Palette)\n",
    "colors = [\"#1f77b4\", \"#2ca02c\", \"#ff7f0e\", \"#d62728\", \"#9467bd\"]\n",
    "\n",
    "top_n = 10\n",
    "pfi_results = {}\n",
    "\n",
    "# Modelle trainieren und PFI berechnen\n",
    "for name, pipe in pipelines_subset.items():\n",
    "    print(f\"Trainiere {name} und berechne PFI...\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "    result = permutation_importance(\n",
    "        pipe, X_train, y_train,\n",
    "        scoring='roc_auc', n_repeats=10,\n",
    "        random_state=42, n_jobs=1\n",
    "    )\n",
    "    pfi_results[name] = result\n",
    "\n",
    "# Max-Wert zur Skalierung finden\n",
    "max_global_importance = max([res.importances_mean.max() for res in pfi_results.values()])\n",
    "\n",
    "# Layout vorbereiten\n",
    "num_models = len(pfi_results)\n",
    "cols = 2\n",
    "rows = (num_models + 1) // cols\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=rows,\n",
    "    cols=cols,\n",
    "    subplot_titles=list(pfi_results.keys()),\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "# Balken hinzufügen\n",
    "for i, (name, result) in enumerate(pfi_results.items()):\n",
    "    try:\n",
    "        cat_pipeline = pipelines_subset[name].named_steps['prep'].named_transformers_['cat']\n",
    "        ohe = cat_pipeline.named_steps['encode'] if hasattr(cat_pipeline, \"named_steps\") else cat_pipeline\n",
    "        ohe_features = list(ohe.get_feature_names_out(cat_cols))\n",
    "    except Exception as e:\n",
    "        print(f\"Konnte OHE für {name} nicht extrahieren: {e}\")\n",
    "        ohe_features = []\n",
    "\n",
    "    base_features = list(num_reduced if \"VIF\" in name else num_cols)\n",
    "    feature_names = base_features + ohe_features\n",
    "    feature_names_clean = [f.replace('_', ' ').title() for f in feature_names]\n",
    "\n",
    "    sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "    top_idx = sorted_idx[:top_n]\n",
    "\n",
    "    x = result.importances_mean[top_idx][::-1]\n",
    "    error_x = result.importances_std[top_idx][::-1]\n",
    "\n",
    "    y = np.array([\n",
    "        name.replace(\" Client\", \"\").replace(\"Transaction \", \"Trans. \").replace(\" Amount\", \"\")\n",
    "        if len(name) < 30 else name[:28] + \"…\"\n",
    "        for name in np.array(feature_names_clean)[top_idx][::-1]\n",
    "    ])\n",
    "\n",
    "    row = i // cols + 1\n",
    "    col = i % cols + 1\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            error_x=dict(type='data', array=error_x),\n",
    "            orientation='h',\n",
    "            marker_color=colors[i % len(colors)],\n",
    "            text=[f\"{val:.3f}\" for val in x],\n",
    "            textposition=\"auto\",\n",
    "            hovertemplate=\"%{y}: %{x:.3f} ± %{error_x.array:.3f}\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Permutation Importance\", range=[0, max_global_importance], row=row, col=col)\n",
    "    fig.update_yaxes(tickfont=dict(size=9), row=row, col=col)\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    height=300 * rows,\n",
    "    width=950,\n",
    "    title_text=\"Top 10 Feature Importance (Permutation) – Ausgewählte Modelle\",\n",
    "    title_font=dict(size=16),\n",
    "    margin=dict(t=80, l=40, r=20, b=40)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41538a1d",
   "metadata": {},
   "source": [
    "Die Visualisierung vergleicht die wichtigsten Merkmale verschiedener Modellklassen anhand ihrer Permutation Feature Importance. Dabei wird deutlich, welche Variablen in den jeweiligen Modellen eine tragende Rolle spielen und wie stabil diese Gewichtung über Modellgrenzen hinweg ist.\n",
    "\n",
    "Zentrale Beobachtungen:\n",
    "\n",
    "- Avg Balance (durchschnittlicher Kontostand) zeigt sich in allen baumbasierten Modellen als konstant wichtigster Prädiktor. Unabhängig davon, ob Random Forest, XGBoost oder HistGradientBoosting verwendet wurde, bildet diese Variable das Rückgrat der Modellentscheidungen. Dies unterstreicht ihre hohe Aussagekraft im Hinblick auf die Kreditkartenaffinität.\n",
    "\n",
    "- Weitere finanzielle Merkmale wie der Kontostand vor der Kreditkarte, maximale Kontosalden oder Transaktionssummen tauchen ebenfalls häufig auf, sind in ihrer Bedeutung jedoch deutlich schwächer ausgeprägt und stärker modellabhängig. Manche Modelle erkennen darin zusätzliche Relevanz, andere stufen sie als vernachlässigbar ein.\n",
    "\n",
    "- Demografische oder soziostrukturelle Variablen wie Urbanisierungsgrad oder Arbeitslosenrate zeigen in baumbasierten Modellen kaum Einfluss, erscheinen aber teils in der Logistischen Regression.\n",
    "\n",
    "Unterschiede zwischen Modellklassen:\n",
    "\n",
    "- Baumbasierte Modelle identifizieren vor allem nichtlineare Muster und interagierende Verhaltensmerkmale – insbesondere in aggregierten Finanzkennzahlen.\n",
    "\n",
    "- Lineare Modelle (z. B. Logistic Regression) gewichten strukturelle Einzelmerkmale wie Kredithistorie oder sozioökonomische Daten stärker und sind empfindlicher gegenüber multikollinearen Einflüssen.\n",
    "\n",
    "\n",
    "Die modellübergreifende Analyse zeigt, dass finanzielle Stabilität gemessen am durchschnittlichen Kontostand ein robuster und konsistenter Prädiktor für die Kreditkartenaffinität ist. Die Unterschiede im Feature Ranking zwischen den Modellklassen verdeutlichen zudem deren jeweilige Stärken: Während baumbasierte Modelle flexibler und leistungsstärker sind, bieten lineare Modelle besser nachvollziehbare Entscheidungen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_3_'></a>[Modellreduktion: Vereinfachung durch Auswahl der wichtigsten Merkmale](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c7590",
   "metadata": {},
   "source": [
    "Basierend auf der Analyse der Permutation Feature Importance und der Partial Dependence Plots wurde das Modell gezielt auf die drei wichtigsten Merkmale reduziert:\n",
    "\n",
    "avg_balance, balance_before_cc, avg_trans_amount, max_balance, max_trans_amount\n",
    "\n",
    "Diese Variablen zeigen den stärksten Einfluss auf die Vorhersageleistung und ermöglichen auch in reduzierter Form ein stabiles und interpretierbares Modell, das sich für den praktischen Einsatz im Cross-Selling eignet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1116ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Auswahl der wichtigsten Features für das reduzierte Modell\n",
    "selected_features = ['avg_balance', 'balance_before_cc', 'avg_trans_amount', 'max_balance', 'max_trans_amount']\n",
    "\n",
    "# 2. Trainings- und Testdaten auf die ausgewählten Features reduzieren\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "\n",
    "# 3. Neues BalancedRandomForest-Modell m erstellen\n",
    "model_reduced = BalancedRandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# 4. Modelltraining auf den reduzierten Daten\n",
    "model_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 5. Vorhersagen und Wahrscheinlichkeiten für Referenz- und reduziertes Modell berechnen\n",
    "y_pred_ref = best_brf.predict(X_test)\n",
    "y_proba_ref = best_brf.predict_proba(X_test)[:, 1]\n",
    "y_pred_reduced = model_reduced.predict(X_test_reduced)\n",
    "y_proba_reduced = model_reduced.predict_proba(X_test_reduced)[:, 1]\n",
    "\n",
    "# ROC-AUC für beide Modelle berechnen\n",
    "roc_ref = roc_auc_score(y_test, y_proba_ref)\n",
    "roc_reduced = roc_auc_score(y_test, y_proba_reduced)\n",
    "\n",
    "# Weitere Klassifikationsmetriken berechnen\n",
    "metrics = {\n",
    "    \"ROC-AUC\": [roc_ref, roc_reduced],\n",
    "    \"Accuracy\": [accuracy_score(y_test, y_pred_ref), accuracy_score(y_test, y_pred_reduced)],\n",
    "    \"Precision\": [precision_score(y_test, y_pred_ref), precision_score(y_test, y_pred_reduced)],\n",
    "    \"Recall\": [recall_score(y_test, y_pred_ref), recall_score(y_test, y_pred_reduced)],\n",
    "    \"F1-Score\": [f1_score(y_test, y_pred_ref), f1_score(y_test, y_pred_reduced)]\n",
    "}\n",
    "\n",
    "# DataFrame für den Vergleich der Metriken erstellen\n",
    "df_comparison = pd.DataFrame(metrics, index=[\"Referenzmodell\", \"Reduziertes Modell\"])\n",
    "print(df_comparison)\n",
    "\n",
    "# Werte für Balkendiagramm vorbereiten\n",
    "labels = df_comparison.columns.tolist()\n",
    "referenz = df_comparison.loc[\"Referenzmodell\"].values\n",
    "reduziert = df_comparison.loc[\"Reduziertes Modell\"].values\n",
    "\n",
    "# Balkendiagramm erstellen zum Vergleich der Modell-Performances\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, referenz, width, label='Referenzmodell')\n",
    "rects2 = ax.bar(x + width/2, reduziert, width, label='Reduziertes Modell')\n",
    "\n",
    "# Achsentitel und Diagrammtitel setzen\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Modellvergleich der Performance-Metriken')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Werte über den Balken anzeigen\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # vertikale Verschiebung\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "# Y-Achse begrenzen\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd04ab8",
   "metadata": {},
   "source": [
    "Aufgrund der Ergebnisse im Bild und der Analyse der Performance-Metriken zeigt sich, dass das reduzierte Modell einige Einbussen in der Vorhersageleistung aufweist, insbesondere beim Recall. Der ROC-AUC-Wert bleibt nahezu unverändert (0.886 vs. 0.879), was eine starke Leistung beider Modelle zeigt. Jedoch ist der Recall beim reduzierten Modell deutlich geringer (0.768 vs. 0.928), was zu einem Rückgang der Fähigkeit führt, Käufer:innen korrekt zu identifizieren. Auch der F1-Score sinkt leicht (0.595 vs. 0.568), was die geringere Genauigkeit beim Erkennen relevanter Käufer:innen widerspiegelt.\n",
    "\n",
    "Angesichts der starken Abnahme des Recalls und der daraus resultierenden Performanceeinbussen im reduzierten Modell, ist es sinnvoll, das ursprüngliche Modell beizubehalten. Die Vereinfachung der Merkmale führt zu einem Verlust an Vorhersagegenauigkeit, was das reduzierte Modell weniger geeignet für den praktischen Einsatz macht. Das ursprüngliche Modell bleibt daher die bevorzugte Wahl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad6015",
   "metadata": {},
   "source": [
    "## <a id='toc9_4_'></a>[Praktische Bedeutung und Erklärung des finalen Modells](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_4_1_'></a>[Lift Kurve](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f4e76",
   "metadata": {},
   "source": [
    "Im folgenden Schritt wird eine Lift-Analyse durchgeführt, um die Effektivität des finalen Modells im Kontext einer zielgerichteten Marketingkampagne zu bewerten.\n",
    "\n",
    "Die Lift-Kurve ist ein etabliertes Verfahren im Predictive Modeling und zeigt, wie gut ein Klassifikationsmodell geeignete Zielgruppen im Vergleich zu einer zufälligen Auswahl identifizieren kann. Dabei wird visualisiert, welcher Anteil der tatsächlichen Zielereignisse (Kreditkartenkäufe) durch die am höchsten bewerteten Modellvorhersagen abgedeckt wird.\n",
    "\n",
    "Die kumulative Darstellung gibt Aufschluss darüber, wie viele Käufer bei zunehmender Zielgruppengrösse erreicht werden. Zusätzlich zeigt die segmentierte Lift-Kurve, in welchen Kundengruppen das Modell besonders effektiv unterscheidet. Diese Informationen unterstützen die strategische Planung von Kampagnen auf Basis von Modellscores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8750b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_lift_and_response_separate_models(y_true, y_scores_dict, segments=10):\n",
    "    n_models = len(y_scores_dict)\n",
    "\n",
    "    # Kürzere Modellnamen\n",
    "    def shorten(name):\n",
    "        if \"Logistic\" in name:\n",
    "            return \"Baseline LogReg\"\n",
    "        elif \"Balanced RF\" in name:\n",
    "            return \"Balanced RF\"\n",
    "        return name[:20] + \"...\"\n",
    "\n",
    "    subplot_titles = []\n",
    "    for name in y_scores_dict.keys():\n",
    "        short = shorten(name)\n",
    "        subplot_titles.extend([f\"{short} Lift\", f\"{short} Response\"])\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=n_models, cols=2,\n",
    "        subplot_titles=subplot_titles,\n",
    "        vertical_spacing=0.25,\n",
    "        horizontal_spacing=0.08\n",
    "    )\n",
    "\n",
    "    for i, (model_name, y_scores) in enumerate(y_scores_dict.items(), start=1):\n",
    "        sorted_data = sorted(zip(y_true, y_scores), key=lambda x: x[1], reverse=True)\n",
    "        y_sorted = np.array([y for y, _ in sorted_data])\n",
    "        n = len(y_sorted)\n",
    "        total_pos = y_sorted.sum()\n",
    "\n",
    "        bin_size = n // segments\n",
    "        lift_vals = []\n",
    "        response_vals = []\n",
    "        segment_centers = []\n",
    "\n",
    "        for j in range(segments):\n",
    "            start = j * bin_size\n",
    "            end = (j + 1) * bin_size if j < segments - 1 else n\n",
    "            segment = y_sorted[start:end]\n",
    "            expected = len(segment) * (total_pos / n)\n",
    "            lift_vals.append(segment.sum() / expected if expected > 0 else 0)\n",
    "            response_vals.append(segment.mean() if len(segment) > 0 else 0)\n",
    "            segment_centers.append((start + end) / 2 / n * 100)\n",
    "\n",
    "        # Lift-Balken\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=segment_centers,\n",
    "            y=lift_vals,\n",
    "            name=f\"{shorten(model_name)} Lift\",\n",
    "            showlegend=(i == 1),\n",
    "            marker_color='blue'\n",
    "        ), row=i, col=1)\n",
    "        fig.update_yaxes(range=[0, 4.5], title_text=\"Lift\", row=i, col=1)\n",
    "\n",
    "        # Response-Kurve\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=segment_centers,\n",
    "            y=response_vals,\n",
    "            name=f\"{shorten(model_name)} Response\",\n",
    "            mode='lines+markers',\n",
    "            marker=dict(color='orange'),\n",
    "            showlegend=(i == 1)\n",
    "        ), row=i, col=2)\n",
    "        fig.update_yaxes(range=[0, 0.7], title_text=\"Response\", row=i, col=2)\n",
    "\n",
    "        # Zufallslinie (nur einmal oben)\n",
    "        if i == 1:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[0, 100], y=[1, 1],\n",
    "                mode='lines',\n",
    "                line=dict(dash='dash', color='gray'),\n",
    "                name=\"Zufall (Lift=1)\",\n",
    "                showlegend=True\n",
    "            ), row=i, col=1)\n",
    "\n",
    "        # X-Achsen: Ticks & Titel\n",
    "        for col in [1, 2]:\n",
    "            fig.update_xaxes(\n",
    "                title_text=\"Top-Kundenanteil (%)\",\n",
    "                tickmode=\"linear\",\n",
    "                tick0=0,\n",
    "                dtick=10,\n",
    "                row=i,\n",
    "                col=col\n",
    "            )\n",
    "\n",
    "    # Gemeinsames Layout\n",
    "    fig.update_layout(\n",
    "        height=320 * n_models,\n",
    "        width=1000,\n",
    "        title_text=\"Segmentierter Lift & Response pro Modell\",\n",
    "        legend=dict(\n",
    "            x=1.02,\n",
    "            y=1,\n",
    "            orientation='v',\n",
    "            font=dict(size=11),\n",
    "            bgcolor='rgba(255,255,255,0)'\n",
    "        ),\n",
    "        margin=dict(t=100, r=180, l=50, b=60),\n",
    "        plot_bgcolor='white',\n",
    "        font=dict(size=13)\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showgrid=True, gridcolor='lightgray', gridwidth=1, title_standoff=10)\n",
    "    fig.update_yaxes(showgrid=True, gridcolor='lightgray', gridwidth=1, title_standoff=10)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_cumulative_lift(y_true, y_scores_dict):\n",
    "    \"\"\"\n",
    "    Plottet kumulative Lift-Kurve für mehrere Modelle.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for model_name, y_scores in y_scores_dict.items():\n",
    "        sorted_data = sorted(zip(y_true, y_scores), key=lambda x: x[1], reverse=True)\n",
    "        y_sorted = np.array([y for y, _ in sorted_data])\n",
    "        total_pos = y_sorted.sum()\n",
    "        cum_true_positives = np.cumsum(y_sorted)\n",
    "        lift_cumulative = cum_true_positives / total_pos\n",
    "        percents = np.arange(1, len(y_sorted) + 1) / len(y_sorted)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=percents,\n",
    "            y=lift_cumulative,\n",
    "            mode='lines',\n",
    "            name=model_name\n",
    "        ))\n",
    "\n",
    "    # Zufallslinie\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=percents,\n",
    "        y=percents,\n",
    "        mode='lines',\n",
    "        name='Zufall',\n",
    "        line=dict(dash='dash', color='gray')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Kumulative Lift-Kurve – Modellvergleich\",\n",
    "        xaxis_title=\"Top-N Kunden (nach Score) [%]\",\n",
    "        yaxis_title=\"Kumulierter Anteil Käufer\",\n",
    "        xaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1,\n",
    "                   showgrid=True, gridcolor='lightgray'),\n",
    "        yaxis=dict(showgrid=True, gridcolor='lightgray'),\n",
    "        plot_bgcolor='white',\n",
    "        font=dict(size=14),\n",
    "        width=1000,\n",
    "        height=450,\n",
    "        margin=dict(t=80, l=60, r=60, b=60)\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Wahrscheinlichkeiten berechnen (Beispiel)\n",
    "y_proba_baseline = pipelines[\"Baseline Logistic Regression (VIF Reduced)\"].predict_proba(X_test)[:, 1]\n",
    "y_proba_final = best_brf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Segmentierte Lift- & Response-Kurven plotten\n",
    "plot_lift_and_response_separate_models(\n",
    "    y_test,\n",
    "    {\n",
    "        \"Baseline Logistic Regression (VIF Reduced)\": y_proba_baseline,\n",
    "        \"Finales Modell (Balanced RF)\": y_proba_final\n",
    "    },\n",
    "    segments=20\n",
    ")\n",
    "\n",
    "# Kumulative Lift-Kurve plotten\n",
    "plot_cumulative_lift(\n",
    "    y_test,\n",
    "    {\n",
    "        \"Baseline Logistic Regression (VIF Reduced)\": y_proba_baseline,\n",
    "        \"Finales Modell (Balanced RF)\": y_proba_final\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c0249",
   "metadata": {},
   "source": [
    "**Segmentierter Lift & Response**\n",
    "\n",
    "Zur Bewertung der Marketingwirksamkeit wurden beide Modelle hinsichtlich ihrer Fähigkeit analysiert, Kreditkartenkäufer gezielt in den Top-Kundensegmenten zu identifizieren. Hierzu wurde der Kundenstamm in 20 gleich grosse Segmente (Top-5 % bis Top-100 %) unterteilt.\n",
    "\n",
    "Beobachtungen:\n",
    "\n",
    "- Das Balanced Random Forest (RF) Modell erzielt in den obersten Kundensegmenten einen Lift-Wert von über 3.8, während die Baseline Logistic Regression (VIF-reduziert) maximal auf etwa 2.6 kommt.\n",
    "\n",
    "- Auch der Response-Wert – also der tatsächliche Käuferanteil pro Segment – ist beim Balanced RF durchgehend höher. Besonders im Top-5 %-Segment liegt der Response bei ~63 %, im Vergleich zu ~45 % bei der Logistischen Regression.\n",
    "\n",
    "- Mit zunehmendem Prozentrang sinkt der Response wie erwartet, allerdings zeigt sich das Balanced RF-Modell durchgehend überlegen – es priorisiert kaufbereite Zielgruppen systematischer.\n",
    "\n",
    "**Kumulative Lift-Kurve**\n",
    "\n",
    "Die kumulative Lift-Kurve untermauert diese Ergebnisse:\n",
    "\n",
    "- Das Balanced RF-Modell identifiziert nahezu 100 % der Käufer bereits innerhalb der obersten 40 % der Kunden.\n",
    "\n",
    "- Die Baseline Logistic Regression benötigt dafür rund 70 % der Kunden.\n",
    "\n",
    "- Die Zufallslinie (Lift = 1) verläuft erwartungsgemäss diagonal und dient als Referenz.\n",
    "\n",
    "**Fazit**\n",
    "\n",
    "Das finale Modell (Balanced Random Forest) bietet eine deutlich höhere Trennschärfe als die baseline-logistische Regression – insbesondere im oberen Scoring-Bereich. Es ermöglicht somit eine zielgerichtete, effiziente Ansprache kaufbereiter Kund:innen und stellt eine klare Verbesserung für Marketinganwendungen dar, bei denen Budgets auf wenige, relevante Zielgruppen konzentriert werden sollen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_4_2_'></a>[Quantitative Beschreibung zentraler Predictive Features](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4cee23",
   "metadata": {},
   "source": [
    "Das finale Modell stützt sich vor allem auf finanzielle Kennzahlen aus dem Kundenverhalten. Die drei wichtigsten Merkmale lassen sich wie folgt beschreiben:\n",
    "\n",
    "**Durchschnittlicher Kontostand (avg_balance)**\n",
    "\n",
    "Dieses Merkmal ist mit deutlichem Abstand der wichtigste Prädiktor im Modell.\n",
    "Je höher der durchschnittliche Kontostand einer Person, desto höher ist ihre Wahrscheinlichkeit, eine Kreditkarte zu erwerben.\n",
    "Die Permutation Feature Importance zeigt hier den stärksten Leistungsabfall bei Permutation, und auch die Partial Dependence Plots bestätigen ein klares Schwellenverhalten: Ab etwa 30’000 steigt die Kaufwahrscheinlichkeit sprunghaft an.\n",
    "\n",
    "**Kontostand vor dem Kreditkartenzeitpunkt (balance_before_cc)**\n",
    "\n",
    "Auch dieser Wert trägt relevant zur Modellleistung bei, wenn auch deutlich geringer als avg_balance.\n",
    "Ein durchgehend positives oder wachsendes Guthaben vor dem Kreditkartenkauf deutet auf stabile Zahlungsfähigkeit hin. Die PDP-Kurve zeigt einen gleichmässig positiven Verlauf – je höher der Wert, desto wahrscheinlicher ein Kauf.\n",
    "\n",
    "**Durchschnittlicher Transaktionsbetrag (avg_trans_amount)**\n",
    "\n",
    "Dieses Merkmal liefert eine Verhaltensdimension: Höhere durchschnittliche Einzeltransaktionen korrelieren mit einer höheren Kaufwahrscheinlichkeit.\n",
    "Die Bedeutung ist im Vergleich zu den beiden vorherigen Merkmalen geringer, aber konsistent positiv. Dies weist auf eine aktivere oder konsumfreudigere Kundengruppe hin.\n",
    "\n",
    "Weitere Merkmale wie min_balance, std_trans_amount, max_trans_amount oder total_spent wurden ebenfalls untersucht, zeigen aber nahezu keinen Einfluss auf die Modellgüte und können für ein reduziertes Modell vernachlässigt werden.\n",
    "\n",
    "**Fazit**\n",
    "\n",
    "Das Modell benötigt nur wenige, gut messbare Merkmale, um zuverlässige Vorhersagen zu treffen.\n",
    "Die Kombination aus durchschnittlichem Kontostand, Saldoentwicklung vor dem Kreditkartenzeitpunkt und Transaktionsverhalten liefert eine präzise und robuste Grundlage für datengetriebene Entscheidungen – ohne den Einsatz sensibler persönlicher Merkmale wie Alter, Geschlecht oder Wohnort.\n",
    "Damit eignet sich das Modell ideal für den praxisnahen Einsatz im Cross-Selling, bei dem Transparenz und Interpretierbarkeit eine zentrale Rolle spielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_4_3_'></a>[Schlussfazit (Beschreibung Funktionsweise und Mehrwert des finalen Modells)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53dc4d9",
   "metadata": {},
   "source": [
    "**Ziel des Modells**\n",
    "\n",
    "Das Modell wurde entwickelt, um vorherzusagen, welche Kundinnen und Kunden mit hoher Wahrscheinlichkeit eine Kreditkarte kaufen würden. Grundlage sind historische Kundendaten aus dynamischen und statischen Entitäten.\n",
    "\n",
    "\n",
    "**Wie das Modell funktioniert**\n",
    "\n",
    "Das Modell analysiert vergangene Daten von Kreditkartenkäufern und erkennt typische Muster.\n",
    "Anhand dieser Merkmale berechnet es für jeden Kunden einen Wahrscheinlichkeitswert (Score) für einen möglichen Kreditkartenkauf. \n",
    "\n",
    "\n",
    "**Was das Modell leistet**\n",
    "\n",
    "In der Praxis kann das Modell genutzt werden, um:\n",
    "\n",
    "- gezielte Kampagnen durchzuführen (z. B. Mailings, Callcenter, Beraterkontakt)\n",
    "- interessierte Kunden frühzeitig zu identifizieren\n",
    "- Marketingressourcen effizienter einzusetzen\n",
    "\n",
    "\n",
    "Die Lift-Kurve zeigt deutlich: Bereits die obersten 10 % der nach Modellscore gerankten Kunden enthalten rund 30 % der tatsächlichen Käufer – das entspricht einem Lift-Faktor von etwa 3.0 im Vergleich zur Zufallsauswahl (Lift = 1).\n",
    "Das bedeutet: Mit einem Bruchteil des Marketingaufwands lassen sich bereits ein Drittel aller Abschlüsse erzielen – ein klarer Effizienzgewinn für gezielte Kampagnen.\n",
    "\n",
    "\n",
    "**Warum das nützlich ist**\n",
    "\n",
    "Für Fachabteilungen wie Marketing, Vertrieb oder Beratung bietet das Modell einen konkreten Anhaltspunkt zur Priorisierung:\n",
    "\n",
    "- Wen sprechen wir zuerst an?\n",
    "- Wem bieten wir aktiv eine Kreditkarte an?\n",
    "- Wo lohnt sich ein Beratungsgespräch?\n",
    "\n",
    "Es ersetzt dabei keine menschliche Einschätzung, sondern unterstützt Entscheidungen datenbasiert – ähnlich wie ein Frühwarnsystem.\n",
    "\n",
    "**Fazit**\n",
    "\n",
    "Das finale Modell hilft, Zielgruppen besser zu verstehen und zu erreichen. Es steigert die Effizienz von Werbe- und Vertriebsmassnahmen, reduziert Streuverluste und schafft die Grundlage für eine datengetriebene, kundenorientierte Ansprache – ohne dass technisches Fachwissen erforderlich ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[Anhang](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb70c5",
   "metadata": {},
   "source": [
    "**Verwendung von ChatGPT im Projekt – Einsatz, Strategien und Bewertung**\n",
    "\n",
    "Im Rahmen der Mini-Challenge „Applied Machine Learning“ wurde ChatGPT als unterstützendes AI-Tool eingesetzt, um die Entwicklung und Dokumentation des Projekts effizienter und strukturierter zu gestalten.\n",
    "\n",
    "**Eingesetzte Aufgabenbereiche**\n",
    "\n",
    "- **Konzeptionelle Fragen und Erklärungen:**  \n",
    "  Verständnisfragen zu Modellierungstechniken, Datenaufbereitung und ML-Methoden wurden mit ChatGPT diskutiert, um das eigene Wissen zu vertiefen.\n",
    "\n",
    "- **Code-Generierung und Debugging:**  \n",
    "  Für wiederkehrende Programmieraufgaben, etwa bei der Erstellung von Pipelines, Berechnung der Permutation Feature Importance (PFI) oder der Visualisierung mittels Plotly, wurde ChatGPT genutzt, um initialen Code zu erstellen oder Fehlerquellen zu identifizieren.\n",
    "\n",
    "- **Dokumentation und Textformulierung:**  \n",
    "  ChatGPT unterstützte bei der Erstellung von klar strukturierten Markdown-Texten für Jupyter-Notebooks, die Nachvollziehbarkeit und Verständlichkeit erhöhen.\n",
    "\n",
    "**Prompting-Strategien**\n",
    "\n",
    "- **Explizite Fragestellungen:**  \n",
    "  Kurze, konkrete Fragen (z. B. „Wie berechne ich PFI für mehrere Modelle?“) führten zu fokussierten, praxisnahen Antworten.\n",
    "\n",
    "- **Schrittweise Anleitungen:**  \n",
    "  Bei komplexeren Aufgaben wurde die Arbeit in kleinere Schritte unterteilt und iterativ mit ChatGPT bearbeitet („Zeig mir zunächst, wie ich Cross-Validation mache, danach PFI...“).\n",
    "\n",
    "- **Kontextuelle Einbettung:**  \n",
    "  Zur besseren Ergebnisqualität wurden relevante Code-Snippets und Projekthintergrund bereitgestellt, sodass die Antworten spezifisch auf das eigene Projekt zugeschnitten waren.\n",
    "\n",
    "**Bewertung der Prompting-Strategien**\n",
    "\n",
    "Die expliziten, klar formulierten Fragen mit konkretem Kontext führten am meisten zum schnellen Lösen der Aufgaben. Insbesondere das iterative Vorgehen half, komplexe Abläufe verständlich und strukturiert umzusetzen. Diese Strategie förderte nicht nur die Effizienz, sondern auch den Lernprozess, da die einzelnen Schritte nachvollziehbar diskutiert und angepasst werden konnten.\n",
    "\n",
    "Der Einsatz von ChatGPT hat wesentlich zur Beschleunigung der Entwicklung beigetragen, ohne dass die Eigenständigkeit darunter litt. Die Kombination aus selbstständigem Arbeiten und gezieltem Einsatz von AI-Unterstützung wurde als besonders effektiv empfunden.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
